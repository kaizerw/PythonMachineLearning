{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Good Training Sets - Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quality of the data and the amount of useful information that it contains are key factors that determine how well a machine learning algorithm can learn. Therefore, it is absolutely critical that we make sure of examine and preprocess a dataset before we feed it to a learning algorithm. In this chapter, we will discuss the essential data preprocessing techniques that will help us build good machine learning models. \n",
    "\n",
    "The topics what we will cover in this chapter are as follows:\n",
    "* Removing and imputing missing values from the dataset\n",
    "* Getting categorical data into shape for machine learning algorithms\n",
    "* Selecting relevant features for the model construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not uncommon in real-world applications for our samples to be missing one or more values for various reasons. There could have been an error in the data collection process, certain measurements are not applicable, or particular fields could have been simply left blank in a survey, for example. We typically see missing values as the blank spaces in our data table or as placeholder strings such as *NaN*, which stands for not a number, or *NULL* (a commonly used indicator of unknown values in relational databases). \n",
    "\n",
    "Unfortunately, most computational tools are unable to handle such missing values, or produce unpredictable results if we simply ignore them. Therefore, it is crucial that we take care of those missing values before we proceed with further analyses. In this section, we will work through several practical techniques for dealing with missing values by removing entries from our dataset or imputing missing values from other samples and features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying missing values in tabular data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But before we discuss several techniques for dealing with missing values, let's create a simple example data frame from a **Comma-separated Values (CSV)** file to get a better grasp of the problem: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      A     B     C    D\n",
       "0   1.0   2.0   3.0  4.0\n",
       "1   5.0   6.0   NaN  8.0\n",
       "2  10.0  11.0  12.0  NaN"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "csv_data = \\\n",
    "'''A,B,C,D\n",
    "1.0,2.0,3.0,4.0\n",
    "5.0,6.0,,8.0\n",
    "10.0,11.0,12.0,'''\n",
    "df = pd.read_csv(StringIO(csv_data))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the preceding code, we read CSV-formatted data into a pandas *DataFrame* via the *read_csv* function and noticed that the two missing cells were replaced by *NaN*. The *StringIO* function in the preceding code example was simply used for the purposes of illustration. It allows us to read the string assigned to *csv_data* into a pandas *DataFrame* as if it was a regular CSV file on our hard drive. \n",
    "\n",
    "For a larger *DataFrame*, it can be tedious to look for missing values manually; in this case, we can use the *isnull* method to return a *DataFrame* with Boolean values that indicate whether a cell contains a numeric value (False) or if data is missing (True). Using the *sum* method, we can then return the number of missing values per column as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A    0\n",
       "B    0\n",
       "C    1\n",
       "D    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way, we can count the number of missing values per column; in the following subsections, we will take a look at different strategies for how to deal with this missing data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although scikit-learn was developed for working with NumPy arrays, it can sometimes be more convenient to preprocess data using pandas' *DataFrame*. We can always access the underlying NumPy array of a *DataFrame* via the *values* attribute before we feed it into a scikit-learn estimator: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  2.,  3.,  4.],\n",
       "       [ 5.,  6., nan,  8.],\n",
       "       [10., 11., 12., nan]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eliminating samples of features with missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the easiest ways to deal with missing data is to simply remove the corresponding features (columns) or samples (rows) from the dataset entirely; rows with missing values can be easily dropped via the *dropna* method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     A    B    C    D\n",
       "0  1.0  2.0  3.0  4.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can drop columns that have at least one *NaN* in any row by setting the *axis* argument to *1*: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      A     B\n",
       "0   1.0   2.0\n",
       "1   5.0   6.0\n",
       "2  10.0  11.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *dropna* method supports several additional parameters that can come in handy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      A     B     C    D\n",
      "0   1.0   2.0   3.0  4.0\n",
      "1   5.0   6.0   NaN  8.0\n",
      "2  10.0  11.0  12.0  NaN \n",
      "\n",
      "     A    B    C    D\n",
      "0  1.0  2.0  3.0  4.0 \n",
      "\n",
      "      A     B     C    D\n",
      "0   1.0   2.0   3.0  4.0\n",
      "2  10.0  11.0  12.0  NaN \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# only drops rows where all columns are NaN\n",
    "print(df.dropna(how='all'), '\\n')\n",
    "\n",
    "# drop rows that have less then 4 real values\n",
    "print(df.dropna(thresh=4), '\\n')\n",
    "\n",
    "# only drop rows where NaN appear in specific columns\n",
    "print(df.dropna(subset=['C']), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the removal of missing data seems to be a convenient approach, it also comes with certain disadvantages; for example, we may end up removing too many samples, which will make a reliable analysis impossible. Or, if we remove too many feature columns, we will run the risk of losing valuable information that our classifier needs to discriminate between classes. In the next section, we will thus look at one of the most commonly used alternatives for dealing with missing values: interpolation techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputing missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, the removal of samples or dropping of entire feature columns is simply no feasible, because we might lose too much valuable data. In this case, we can use different interpolation techniques to estimate the missing values from the other training samples in our dataset. One of the most common interpolation techniques is **mean imputation**, where we simply replace the missing value with the mean value of the entire feature column. A convenient way to achieve this is by using the *Imputer* class from scikit-learn, as shown in the following code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1. ,  2. ,  3. ,  4. ],\n",
       "       [ 5. ,  6. ,  7.5,  8. ],\n",
       "       [10. , 11. , 12. ,  6. ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "imr = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "imr = imr.fit(df.values)\n",
    "imputed_data = imr.transform(df.values)\n",
    "imputed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we replaced each *NaN* value with the corresponding mean, which is separately calculated for each feature column. If we changed the *axis=0* setting to *axis=1*, we would calculate the row means. Other options for the *strategy* prameter are *median* or *most_frequent*, where the latter replaces the missing values with the most frequent values. This is useful for imputing categorical feature values, for example, a feature column that stores an encoding of color names, such as red, green and blue, and we will encounter examples of such data later in this chapter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the scikit-learn estimator API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, we used the *Imputer* class from scikit-learn to impute missing values in our dataset. The *Imputer* class belongs to the so-called **transformer** classes in scikit-learn, which are used for data transformation. The two essential methods for those estimators are *fit* and *transform*. The *fit* method is used to learn the parameters from the training data, and the *transform* method uses those parameters to transform the data. Any data array that is transformed needs to have the same number of features as the data array that was used to fit the model. The following figure illustrates how a transformer, fitten on the training data, is used to transform a training dataset as well as a new test dataset: \n",
    "\n",
    "<img src='images/04_01.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifiers that we used previously belong to the so-called **estimators** in scikit-learn with an API that is conceptually very similar to the transformer class. Estimators have a *predict* method but can also have a *transform* method, as we will see later in this chapter. \n",
    "\n",
    "As you may recall, we also used the *fit* method to learn the parameters of a model when we trained those estimators for classification. However, in supervised learning tasks, we additionally provide the class labels for fitting the model, which can then be used to make predictions about new data samples via the *predict* method, as illustrated in the following figure:\n",
    "\n",
    "<img src='images/04_02.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling categorial data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have only been working with numerical values. However, it is not uncommon that real-world datasets contain one or more categorical feature columns. In this section, we will make use of simple yet effective examples to see how we deal with this type of data in numerical computing libraries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nominal and ordinal features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we are talking about categorical data, we have to further distinguish between **nominal** and **ordinal** features. Ordinal features can be understood as categorical values that can be sorted or ordered. For example, t-shirt size would be an ordinal feature, because we can define an order XL > L > M. In contrast, nominal features do not imply any order and, to continue with the previous example, we could think of t-shirt color as a nominal feature since it typically does not make sense to say that, for example, red is larger than blue. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a example dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we explore different techniques to handle such categorical data, let's create a new *DataFrame* to illustrate the problem: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>color</th>\n",
       "      <th>size</th>\n",
       "      <th>price</th>\n",
       "      <th>classlabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>green</td>\n",
       "      <td>M</td>\n",
       "      <td>10.1</td>\n",
       "      <td>class1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>red</td>\n",
       "      <td>L</td>\n",
       "      <td>13.5</td>\n",
       "      <td>class2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blue</td>\n",
       "      <td>XL</td>\n",
       "      <td>15.3</td>\n",
       "      <td>class1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   color size  price classlabel\n",
       "0  green    M   10.1     class1\n",
       "1    red    L   13.5     class2\n",
       "2   blue   XL   15.3     class1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame([\n",
    "    ['green', 'M', 10.1, 'class1'], \n",
    "    ['red', 'L', 13.5, 'class2'], \n",
    "    ['blue', 'XL', 15.3, 'class1']])\n",
    "df.columns = ['color', 'size', 'price', 'classlabel']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the preceding output, the newly created *DataFrame* contains a nominal feature (*color*), an ordinal feature (*size*), and a numerical feature (*price*) column. The class labels (assuming that we created a dataset for a supervised learning task) are stored in the last column. The learning algorithms for classification that we discuss in this book do not use ordinal information in class labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping ordinal features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure that the learning algorithm interprets the ordinal features correctly, we need to convert the categorical string values into integer. Unfortunately, there is no convenient function that can automatically derive the correct order of the labels of our *size* feature, so we have to define the mapping manually. In the following simple example, let's assume that we know the numerical difference between features, for example, $XL = L + 1 = M + 2$: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>color</th>\n",
       "      <th>size</th>\n",
       "      <th>price</th>\n",
       "      <th>classlabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>green</td>\n",
       "      <td>1</td>\n",
       "      <td>10.1</td>\n",
       "      <td>class1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>red</td>\n",
       "      <td>2</td>\n",
       "      <td>13.5</td>\n",
       "      <td>class2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blue</td>\n",
       "      <td>3</td>\n",
       "      <td>15.3</td>\n",
       "      <td>class1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   color  size  price classlabel\n",
       "0  green     1   10.1     class1\n",
       "1    red     2   13.5     class2\n",
       "2   blue     3   15.3     class1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size_mapping = {\n",
    "    'XL': 3, \n",
    "    'L': 2, \n",
    "    'M': 1\n",
    "}\n",
    "df['size'] = df['size'].map(size_mapping)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to transform the integer values back to the original string representation at a later stage, we can simply define a reverse-mapping dictionary that can then be used via the pandas *map* method on the transformed feature column, similiar to the *size_mapping* dictionary that we used previously. We can use it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     M\n",
       "1     L\n",
       "2    XL\n",
       "Name: size, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_size_mapping = {v: k for k, v in size_mapping.items()}\n",
    "df['size'].map(inv_size_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding class labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many machine learning libraries require that class labels are encoded as integer values. Although most estimators for classification in scikit-learn convert class labels to integers internally, it is considered good practice to provide class labels as integer arrays to avoid technical glitches. To encode the class labels, we can use an approach similar to the mapping of ordinal features discussed previously. We need to remember that class labels are not ordinal, and it does not matter which integer number we assign to a particular string label. Thus, we can simply enumerate the class labels, starting at 0: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class1': 0, 'class2': 1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class_mapping = {label:idx for idx,label in \n",
    "                 enumerate(np.unique(df['classlabel']))}\n",
    "class_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can use the mapping dictionary to transform the class labels into integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>color</th>\n",
       "      <th>size</th>\n",
       "      <th>price</th>\n",
       "      <th>classlabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>green</td>\n",
       "      <td>1</td>\n",
       "      <td>10.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>red</td>\n",
       "      <td>2</td>\n",
       "      <td>13.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blue</td>\n",
       "      <td>3</td>\n",
       "      <td>15.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   color  size  price  classlabel\n",
       "0  green     1   10.1           0\n",
       "1    red     2   13.5           1\n",
       "2   blue     3   15.3           0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['classlabel'] = df['classlabel'].map(class_mapping)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reverse the key-value pairs in the mapping dictionary as follows to map the converted class labels back to the original string representation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>color</th>\n",
       "      <th>size</th>\n",
       "      <th>price</th>\n",
       "      <th>classlabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>green</td>\n",
       "      <td>1</td>\n",
       "      <td>10.1</td>\n",
       "      <td>class1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>red</td>\n",
       "      <td>2</td>\n",
       "      <td>13.5</td>\n",
       "      <td>class2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blue</td>\n",
       "      <td>3</td>\n",
       "      <td>15.3</td>\n",
       "      <td>class1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   color  size  price classlabel\n",
       "0  green     1   10.1     class1\n",
       "1    red     2   13.5     class2\n",
       "2   blue     3   15.3     class1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_class_mapping = {v:k for k,v in class_mapping.items()}\n",
    "df['classlabel']= df['classlabel'].map(inv_class_mapping)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, there is a convenient *LabelEncoder* class directly implemented in scikit-learn to achieve this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class_le = LabelEncoder()\n",
    "y = class_le.fit_transform(df['classlabel'].values)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the *fit_transform* method is just a shortcut for calling *fit* and *transform* separately, and we can use the *inverse_transform* method to transform the integer class labels back to their original string representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['class1', 'class2', 'class1'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_le.inverse_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing one-hot encoding on nominal features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, we used a simple dictionary-mapping approach to convert the ordinal *size* feature into integers. Since scikit-learn's estimators for classification treat class labels as categorical data that does not imply any order (nominal), we used the convenient *LabelEncoder* to encode the string labels into integers. It may appear that we could use a similar approach to transform the nominal *color* column of our dataset, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 10.1],\n",
       "       [2, 2, 13.5],\n",
       "       [0, 3, 15.3]], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[['color', 'size', 'price']].values\n",
    "color_le = LabelEncoder()\n",
    "X[:, 0] = color_le.fit_transform(X[:, 0])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After executing the preceding code, the first column of the NumPy array *X* now holds the new *color* values, which are encoded as follows: \n",
    "* blue = 0\n",
    "* green = 1\n",
    "* red = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we stop at this point and feed the array to our classifier, we will make one of the most common mistakes in dealing with categorical data. Can you spot the problem? Although the color values do not come in any particular order, a learning algorithm will now assume that *green* is larger than *blue*, and *red* is largen than *green*. Although this assumption is incorrect, the algorithm could still produce useful results. However, those results would not be optimal. \n",
    "\n",
    "A common workaround for this problem is to use a techinique called **one-hot encoding**. The idea behind this approach is to create a new dummy feature for each unique value in the nominal feature column. Here, we would convert the *color* feature into three new features: *blue*, *green* and *red*. Binary values can then be used to indicate the particular *color* of a sample; for example, a *blue* sample can be encoded as *blue=1,green=0,red=0*. To perform this transformation, we can use the *OneHotEncoder* that is implemented in the *scikit-learn.preprocessing* module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0. ,  1. ,  0. ,  1. , 10.1],\n",
       "       [ 0. ,  0. ,  1. ,  2. , 13.5],\n",
       "       [ 1. ,  0. ,  0. ,  3. , 15.3]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder(categorical_features=[0])\n",
    "ohe.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we initialized the *OneHotEncoder*, we defined the column position of the variable that we want to transform via the *categorical_features* parameter (note that *color* is the first column in the feature matrix *X*). By default, the *OneHotEncoder* returns a sparse matrix when we use the *transform* method, and we converted the sparse matrix representation into a regular (dense) NumPy array for the purpose of visualization via the *toarray* method. Sparse matrices are a more efficient way of storing large dataset and one that is supported by many scikit-learn function, which is especially useful if an array contains a lot of zeros. To omit the *toarray* step, we could alternatively initialize the encoder as *OneHotEncoder(..., sparse=False)* to return a regular NumPy array. \n",
    "\n",
    "An even more convenient way to create those dummy features via one-hot encoding is to use the *get_dummies* method implemented in pandas. Applied to a *DataFrame*, the *get_dummies* method will only convert string columns and leave all other columns unchanged: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>size</th>\n",
       "      <th>color_blue</th>\n",
       "      <th>color_green</th>\n",
       "      <th>color_red</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   price  size  color_blue  color_green  color_red\n",
       "0   10.1     1           0            1          0\n",
       "1   13.5     2           0            0          1\n",
       "2   15.3     3           1            0          0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.get_dummies(df[['price', 'color', 'size']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we are using one-hot encoding datasets, we have to keep in mind that it introduces multicollinearity, which can be an issue for certain methods (for instance, methods that require matrix inversion). If features are highly correlated, matrices are computationally difficult to invert, which can lead to numerically unstable estimates. To reduce the correlation among variables, we can simply remove one feature column from the one-hot encoded array. Note that we do not lose any important information by removing a feature column, though, for example, if we remove the column *color_blue*, the feature information is still preserved since if observe *color_green=0* and *color_red=0*, it implies that the observation must be *blue*. \n",
    "\n",
    "If we use the *get_dummies* function, we can drop the first column by passing *True* argument to the *drop_first* parameter, as shown in the following code example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>size</th>\n",
       "      <th>color_green</th>\n",
       "      <th>color_red</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   price  size  color_green  color_red\n",
       "0   10.1     1            1          0\n",
       "1   13.5     2            0          1\n",
       "2   15.3     3            0          0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.get_dummies(df[['price', 'color', 'size']], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *OneHotEncoder* does not have a parameter for column removal, but we can simply slice the one-hot encoded NumPy array as shown in the following snippet: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1. ,  0. ,  1. , 10.1],\n",
       "       [ 0. ,  1. ,  2. , 13.5],\n",
       "       [ 0. ,  0. ,  3. , 15.3]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe = OneHotEncoder(categorical_features=[0])\n",
    "ohe.fit_transform(X).toarray()[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partitioning a dataset into separate training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We briefly introduced the concept of partitioning a dataset into separate datasets for training and testing. Remember that comparing predictions to true labels in the test set can be understood as the unbiased performance evaluation of our model before we let it loose on the real world. In this section, we will prepare a new dataset, the **Wine** dataset. After we have preprocessed the dataset, we will explore different techniques for feature selection to reduce the dimensionality of the dataset. The Wine dataset is another open-source dataset, it consists of 178 wine samples with 13 features describing their different chemical properties. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the *pandas* library, we will directly read in the open source wine dataset from UCI machine learning repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class labels [1 2 3]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class label</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class label  Alcohol  Malic acid   Ash  Alcalinity of ash  Magnesium  \\\n",
       "0            1    14.23        1.71  2.43               15.6        127   \n",
       "1            1    13.20        1.78  2.14               11.2        100   \n",
       "2            1    13.16        2.36  2.67               18.6        101   \n",
       "3            1    14.37        1.95  2.50               16.8        113   \n",
       "4            1    13.24        2.59  2.87               21.0        118   \n",
       "\n",
       "   Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
       "0           2.80        3.06                  0.28             2.29   \n",
       "1           2.65        2.76                  0.26             1.28   \n",
       "2           2.80        3.24                  0.30             2.81   \n",
       "3           3.85        3.49                  0.24             2.18   \n",
       "4           2.80        2.69                  0.39             1.82   \n",
       "\n",
       "   Color intensity   Hue  OD280/OD315 of diluted wines  Proline  \n",
       "0             5.64  1.04                          3.92     1065  \n",
       "1             4.38  1.05                          3.40     1050  \n",
       "2             5.68  1.03                          3.17     1185  \n",
       "3             7.80  0.86                          3.45     1480  \n",
       "4             4.32  1.04                          2.93      735  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_wine = pd.read_csv('https://archive.ics.uci.edu/'\n",
    "                      'ml/machine-learning-databases/'\n",
    "                      'wine/wine.data', header=None)\n",
    "# To read from the local file\n",
    "# df_wine = pd.read_csv('wine.data', header=None)\n",
    "\n",
    "df_wine.columns = ['Class label', 'Alcohol', \n",
    "                   'Malic acid', 'Ash', \n",
    "                   'Alcalinity of ash', 'Magnesium', \n",
    "                   'Total phenols', 'Flavanoids', \n",
    "                   'Nonflavanoid phenols', \n",
    "                   'Proanthocyanins', \n",
    "                   'Color intensity', 'Hue', \n",
    "                   'OD280/OD315 of diluted wines', \n",
    "                   'Proline']\n",
    "print('Class labels', np.unique(df_wine['Class label']))\n",
    "df_wine.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The samples belong to one of three different classes, 1, 2, and 3, which refer to the three different types of graph grown in the same region in Italy but derived from different wine cultivars, as described in the dataset summary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A convenient way to randomly partition this dataset into separate test and training datasets is to use the *train_test_split* function from scikit-learn's *model_selection* submodule: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\n",
    "X_train, X_test, y_train, y_test =\\\n",
    "    train_test_split(X, y, test_size=0.3, \n",
    "                     random_state=0, \n",
    "                     stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we assigned the NumPy array representation of the feature columns 1-13 to the variable *X*; we assigned the class labels from the first column to the variable *y*. Then, we used the *train_test_split* function to randomly split *X* and *y* into separate training and test datasets. By setting *test_size=0.3*, we assigned 30 percent of the wine samples to *X_test* and *y_test*, and the remaining 70 percent of the samples were assigned to *X_train* and *y_train*, respectively. Proving the class label array *y* as an argument to *stratify* ensures that both training and test datasets have the same class proportions as the original dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are dividing a dataset into training and test datasets, we have to keep in mind that we are withholding valuable information that the learning algorithm could benefit from. Thus, we do not want to allocate too much information to the test set. However, the smaller the test set, the more inaccurate the estimation of the generalization error. Dividing a dataset into training and test sets is all about balancing this trade-off. In practice, the most commonly used splits are 60:40, 70:20, or 80:20, depending on the size of the initial dataset. However, for large datasets, 90:10 or 99:1 splits into training and test subsets are also common and appropriate. Instead of discarding the allocated test data after model training and evaluation, it is a common practice to retrain a classifier on the entire dataset as it can improve the predictive performance of the model. While this approach is generally recommended, it could lead to worse generalization performance if the dataset is small and the test set contains outliers, for example. Also, after refitting the model on the whole dataset, we do not have any independent data left to evaluate its performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bringing features onto the same scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature scaling** is a crucial step in our preprocessing pipeline that can easily be forgotten. Decision trees and random forests are two of the very few machine learning algorithms where we do not need to worry about feature scaling. Those algorithms are scale invariant. However, the majority of machine learning and optimization algorithms behave much faster if features are on the same scale, as we have seen before when we implemented the **gradient descent** optimization algorithm. \n",
    "\n",
    "The importance of feature scaling can be illustrated by a simple example. Let's assume that we have two features where one feature is measured on a scale from 1 to 10 and the second feature is measured on a scale from 1 to 100000, respectively. When we think of the squared error function in Adaline, it is intuitive to say that the algorithm will mostly be busy optimizing the weights according to the larger errors in the second feature. Another example is the **k-nearest neighbors (KNN)** algorithm with a Euclidean distance measure; the computed distances between samples will be dominated by the second feature axis. \n",
    "\n",
    "Now, there are two common approaches to bring different features onto the same scale: **normalization** and **standardization**. Those terms are often used quite loosely in different fields, and the meaning has to be derived from the context. Most often, normalization refers to the rescaling of the features to a range of [0, 1], which is a special case of **min-max scaling**. To normalize our data, we can simply apply the min-max scaling to each feature column, where the new value $x^{(i)}_{norm}$ of a sample $x^{(i)}$ can be calculated as follows: \n",
    "\n",
    "$$x^{(i)}_{norm} = \\frac{x^{(i)} - x_{min}}{x_{max} - x_{min}}$$\n",
    "\n",
    "Here, $x^{(i)}$ is a particular sample, $x_{min}$ is the smallest value in the feature column, and $x_{max}$ the largest value. \n",
    "\n",
    "The min-max scaling procedure is implemented in scikit-learn and can be used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mms = MinMaxScaler()\n",
    "X_train_norm = mms.fit_transform(X_train)\n",
    "X_test_norm = mms.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although normalization via min-max scaling is a commonly used technique that is useful when we need values in a bounded interval, stardardization can be more practical for many machine learning algorithms, especially for optimization algorithms such as gradient descent. The reason is that many linear models, such as the logistic regression and SVM, initialize the weights to 0 or small random values close to 0. Using standardization, we center the features columns at mean 0 with standard deviation 1 so that the feature columns takes the form of a normal distribution, which makes it easier to learn the weights. Furthermore, stardardization maintains useful information about outliers and makes the algorithm less sensitive to them in contrast to min-max scaling, which scales the data to a limited range of values.\n",
    "\n",
    "The procedure for standardization can be expressed by the following equation:\n",
    "\n",
    "$$x^{(i)}_{std} = \\frac{x^{(i)} - \\mu_x}{\\sigma_x}$$\n",
    "\n",
    "Here, $\\mu_x$ is the sample mean of a particular feature column and $\\sigma_x$ is the corresponding standard deviation. \n",
    "\n",
    "You can perform the standardization and normalization example by executing the following code examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standardized: [-1.46385011 -0.87831007 -0.29277002  0.29277002  0.87831007  1.46385011]\n",
      "normalized: [0.  0.2 0.4 0.6 0.8 1. ]\n"
     ]
    }
   ],
   "source": [
    "ex = np.array([0, 1, 2, 3, 4, 5])\n",
    "print('standardized:', (ex-ex.mean())/ex.std())\n",
    "print('normalized:', (ex - ex.min())/(ex.max()-ex.min()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the *MinMaxScaler* class, scikit-learn also implements a class for standardization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "stdsc = StandardScaler()\n",
    "X_train_std = stdsc.fit_transform(X_train)\n",
    "X_test_std = stdsc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, it is also important to highlight that we fit the *StandardScaler* class only once, on the training data, and use those parameters to transform the test set or any new data point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting meaningful features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we notice that a model performs much better on a training dataset than on the test dataset, this observation is a strong indicator of **overfitting**. As we discussed before, overfitting means the models fits the parameters too closely with regard to the particular observations in the training dataset, but does not generalize well to new data, and we say the model has a *high variance*. The reason for the overfitting is that our model is too complex for the given training data. Common solutions to reduce the generalization error are listed as follows:\n",
    "* Collect more training data\n",
    "* Introduce a penalty for complexity via regularization\n",
    "* Choose a simpler model with fewer parameters\n",
    "* Reduce the dimensionaly of the data\n",
    "Collecting more training data is often not applicable. Later we will learn about a useful technique to check whether more training data is helpful at all. In the following section, we will look at common ways to reduce overfitting by regularization and dimensionality reduction via feature selection, which leads to simpler models by requiring fewer parameters to be fitted to the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 and L2 regularization as penalties against model complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recall that **L2 regularization** is one approach to reduce the complexity of a model by penalizing large individual weights, where we defined the L2 norm of our weight vector *w* as follows:\n",
    "\n",
    "$$L2: ||w||^2_2 = \\sum^{m}_{j=1}w^2_j$$\n",
    "\n",
    "Another approach to reduce the model complexity is the related **L1 regularization**:\n",
    "\n",
    "$$L1: ||w||_1 = \\sum^{m}_{j=1}|w_j|$$\n",
    "\n",
    "Here, we simply replaced the square of the weights by the sum of the absolute values of the weights. In contrast to L2 regularization, L1 regularization usually yields sparse feature vector; most features weights will be zero. Sparsity can be useful in practice if we have a high-dimensional dataset with many features that are irrelevant, especially cases where we have more irrelevant dimensions than samples. In this sense, L1 regularization can be understood as a technique for feature selection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A geometric interpretation of L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the previous section, L2 regularization adds a penalty term to the cost function that effectively results in less extreme weigh values compared to a model trained with an unregularized cost function. To better understand how L1 regularization encourages sparsity, let's take a step back and take a look at a geometric interpretation of regularization. Let us plot the contours of a convex cost function for two weight coefficients, $w_1$ and $w_2$. Here, we will consider the **Sum of Squared Errors (SSE)** cost function that we used for Adaline, since it is spherical and easier to draw than the cost function of logistic regression; however, the same concepts apply to the latter. Remember that our goal is to find the combination of weight coefficients that minimize the cost function for the training data, as shown in the following figure (the point in the center of the ellipses): \n",
    "\n",
    "<img src='images/04_04.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can think of regularization as adding a penalty term to the cost function to encourage smaller weights; or in other words, we penalize large weights. \n",
    "\n",
    "Thus, by increasing the regularization strength via the regularization parameter $\\lambda$, we shrink the weights towards zero and decrease the dependence of our model on the training data. Let us illustrate this concept in the following figure for the L2 penalty term:\n",
    "\n",
    "<img src='images/04_05.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quadratic L2 regularization term is represented by the shaded ball. Here, our weight coefficients cannot exceed our regularization budget; the combination of the weight coefficients cannot fall outside the shaded area. On the other hand, we still want to minimize the cost function. Under the penalty constraint, our best effort is to choose the point where the L2 ball intersects with the contours of the unpenalized cost function. The larger the value of the regularization $\\lambda$ gets, the faster the penalized cost grows, which leads to a narrower L2 ball. For example, if we increase the regularization parameter towards infinity, the weight coefficients will become effectively zero, denoted by the center of the L2 ball. To summarize the main message of the example, our goal is to minimize the sum of the unpenalized cost plus the penalty term, which can be understood as adding bias and preferring a simpler model to reduce the variance in the absence of sufficient training data to fit the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse solutions with L1 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us discuss L1 regularization and sparsity. The main concept behind L1 regularization is similar to what we have discussed in the previous section. However, since the L1 penalty is the sum of the absolute weight coefficients (remember that the L2 term is quadratic), we can represent it as a diamond-shape budget, as shown in the following figure: \n",
    "\n",
    "<img src='images/04_06.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preceding figure, we can see that the contour of the cost function touches the L1 diamond at $w_1=0$. Since the contours of an L1 regularized system are sharp, it is more likely that the optimum; that is, the intersection between the ellipses of the cost function and the boundary of the L1 diamond, is located on the axes, which encourages sparsity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For regularized models in scikit-learn that support L1 regularization, we can simply set the *penalty* parameter to *'l1'* to obtain a sparse solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LogisticRegression(penalty='l1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applied to the standardized Wine data, the L1 regularized logistic regression would yield the following sparse solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 1.0\n",
      "Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(penalty='l1', C=1.0)\n",
    "lr.fit(X_train_std, y_train)\n",
    "print('Training accuracy:', lr.score(X_train_std, y_train))\n",
    "print('Test accuracy:', lr.score(X_test_std, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both training and test accuracies (both 100 percent) indicate that our model does a perfect job on both datasets. When we access the intercept terms via the *lr.intercept_* attribute, we can see that the array returns these values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.26345948, -1.21637638, -2.37039433])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, we fit the *LogisticRegression* object on a multiclass dataset, it uses the **One-versus-Rest (OvR)** approach by default, where the first intercept belongs to the model that fits class 1 versus class and 3, the second value is the intercept of the model that fits class 2 versus class 1 and 3, and the third value is the intercept of the model that fits class 3 versus class 1 and 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.24560149,  0.18083923,  0.74356908, -1.16123559,  0.        ,\n",
       "         0.        ,  1.16996853,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.54700546,  2.51062974],\n",
       "       [-1.5373154 , -0.38690538, -0.99465785,  0.36398878, -0.05964683,\n",
       "         0.        ,  0.66766912,  0.        ,  0.        , -1.93424417,\n",
       "         1.23440971,  0.        , -2.23195484],\n",
       "       [ 0.13574068,  0.16844252,  0.35727549,  0.        ,  0.        ,\n",
       "         0.        , -2.43803574,  0.        ,  0.        ,  1.56372255,\n",
       "        -0.81879496, -0.49238285,  0.        ]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight array that we accessed via the *lr.coef_* attribute contains three rows of weight coefficients, one weight vector for each class. Each row consists of 13 weights where each weight is multiplied by the respective feature in the 13-dimensional Wine dataset to calculate the net input. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result of L1 regularization, which serves as a method for feature selection, we just trained a model that is robust to the potentially irrelevant feature in this dataset. \n",
    "\n",
    "Stricly speaking, the weight vectors from the previous example are not necessarily sparse, though, because they contain more non-zero than zero entries. However, we could enforce sparsity (more zero entries) by further increasing the regularization strength, that is, choosing lower values for the *C* parameter. \n",
    "\n",
    "In the last example on regularization in this chapter, we will vary the regularization strength and plot the regularization path; the weight coefficients of the different features for different regularization strengths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmUAAAEOCAYAAADIVGAjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEABCGQAAQAAIZQAAAAEglAEAAATg/zG8xrhBc2AtAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "colors = ['blue', 'green', 'red', 'cyan', \n",
    "          'magenta', 'yellow', 'black', \n",
    "          'pink', 'lightgreen', 'lightblue', \n",
    "          'gray', 'indigo', 'orange']\n",
    "\n",
    "weights, params = [], []\n",
    "for c in np.arange(-4.0, 6.0):\n",
    "    lr = LogisticRegression(penalty='l1', C=10.**c, \n",
    "                            random_state=0)\n",
    "    lr.fit(X_train_std, y_train)\n",
    "    weights.append(lr.coef_[1])\n",
    "    params.append(10**c)\n",
    "    \n",
    "weights = np.array(weights)\n",
    "\n",
    "for column, color in zip(range(weights.shape[1]), colors):\n",
    "    plt.plot(params, weights[:, column], \n",
    "             label=df_wine.columns[column+1], \n",
    "             color=color)\n",
    "plt.axhline(0, color='black', linestyle='--', linewidth=3)\n",
    "plt.xlim([10**-5, 10**5])\n",
    "plt.ylabel('weight coefficient')\n",
    "plt.xlabel('C')\n",
    "plt.xscale('log')\n",
    "plt.legend(loc='upper left')\n",
    "ax.legend(loc='upper center', \n",
    "          bbox_to_anchor=(1.38, 1.03), \n",
    "          ncol=1, fancybox=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting plot provide us with further insights into the behavior of L1 regularization. As we can see, all feature weights will be zero if we penalize the model with strong regularization parameter ($C < 0.1$); *C* is the inverse of the regularization parameter $\\lambda$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential feature selection algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative way to reduce the complexity of the model and avoid overfitting is **dimensionality reduction** via feature selection, which is especially useful for unregularized models. There are two main categories of dimensionality reduction techniques: **feature selection** and **feature extraction**. Via feature selection, we select a subset of the original features, whereas in feature extraction, we derive information from the feature set and construct a new feature subspace. \n",
    "\n",
    "Sequential feature selection algorithms are a family of greedy search algorithms that are used to reduce an initial *d*-dimensional feature space to a *k*-dimensional feature subspace where $k<d$. The motivation behind feature selection algorithms is to automatically select a subset of features that are most relevant to the problem, to improve computational efficiency or reduce the generalization error of the model by removing irrelevant features or noise, which can be useful for algorithms that do not support regularization. \n",
    "\n",
    "A classic sequential feature selection algorithm is **Sequential Backward Selection (SBS)**, which aims to reduce the dimensionality of the initial feature subspace with a minimum decay in performance of the classifier to improve upon computational efficiency. In certain cases, SBS can even improve the predictive power of the model if a model suffers from overfitting. \n",
    "\n",
    "The idea behind the SBS algorithm is quite simple: SBS sequentially removes features from the full feature subset until the new feature subspace contains the desired number of features. In order to determine which feature is to be removed at each stage, we need to define the criterion function $J$ that we want to minimize. The criterion calculated by the criterion function can simply be the difference in performance of the classifier before and after the removal of a particular feature. Then, the feature to be removed at each stage can simply be defined as the feature that maximizes the criterion; or in more intuitive terms, at each stage we eliminate the feature that causes the least performance loss after removal. Based on the preceding definition of SBS, we can outline the algorithm in four simple steps: \n",
    "1. Initialize the algorithm with $k=d$, where d is the dimensionality of the full feature space $X_d$. \n",
    "2. Determine the feature $x^-$ that maximizes the criterion: $x^- = \\text{arg max} J(X_k - x)$, where $X \\in X_k$. \n",
    "3. Remove the feature $x$ from the feature set: $X_{k-1} = X_k = x^-; k = k - 1$. \n",
    "4. Terminate if $k$ equals the number of desired features; otherwise, go to step 2. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the SBS algorithm has not been implemented in scikit-learn yet. But since it is so simple, let us go ahead and implement it in Python from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class SBS():\n",
    "    def __init__(self, estimator, k_features, \n",
    "                 scoring=accuracy_score, test_size=0.25, \n",
    "                 random_state=1):\n",
    "        self.scoring = scoring\n",
    "        self.estimator = clone(estimator)\n",
    "        self.k_features = k_features\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = \\\n",
    "            train_test_split(X, y, test_size=self.test_size, \n",
    "                             random_state=self.random_state)\n",
    "        \n",
    "        dim = X_train.shape[1]\n",
    "        self.indices_ = tuple(range(dim))\n",
    "        self.subsets_ = [self.indices_]\n",
    "        score = self._calc_score(X_train, y_train, \n",
    "                                 X_test, y_test, \n",
    "                                 self.indices_)\n",
    "        self.scores_ = [score]\n",
    "        \n",
    "        while dim > self.k_features:\n",
    "            scores = []\n",
    "            subsets = []\n",
    "            \n",
    "            for p in combinations(self.indices_, r=dim-1):\n",
    "                score = self._calc_score(X_train, y_train, \n",
    "                                         X_test, y_test, p)\n",
    "                scores.append(score)\n",
    "                subsets.append(p)\n",
    "                \n",
    "            best = np.argmax(scores)\n",
    "            self.indices_ = subsets[best]\n",
    "            self.subsets_.append(self.indices_)\n",
    "            dim -= 1\n",
    "            \n",
    "            self.scores_.append(scores[best])\n",
    "        self.k_score_ = self.scores_[-1]\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X[:, self.indices_]\n",
    "    \n",
    "    def _calc_score(self, X_train, y_train, X_test, y_test, \n",
    "                    indices):\n",
    "        self.estimator.fit(X_train[:, indices], y_train)\n",
    "        y_pred = self.estimator.predict(X_test[:, indices])\n",
    "        score = self.scoring(y_test, y_pred)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preceding implementation, we defined the *k_features* parameter to specify the desired number of features we want to return. By default, we use the *accuracy_score* from scikit-learn to evaluate the performance of a model (an estimator for classification) on the feature subsets. Inside the *while* loop of the fit method, the feature subsets created by the *itertools.combination* function are evaluated and reduced until the feature subset has the desired dimensionality. In each iteration, the accuracy score of the best subset is collected in a list, *self.scores_*, based on the internally created test dataset *X_test*. We will use those scores later to evaluate the results. The column indices of the final feature subset are assigned to *self.indices_*, which we can use via the *transform* method to return a new data array with the selected feaure columns. Note that, instead of calculating the criterion explicity inside the *fit* method, we simply removed the feature that is not contained in the best performing feature subset. \n",
    "\n",
    "Now, let us see our SBS implementation in action using the KNN classifier from scikit-learn: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.SBS at 0x7f6ae98feef0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "sbs = SBS(knn, k_features=1)\n",
    "sbs.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although our SBS implementation already splits the dataset into a test and training dataset inside the *fit* function, we still fed the training dataset *X_train* to the algorithm. The SBS *fit* method will then create new training subsets for testing (validation) and training, which is why this test set is also called the **validation dataset**. This approach is necessary to prevent our *original* test set from becoming part of the training data. \n",
    "\n",
    "Remember that our SBS algorithm collects the scores of the best feature subset at each stage, so let us move on to the more exciting part of our implementation and plot the classification accuracy of the KNN classifier that was calculated on the validation dataset. The code is as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiISk5KFiIjE9H8yBiCNrBq8sgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "k_feat = [len(k) for k in sbs.subsets_]\n",
    "\n",
    "plt.plot(k_feat, sbs.scores_, marker='o')\n",
    "plt.ylim([0.7, 1.02])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Number of features')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the preceding figure, the accuracy of the KNN classifier improved on the validation dataset as we reduced the number of features, which is likely due to a decrease of the **curse of dimensionality** that we discussed in the context of the KNN algorithms. Also, we can see in the preceding plot that the classifier achieved 100 percent accuracy for $k={3, 7, 8, 9, 10, 11, 12}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To satisfy our own curiosity, let's see what the smallest feature subset ($k=3$) that yielded such a good performance on the validation dataset looks like: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Alcohol', 'Malic acid', 'OD280/OD315 of diluted wines'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "k3 = list(sbs.subsets_[10])\n",
    "print(df_wine.columns[1:][k3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the preceding code, we obtained the column indices of the tree-feature subset from the 10th position in the *sbs.subsets_* attribute and returned the corresponding feature names from the column-index of the pandas Wine *DataFrame*. \n",
    "\n",
    "Next let's evaluate the performance of the KNN classifier on the original test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.967741935483871\n",
      "Test accuracy: 0.9629629629629629\n"
     ]
    }
   ],
   "source": [
    "knn.fit(X_train_std, y_train)\n",
    "print('Training accuracy:', knn.score(X_train_std, y_train))\n",
    "print('Test accuracy:', knn.score(X_test_std, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preceding code section, we used the complete feature set and obtained approximately 97 percent accuracy on the training dataset and approximately 96 percent accuracy on the test, which indicates that our model already generalizes well to new data. Now, let us see the selected three-feature subset and see how well KNN performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9516129032258065\n",
      "Test accuracy: 0.9259259259259259\n"
     ]
    }
   ],
   "source": [
    "knn.fit(X_train_std[:, k3], y_train)\n",
    "print('Training accuracy:', knn.score(X_train_std[:, k3], y_train))\n",
    "print('Test accuracy:', knn.score(X_test_std[:, k3], y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using less than a quarter of the original features in the Wine dataset, the prediction accuracy on the test declined slightly. This may indicate that those three features do not provide less discriminatory information than the original dataset. However, we also have to keep in mind that the Wine dataset is a small dataset, which is very susceptible to randomness; that is, the way we split the dataset into training and test subsets, and how we split the training dataset further into a training and validation subset. \n",
    "\n",
    "While we did not increase the performance of the KNN model by reducing the number of features, we shrank the size of the dataset, which can be useful in real-world applications that may involve expensive data collection steps. Also, by substantially reducing the number of features, we obtain simpler models, which are easier to interpret. \n",
    "\n",
    "**Feature selection algorithms in scikit-learn**\n",
    "\n",
    "There are many more feature selection algorithms available via scikit-learn. Those include **recursive backward elimination** based on feature weights, tree-based methods to select features by importance, and univariate statistical tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing feature importance with random forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous chapter, you learned how to use L1 regularization to zero out irrelevant features via logistic regression, and use the SBS algorithm for feature selection and apply it to a KNN algorithm. Another useful approach to select relevant features from a dataset is to use a **random forest**, an ensemble technique. Using a random forest, we can measure the feature importance as the averaged impurity decrease computed from all decision trees in the forest, without making any assumptions about whether our data is linearly separable or not. Conveniently, the random forest implementation in scikit-learn already collects the feature importance values for us so that we can access them via the *feature_importances_* attribute after fitting a *RandomForestClassifier*. By executing the following code, we will now train a forest of 10000 three on the Wine dataset and rank the 13 features by their respective importance measures, remember that we do not need to use stardardized or normalized features in tree-based models: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1) Proline                        0.185453\n",
      " 2) Flavanoids                     0.174751\n",
      " 3) Color intensity                0.143920\n",
      " 4) OD280/OD315 of diluted wines   0.136162\n",
      " 5) Alcohol                        0.118529\n",
      " 6) Hue                            0.058739\n",
      " 7) Total phenols                  0.050872\n",
      " 8) Magnesium                      0.031357\n",
      " 9) Malic acid                     0.025648\n",
      "10) Proanthocyanins                0.025570\n",
      "11) Alcalinity of ash              0.022366\n",
      "12) Nonflavanoid phenols           0.013354\n",
      "13) Ash                            0.013279\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaIAAAEYCAYAAAAeWvJ8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEGoVFVEIIYRaRUUUQgihVv8fmeXH7RHfl+oAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "feat_labels = df_wine.columns[1:]\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=500, \n",
    "                                random_state=1)\n",
    "forest.fit(X_train, y_train)\n",
    "importances = forest.feature_importances_\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "for f in range(X_train.shape[1]):\n",
    "    print('%2d) %-*s %f' % (f+1, 30, feat_labels[indices[f]], \n",
    "                            importances[indices[f]]))\n",
    "plt.title('Feature Importance')\n",
    "\n",
    "plt.bar(range(X_train.shape[1]), \n",
    "        importances[indices], align='center')\n",
    "plt.xticks(range(X_train.shape[1]), \n",
    "           feat_labels[indices], rotation=90)\n",
    "plt.xlim([-1, X_train.shape[1]])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After executing the code, we created a plot that ranks the different features in the Wine dataset, by their relative importance; note that the feature importance values are normalized so that they sum up to 1.0.\n",
    "\n",
    "We can conclude that the proline and flavanoid levels, the color intensity, the OD280/OD315 diffraction, and the alcohol concentration are the most discriminative features in the dataset based on the average impurity decrease in the 500 decision trees, Interestingly, two of the top-ranked features in the plot are also in the tree-feature subset selection from the SBS algorithm that we implemented in the previous section (alcohol concentration and OD280/OD315 of diluted wines). However, as far as interpretability is concerned, the random forest technique comes with an important *gotcha* that is worth mentioning. If two or more features are highly correlated, one feature may be ranked very highly while the information of the other features(s) may not be fully captured. On the other hand, we do not need to be concerned about this problem if we are merely interested in the predictive performance of a model rather than the interpretation of feature importance values. \n",
    "\n",
    "To conclude this section about feature importance values and random forests, it is worth mentioning that scikit-learn also implements a *SelectFromModel* object that selects features based on a user-specified threshold after model fitting, which is useful if we want to use the *RandomForestClassifier* as a feature selector and intermediate step in a scikit-learn *Pipeline* object, which allows us to connect different preprocessing steps with an estimator. For example, we could set the *threshold* to 0.1 to reduce the dataset to five most important features using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples that meet this criterion: 124\n",
      " 1) Proline                        0.185453\n",
      " 2) Flavanoids                     0.174751\n",
      " 3) Color intensity                0.143920\n",
      " 4) OD280/OD315 of diluted wines   0.136162\n",
      " 5) Alcohol                        0.118529\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "sfm = SelectFromModel(forest, threshold=0.1, prefit=True)\n",
    "X_selected = sfm.transform(X_train)\n",
    "print('Number of samples that meet this criterion:', X_selected.shape[0])\n",
    "for f in range(X_selected.shape[1]):\n",
    "    print('%2d) %-*s %f' % (f+1, 30, feat_labels[indices[f]], \n",
    "                            importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started this chapter by looking at useful techniques to make sure that we handle missing data correctly. Before we feed data to a machine learning algorithm, we also have to make sure that we encode categorical variables correctly, and we have seen how we can map ordinal and nominal feature values to integer representations. \n",
    "\n",
    "Moveover, we briefly discussed L1 regularization, which can help us to avoid overfitting by reducing the complexity of a model. As an alternative approach to removing irrelevant features, we used a sequential feature selection algorithm to select meaning features from the dataset. \n",
    "\n",
    "In the next chapter, you will learn about yet another useful approach to dimensionality reduction: feature extraction. It allows us to compress features onto a lower-dimensional subpace; rather than removing features entirely as in feature selection. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
