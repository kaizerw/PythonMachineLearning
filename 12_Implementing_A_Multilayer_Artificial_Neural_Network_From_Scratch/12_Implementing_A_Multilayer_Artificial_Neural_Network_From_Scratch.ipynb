{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a Multilayer Artificial Neural Network from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may know, deep learning is getting a lot of attention from the press and is without any doubt the hottest topic in machine learning field. Deep learning can be understood as a set of algorithms that were developed to train artificial neural networks with many layers most efficiently. In this chapter, you will learn the basic concepts of artificial neural networks so that you will be well-equipped for the following chapters, which will introduce advanced Python-based deep learning libraries and **Deep Neural Networks (DNN)** architectures that are particularly well-suited for image and text-analyses.\n",
    "\n",
    "The topics that we will cover in this chapter are as follows:\n",
    "\n",
    "* Getting a conceptual understanding of multilayer neural networks\n",
    "* Implementing the fundamental backpropagation algorithm for neural network training from scratch\n",
    "* Training a basic multilayer neural network for image classfication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling complex functions with artificial neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the beginning of this book, we started our journey through machine learning algorithms with artificial neurons. Artificial neurons represent the building blocks of the multilayer artificial neural networks that we will discuss in this chapter. The basic concept behind artificial neural networks was built upon hypothesis and models of how the human brain works to solve complex problem tasks. Although artificial neural networks have gained a lot of popularity in recent years, early studies of neural networks go back to the 1940s when Warren McCulloch and Walter Pitt first described how neurons could work. \n",
    "\n",
    "However, in the decades that followed the first implementation of the **McCulloch-Pitt neuron** model - Rosenblatt's perceptron in the 1950s, many researchers and machine learning practitioners slowly began to lose interest in neural networks since no one had a good solution for training a neural network with multiple layers. Eventually, interest in neural networks was rekindled in 1986 when D.E. Rumelhart, G.E. Hinton, and R.J. Williams were involved in the (red)discovery and popularization of the backpropagation algorithm to train neural networks more efficiently, which we will discuss in more detail later in this chapter. Readers who are interested in the history of **Artificial Intelligence (AI)**, machine learning, and neural networks are also encouraged to read the Wikipedia article on *AI winter*, which are periods of time where a large portion of the research community lost interest in the study of neural networks. \n",
    "\n",
    "However, neural networks have never been as popular as they are today, thanks to the many major breakthroughs that have been made in the previous decade, which resulted in what we now call deep learning algorithms and architectures - neural networks that are composed of many layers. Neural networks are a hot topic not only in academic research but also in big technology companies such as Facebook, Microsoft, and Google, who invest heavily in artificial neural networks and deep learning research. As of today, complex neural networks powered by deep learning algorithms are considered state of the art when it comes to complex problem solving such as image and voice recognition. Popular examples of the products in our everyday life that are powered by deep learning are Google's image search and Google Translate - an application for smartphones than can automatically recognize text in images for real-time translation into more than 20 languages. \n",
    "\n",
    "Many exciting applications of DNNs have been developed at major tech companies and the pharmaceutical industry as listed in the following, non-comprehensive list of examples: \n",
    "\n",
    "* Facebook's DeepFace for tagging images\n",
    "* Baidu's DeepSpeech, which is able to handle voice queries in Mandarin\n",
    "* Google's new language translation service\n",
    "* Novel techniques for drug discovery and toxicity prediction\n",
    "* A mobile application that can detect skin cancer with an accuracy similar to professionally trained dermatologists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-layer neural network recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter is all about multilayer neural networks, how they work, and how to train them to solve complex problems. However, before we dig deeper into a particular multilayer neural network architecture, let's briefly reiterate some of the concepts of single-layer neural networks that we introduced in the beginning of this book, namely, the **ADAptive LInear NEuron (Adaline)** algorithm, which is shown is the following figure:\n",
    "\n",
    "<img src='images/12_01.png'>\n",
    "\n",
    "Previously, we implemented the Adaline algorithm to perform binary classification, and we used the gradient descent optimization algorithm to learn the weight coefficients of the model. In every epoch (pass over the training set), we updated the weight vector $w$ using the following update rule: \n",
    "\n",
    "$$w := w + \\Delta w, \\text{where} \\Delta w = \\eta J(w)$$\n",
    "\n",
    "In other words, we computed the gradient based on the whole training set and updated the weights of the model by taking a step into the opposite direction of the gradient $J(w)$. In order to find the optimal weights of the model, we optimized an objective function that we defined as the **Sum of Squared Errors (SSE)** cost function $J(w)$. Furthermore, we multiplied the gradient by a factor, the learning rate $\\eta$, which we had to choose carefully to balance the speed of learning against the risk of overshooting the global minimum of the cost function. \n",
    "\n",
    "In gradient descent optimization, we updated all weights simultaneously after each epoch, and we defined the partial derivative for each weight $w_j$ in the weight vector $w$ as follows:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial w_j}J(w) = -\\sum_i{y^{(i)} - a^{(i)}}x^{(i)}_j$$\n",
    "\n",
    "Here, $y^{(i)}$ is the target class label of a particular sample $x^{(i)}$, and $a^{(i)}$ is the activation of the neuron, which is a linear function in the special case of Adaline. Furthermore, we defined the activation function $\\phi$ as follows:\n",
    "\n",
    "$$\\phi(z) = z = a$$\n",
    "\n",
    "Here, the net input $z$ is a linear combination of the weights that are connecting the input to the output layer:\n",
    "\n",
    "$$z = \\sum_j w_jx_j = w^Tx$$\n",
    "\n",
    "While we used the activation $\\phi(z)$ to compute the gradient update, we implemented a threshold function to squash the continuous valued output into binary class labels for prediction:\n",
    "\n",
    "$$Å· = 1 \\, \\text{if} \\, g(z) \\ge 0; -1 \\, \\text{otherwise}$$\n",
    "\n",
    "Note that although Adaline consists of two layers, one input layer and one output layer, it is called single-layer network because of its single link between the input and output layers. \n",
    "\n",
    "Also, we learned about a certain trick to accelerate the model training, the so-called **stochastic gradient descent** optimization. Stochastic gradient descent approximates the cost from a single training sample (online learning) or a small subset of training samples (mini-batch learning). We will make use of this concept later in this chapter when we implement and train a multilayer perceptron. Apart from faster learning - due to the more frequent weight updates compared to gradient descent - its noisy nature is also regarded as beneficial when training multilayer neural networks with non-linear activation functions, which do not have a convex cost function. Here, the added noise can help to escape local cost minima, but we will discuss this topic in more detail later in this chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing the multilayer neural network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will learn how to connect multiple single neurons to a multilayer feedforward neural network; this special type of *fully connected* network is also called **Multilayer Perceptron (MLP)**. The following picture illustrates the concept of an MLP consisting of three layers:\n",
    "\n",
    "<img src='images/12_02.png'>\n",
    "\n",
    "The MLP depicted in the preceding figure has one input layer, one hidden layer, and one output layer. The units in the hidden layear are fully connected to the input layer. If such a network has more than one hidden layer, we also call it a **deep artificial neural network**. \n",
    "\n",
    "We can add an arbitrary number of hidden layers to the MLP to create deeper network architectures. Practically, we can think of the number of layers and units in a neural network as additional hyperparameters that we want to optimize for a given problem task using cross-validation techniques. \n",
    "\n",
    "However, the error gradients that we will calculate later via backpropagation will become increasingly small as more layers are added to a network. This vanishing gradient problem makes the model learning more challenging. Therefore, special algorithms have been developed to help train such deep neural network structures; this is known as **deep learning**. \n",
    "\n",
    "As shown in the preceding figure, we denote the *i*th activation unit in the *l*th layer as $a_i^{(l)}$. To make the math and code implementations a bit more intuitive, we will not use numerical indices to refer to layers, but we will use the *in* superscript for the input layer, the *h* superscript for the hidden layer, and the *out* superscript for the output layer. For instance, $a_i^{(in)}$ refers to the *i*th value in the input layer, $a_i^{(h)}$ refers to the *i*th unit in the hidden layer, adn $a_i^{(out)}$ refers to the *i*th unit in the output layer. Here, the activation unit $a_0^{(in)}$ and $a_0^{(h)}$ are the **bias units**, which we set equal to 1. The activation of the units in the input layer is just its input plus the unit bias. \n",
    "\n",
    "Later in this chapter, we will implement the multilayer perceptron using separate vectors for the bias unit, which makes the code implementation more efficient and easier to read. This concept is also used by TensorFlow, a deep learning library that we will introduce later. However, the mathematical equations that will follow, would appear more complex or convoluted if we had to work with additional variables for the bias. However, note that the computation via appending 1s to the input vector (as shown previously) and using a weight variable as bias is exactly the same as operating with separate bias vectors; it is merely a different convention. \n",
    "\n",
    "Each unit in layer *l* is connected to all units in layer $l+1$ vi a weight coefficient. For example, the connection between the $k$th unit in the layer $l$ to the $j$th unit in layer $l+1$ will be written as $w_{k,j}^{l}$. Referring back to the previous figure, we denote the weight matrix that connects the input to the hidden layer as $W^{(h)}$, and write the matrix that connects the hidden layer to the output layer as $W^{(out)}$. \n",
    "\n",
    "While one unit in the output layer would suffice for a binary classification task, we saw a more general form of neural network in the preceding figure, which allows us to perform multiclass classification via a generalization of the **One-versus-all (OvA)** technique. To better understand how this works, remember the one-hot representation of categorical variables. For example, we can encode the three class labels in the familiar Iris dataset (0=setosa, 1=versicolor, 2=virginica) as follows:\n",
    "\n",
    "$$0 = [1, 0, 0], 1 = [0, 1, 0], 2 = [0, 0, 1]$$\n",
    "\n",
    "This one-hot vector representation allows us to tackle classification tasks with an arbitrary number of unique class labels present in the training set. \n",
    "\n",
    "If you are new to neural network representations, the indexing notation (subscripts and superscripts) may look a little bit confusing at first. What may seem overly complicated at first will make much more sense in later section when we vectorize the neural network representation. As introduced earlier, we summarize the weights that connect the input and hidden layers by a matrix $w^{(h)} \\in \\mathbb{R}^{m \\times d}$, where $d$ is te number of hidden units and $m$ is the number of input units including the bias unit. Since it is important to internalize this notation to follow the concepts later in this chapter, let's summarize what we have just learned in a descriptive illustration of a simplified 3-4-3 multilayer perceptron: \n",
    "\n",
    "<img src='images/12_03.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activating a neural network via forward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will describe the process of **forward propagation** to calculate the output of an MLP model. To understand how it fits into the context of learning an MLP model, let's summarize the MLP learning procedure in three simple steps: \n",
    "\n",
    "1. Starting at the input layer, we forward propagate the patterns of the training data through the network to generate an output. \n",
    "2. Based on the network's output, we calculate the error that we want to minimize using a cost function that we will describe later. \n",
    "3. We backpropagate the error, find its derivative with respect to each weight in the network, and update the model.\n",
    "\n",
    "Finally, after we repeat these three steps for multiple epochs and learn the weights of the MLP, we use forward backpropagation to calculate the network output and apply a threshold function to obtain the predicted class labels in the one-hot representation, which we described in the previous section.\n",
    "\n",
    "Now, let's walk through the individual steps of forward propagation to generate an output from the patters in the training data. Since each unit in the hidden layer is connected to all units in the input layers, we first calculate the activation unit of the hidden layer $a_l^{(h)}$ as follows:\n",
    "\n",
    "$$z_1^{(h)} = a_0^{(in)}w_{0,1}^{(h)} + a_1^{(in)}w_{1,1}^{(h)} + \\ldots + a_m^{(in)}w_{m,1}^{(h)}$$\n",
    "\n",
    "$$a_1^{(h)} = \\phi(z_1^{(h)})$$\n",
    "\n",
    "Here, $z_1^{(h)}$ is the net input and $\\phi$ is the activation function, which has to be differentiable to learn the weights that connect the neurons using a gradient-based approach. To be able to solve complex problem such as image classification, we need non-linear activation functions in our MLP model, for example, the sigmoid (logistic) activation function that we remember from the section about logistic regression:\n",
    "\n",
    "$$\\phi(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "As we can remember, the sigmoid function is an S-shaped curve that maps the net input $z$ onto a logistic distribution in the range 0 to 1, which cuts the $y$-axis at $z=0$, as shown in the following graph: \n",
    "\n",
    "<img src='images/12_04.png'>\n",
    "\n",
    "MLP is a typical example of a feedforward artificial neural network. The term **feedforward** refers to the fact that each layer serves as the input to the next layer without loops, in contrast to recurrent neural network - an architecture that we will discuss later in this chapter and discuss in more detail in the last chapter of this book. The term *multilayer perceptron* may sound a little bit confusing since the artificial neurons in this network architecture are typically sigmoid units, not perceptrons. Intuitively, we can think of the neurons in the MLP as logistic regression units that return values in the continuous range between 0 and 1. \n",
    "\n",
    "For purposes of code efficiency and readability, we will now write the activation in a more compact form using the concepts of basic linear algebra, which will allow us to vectorize our code implementation via NumPy rather than writing multiple nested and computationally expensive Python *for* loops: \n",
    "\n",
    "$$z^{(h)} = a^{(in)}W^{(h)}$$\n",
    "\n",
    "$$a^{(h)} = \\phi(z^{(h)})$$\n",
    "\n",
    "Here, $a^{(in)}$ is our $l \\times m$ dimensional feature vector of a sample $x^{(in)}$ plus a bias unit. $W^{(h)}$ is an $m \\times d$ dimensional weight matrix where $d$ is the number of units in the hidden layer. After matrix-vector multiplication, we obtain the $l \\times d$ dimensional net input $z^{(h)}$ to calculate the activation $a^{(h)}$ (where $a^{(h)} \\in \\mathbb{R}^{l \\times d}$). Furthermore, we can generalize this computation to all $n$ samples in the training set: \n",
    "\n",
    "$$Z^{(h)} = A^{(in)}W^{(h)}$$\n",
    "\n",
    "Here, $ A^{(in)}$ is not an $n \\times m$ matrix, and the matrix-matrix multiplication will result in an $n \\times d$ dimensional net input matrix $Z^{(h)}$. Finally, we apply the activation function $\\phi$ to each value in the net input matrix to get the $n \\times d$ activation matrix $A^{(h)}$ for the next layer (here, the output layer):\n",
    "\n",
    "$$a^{(h)} = \\phi(z^{(h)})$$\n",
    "\n",
    "Similarly, we can write the activation of the output layer in vectorized form for multiple samples: \n",
    "\n",
    "$$Z^{(out)} = A^{(h)}W^{(out)}$$\n",
    "\n",
    "Here, we multiply the $d \\times t$ matrix $W^{(out)}$ ($t$ is the number of output units) by the $n \\times d$ dimensional matrix $A^{(h)}$ to obtain the $n \\times t$ dimensional matrix $Z^{(out)}$ (the columns in this matrix represent the output for each sample). \n",
    "\n",
    "Lastly, we apply the sigmoid activation function to obtain the continuous valued output of our network:\n",
    "\n",
    "$$a^{(out)} = \\phi(z^{(out)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying handwritten digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, we covered a lot of the theory around neural networks, which can be a little bit overwhelming if you are new to this topic. Before we continue with the discussion of the algorithm for learning the weights of the MLP model, backpropagation, let's take a short break from the theory and see a neural network in action. \n",
    "\n",
    "In this section, we will implement and train our first multilayer neural network to classify handwritten digits from the popular **Mixed National Institute of Standards and Technology (MNIST)** dataset that has been constructed by Yann LeCun and others, and servers as a popular benchmark dataset for machine learning algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining the MNIST dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset is publicly available at http://yann.lecun.com/exdb/mnist/ and consists of the following four parts: \n",
    "\n",
    "* Training set images: *train-images-idx3-ubyte.gz* (60,000 samples)\n",
    "* Training set labels: *train-labels-idx1-ubyte.gz* (60,000 labels)\n",
    "* Test set images: *t10k-images-idx3-ubyte.gz* (10,000 samples)\n",
    "* Test set labels: *t10k-labels-idx1-ubyte.gz* (10,000 labels)\n",
    "\n",
    "The MNIST dataset was constructed from two datasets from the US **National Institute of Standards and Technology (NIST)**. The training dataset consists of handwritten digits from 250 different people, 50 percent high school students, and 50 percent employees from the Census Bureau. Note that the test set contains handwritten digits from different people following the same split. \n",
    "\n",
    "The images are stored in byte format, and we will read them into NumPy arrays that we will use to train and test our MLP implementation. In order to do that, we will define the following helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import struct\n",
    "import numpy as np\n",
    "\n",
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"Load MNIST data from 'path'\"\"\"\n",
    "    labels_path = os.path.join(path, '%s-labels-idx1-ubyte' % kind)\n",
    "    images_path = os.path.join(path, '%s-images-idx3-ubyte' % kind)\n",
    "    \n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II', lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath, dtype=np.uint8)\n",
    "        \n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", imgpath.read(16))\n",
    "        images = np.fromfile(imgpath, dtype=np.uint8).reshape(len(labels), 784)\n",
    "        images = ((images / 255.) - .5) * 2\n",
    "    \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *load_mnist* function returns two arrays, the first being an $n \\times m$ dimensional NumPy array (*images*), where $n$ is the number of samples and $m$ is the number of features (here, pixels). The training dataset consists of 60,000 training digits and the test set contains 10,000 samples, respectively. The images in the MNIST dataset consist of 28 x 28 pixels, and each pixel is represented by a gray scale intensity value. Here, we unroll the 28 x 28 pixels into one-dimensional row vectors, which represent the rows in our *images* array (784 per row or image). The second array (*labels*) returned by the *load_mnist* function contains the corresponding target variable, the class labels (integers 0-9) of the handwritten digits. \n",
    "\n",
    "We normalized the pixels values in the MNIST to the range -1 to 1 (originally 0 to 255). The reason behind this is that gradient-based optimization is much more stable under these conditions. Note that we scaled the images on a pixel-by-pixel basis, which is different from the feature scaling approach that we took in previous chapters. Previously, we derived scaling paramters from the training set and used these to scale each column in the training set and test set. However, when working with image pixels, centering them at zero and rescaling them to a [-1, 1] range is also common and usually works in practice. \n",
    "\n",
    "Another recently developed trick to improve convergence in gradient-based optimization through input scaling is batch normalization, which is an advanced topic that we will not cover in this book. \n",
    "\n",
    "By executing the following code, we will now load the 60,000 training instances as well as the 10,000 test samples from the local directories where we unzipped the MNIST dataset (in the following code snippet, it is assumed that the downloaded MNIST files were unzipped to the same directory in which this code was executed): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 60000, columns: 784\n",
      "Rows: 10000, columns: 784\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = load_mnist('', kind='train')\n",
    "print('Rows: %d, columns: %d' % (X_train.shape[0], X_train.shape[1]))\n",
    "\n",
    "X_test, y_test = load_mnist('', kind='t10k')\n",
    "print('Rows: %d, columns: %d' % (X_test.shape[0], X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea of how those images in MNIST look, let's visualize examples of the digits 0-9 after reshaping the 784-pixel vectors from our feature matrix into the original 28 x 28 image that we can plot via Matplotlib's *imshow* function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAADRCAYAAACZ6CZ9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=5, sharex=True, sharey=True)\n",
    "\n",
    "ax = ax.flatten()\n",
    "for i in range(10):\n",
    "    img = X_train[y_train==i][0].reshape(28, 28)\n",
    "    ax[i].imshow(img, cmap='Greys')\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should now see a plot of the 2 x 5 subfigures showing a representative image of each unique digit. \n",
    "\n",
    "In addition, let's also plot multiple examples of the same digit to see how different the handwritting really is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEYCAYAAAC6MEqvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCCEMTJKAEEIYmCQBIYQwsP8HXD495LQRVRQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 25 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=5, ncols=5, sharex=True, sharey=True)\n",
    "\n",
    "ax = ax.flatten()\n",
    "for i in range(25):\n",
    "    img = X_train[y_train==7][i].reshape(28, 28)\n",
    "    ax[i].imshow(img, cmap='Greys')\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After executing the code, we should now see the first 25 variantes of the digit 7. \n",
    "\n",
    "After we have gone through all the previous steps, it is a good idea to save the scaled images in a format that we can load more quickly into a new Python session to avoid the overhead of reading in and processing the data again. When we are working with NumPy arrays, an efficient yet most convenient method to save multidimensional arrays to disk is NumPy's *savez* function. \n",
    "\n",
    "In short, the *savez* function is analogous to Python's *pickle* module that we used before, but optimized for storing NumPy arrays. The *savez* function produces zipped archives of our data, producing *.npz* files that contain files in the *.npz* format. Further, instead of using *savez*, we will use *savez_compressed*, which uses the same syntax as *savez*, but further compresses the output file down to substantially smaller file sizes. The following code snippet will save both the training and test datasets to the archive file *'mnist_scaled.npz'*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.savez_compressed('mnist_scaled.npz', \n",
    "                    X_train=X_train, \n",
    "                    y_train=y_train, \n",
    "                    X_test=X_test, \n",
    "                    y_test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we created the *.npz* files, we can load the preprocessed MNIST images arrays using NumPy's *load* function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = np.load('mnist_scaled.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *mnist* variable now references to an object that can access the four data arrays as we provided them keywork arguments to the *savez_compressed* function, which are listed under the files attribute list of the *mnist* object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['X_train', 'y_train', 'X_test', 'y_test']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, to load the training data into our current Python session, we will access the *'X_train'* array as follows (similar to a Python dictionary):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = mnist['X_train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a list comprehension, we can retrieve all four data arrays as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = [mnist[f] for f in mnist.files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that while the preceding *np.savez_compressed* and *np.load* examples are not essential for executing the code at this chapter, it serves as a demonstration of how to save and load NumPy arrays conveniently and efficiently. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a multilayer perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In subsection, we will now implement the code of an MLP with one input, one hidden, and one output layers to classify the images in the MNIST dataset. I have tried to keep the code as simple as possible. However, it may seem a little bit complicated at first.\n",
    "\n",
    "The code will contain parts that we have not talked about yet, such as the backpropagation algorithm, but most of the code should look familiar to you based on the Adaline implementation, and the discussion of forward propagation in earlier sections. \n",
    "\n",
    "DO not worry if not all the code makes immediate sense to you; we will follow up on certain parts later in this chapter. However, going over the code at this stage can make it easier to follow the theory later. \n",
    "\n",
    "The following is the implementation of a multilayer perceptron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "class NeuralNetMLP(object):\n",
    "    \"\"\"Feedforward neural network / Multi-layer perceptron classifier\n",
    "    \n",
    "    Parameters\n",
    "    ---------------\n",
    "    n_hidden : int (default: 30)\n",
    "        Number of hidden units\n",
    "    l2 : float (default: 0.)\n",
    "        Lambda value for L2-regularization.\n",
    "        No regularization if l2=0. (default)\n",
    "    epochs : int (default: 100)\n",
    "        Number of passes over the training set\n",
    "    eta : float (default: 0.001)\n",
    "        Learning rate\n",
    "    shuffle : bool (default: True)\n",
    "        Shuffles training data every epoch\n",
    "        if True to prevent cycles\n",
    "    minibatch_size : int (default: 1)\n",
    "        Number of training samples per minibatch\n",
    "    seed : int (default: None)\n",
    "        Random seed for initializing weights and shuffling\n",
    "        \n",
    "    Attributes\n",
    "    ---------------\n",
    "    eval_ : dict\n",
    "        Dictionary collecting the cost, training accuracy, \n",
    "        and validation accuracy for each epoch during training\n",
    "    \"\"\"\n",
    "    def __init__(self, n_hidden=30, l2=0., epochs=100, \n",
    "                 eta=0.001, shuffle=True, minibatch_size=1, \n",
    "                 seed=None):\n",
    "        self.random = np.random.RandomState(seed)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l2 = l2\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatch_size = minibatch_size\n",
    "        \n",
    "    def _onehot(self, y, n_classes):\n",
    "        \"\"\"Encode labels into one-hot representation\n",
    "        \n",
    "        Parameters\n",
    "        ---------------\n",
    "        y : array, shape = [n_samples]\n",
    "            Target values\n",
    "            \n",
    "        Returns\n",
    "        ---------------\n",
    "        onehot : array = shape = (n_samples, n_labels)        \n",
    "        \"\"\"\n",
    "        onehot = np.zeros((n_classes, y.shape[0]))\n",
    "        for idx, val in enumerate(y.astype(int)):\n",
    "            onehot[val, idx] = 1.\n",
    "        return onehot.T\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        \"\"\"Compute logistic function (sigmoid)\"\"\"\n",
    "        return 1. / (1. + np.exp(-np.clip(z, -250, 250)))\n",
    "    \n",
    "    def _forward(self, X):\n",
    "        \"\"\"Compute forward propagation step\"\"\"\n",
    "        \n",
    "        # step 1: net input of hidden layer\n",
    "        # [n_samples, n_features] dot [n_features, n_hidden]\n",
    "        # -> [n_samples, n_hidden]\n",
    "        z_h = np.dot(X, self.w_h) + self.b_h\n",
    "        \n",
    "        # step 2: activation of hidden layer\n",
    "        a_h = self._sigmoid(z_h)\n",
    "        \n",
    "        # step 3: net input of output layer\n",
    "        # [n_samples, n_hidden] dot [n_hidden, n_classlabels]\n",
    "        # -> [n_samples, n_classlabels]\n",
    "        z_out = np.dot(a_h, self.w_out) + self.b_out\n",
    "        \n",
    "        # step 4: activation output layer\n",
    "        a_out = self._sigmoid(z_out)\n",
    "        \n",
    "        return z_h, a_h, z_out, a_out\n",
    "    \n",
    "    def _compute_cost(self, y_enc, output):\n",
    "        \"\"\"Compute cost function\n",
    "        \n",
    "        Parameters\n",
    "        ---------------\n",
    "        y_enc : array, shape = (n_samples, n_labels)\n",
    "            one-hot encoded class labels\n",
    "        output : array, shape = [n_samples, n_output_units]\n",
    "            Activation of the output layer (forward propagation)\n",
    "        \n",
    "        Returns \n",
    "        ---------------\n",
    "        cost : float\n",
    "            Regularized cost\n",
    "        \"\"\"\n",
    "        L2_term = (self.l2 * (np.sum(self.w_h ** 2.) + np.sum(self.w_out ** 2.)))\n",
    "        term1 = -y_enc * (np.log(output))\n",
    "        term2 = (1. - y_enc) * np.log(1. - output)\n",
    "        cost = np.sum(term1 - term2) + L2_term\n",
    "        return cost\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\n",
    "        \n",
    "        Parameters\n",
    "        ---------------\n",
    "        X : array, shape = [n_samples, n_features]\n",
    "            Input layer with original features\n",
    "        \n",
    "        Returns \n",
    "        ---------------\n",
    "        y_pred : array, shape = [n_samples]\n",
    "            Predicted class labels\n",
    "        \"\"\"\n",
    "        z_h, a_h, z_out, a_out = self._forward(X)\n",
    "        y_pred = np.argmax(z_out, axis=1)\n",
    "        return y_pred\n",
    "    \n",
    "    def fit(self, X_train, y_train, X_valid, y_valid):\n",
    "        \"\"\"Learn weights from training data\n",
    "        \n",
    "        Parameters\n",
    "        ---------------\n",
    "        X_train : array, shape = [n_samples, n_features]\n",
    "            Input layer with original features\n",
    "        y_train : array, shape = [n_samples]\n",
    "            Target class labels\n",
    "        X_valid : array, shape = [n_samples, n_features]\n",
    "            Sample features for validation during training\n",
    "        y_valid : array, shape = [n_samples]\n",
    "            Simple labels for validation during training\n",
    "            \n",
    "        Returns\n",
    "        ---------------\n",
    "        self\n",
    "        \"\"\"\n",
    "        # no. of class labels\n",
    "        n_output = np.unique(y_train).shape[0]\n",
    "        n_features = X_train.shape[1]\n",
    "        \n",
    "        # Weight initialization\n",
    "        \n",
    "        # weights for input -> hidden\n",
    "        self.b_h = np.zeros(self.n_hidden)\n",
    "        self.w_h = np.random.normal(loc=0.0, scale=0.1, \n",
    "                                    size=(n_features, \n",
    "                                          self.n_hidden))\n",
    "        \n",
    "        # weights for hidden -> output\n",
    "        self.b_out = np.zeros(n_output)\n",
    "        self.w_out = self.random.normal(loc=0.0, scale=0.1, \n",
    "                                        size=(self.n_hidden, \n",
    "                                              n_output))\n",
    "\n",
    "        # for progr. format.\n",
    "        epoch_strlen = len(str(self.epochs))\n",
    "        self.eval_ = {'cost': [], \n",
    "                      'train_acc': [], \n",
    "                      'valid_acc': []}\n",
    "        \n",
    "        y_train_enc = self._onehot(y_train, n_output)\n",
    "        \n",
    "        # iterate over training epochs\n",
    "        for i in range(self.epochs):\n",
    "            # iterate over minibatches\n",
    "            indices = np.arange(X_train.shape[0])\n",
    "            \n",
    "            if self.shuffle:\n",
    "                self.random.shuffle(indices)\n",
    "                \n",
    "            for start_idx in range(0, indices.shape[0] - self.minibatch_size + 1, self.minibatch_size):\n",
    "                batch_idx = indices[start_idx:start_idx + self.minibatch_size]\n",
    "                \n",
    "                # forward propagation\n",
    "                z_h, a_h, z_out, a_out = self._forward(X_train[batch_idx])\n",
    "                \n",
    "                # Backpropagation\n",
    "                \n",
    "                # [n_samples, n_classlabels]\n",
    "                sigma_out = a_out - y_train_enc[batch_idx]\n",
    "                \n",
    "                # [n_samples, n_hidden]\n",
    "                sigmoid_derivative_h = a_h * (1. - a_h)\n",
    "                \n",
    "                # [n_samples, n_classlabels] dot [n_classlabels, n_hidden]\n",
    "                # -> [n_samples, n_hidden]\n",
    "                sigma_h = (np.dot(sigma_out, self.w_out.T) * sigmoid_derivative_h)\n",
    "                \n",
    "                # [n_features, n_samples] dot [n_samples, n_hidden]\n",
    "                # -> [n_features, n_hidden]\n",
    "                grad_w_h = np.dot(X_train[batch_idx].T, sigma_h)\n",
    "                grad_b_h = np.sum(sigma_h, axis=0)\n",
    "                \n",
    "                # [n_hidden, n_samples] dot [n_samples, n_classlabels]\n",
    "                # -> [n_hidden, n_classlabels]\n",
    "                grad_w_out = np.dot(a_h.T, sigma_out)\n",
    "                grad_b_out = np.sum(sigma_out, axis=0)\n",
    "                \n",
    "                # Regularization and weight updates\n",
    "                delta_w_h = (grad_w_h + self.l2*self.w_h)\n",
    "                delta_b_h = grad_b_h # bias is not regularized\n",
    "                self.w_h -= self.eta * delta_w_h\n",
    "                self.b_h -= self.eta * delta_b_h\n",
    "                \n",
    "                delta_w_out = (grad_w_out + self.l2*self.w_out)\n",
    "                delta_b_out = grad_b_out # bias is not regularized\n",
    "                self.w_out -= self.eta * delta_w_out\n",
    "                self.b_out -= self.eta * delta_b_out\n",
    "            \n",
    "            # Evaluation\n",
    "            \n",
    "            # Evaluation after each epoch during training\n",
    "            z_h, a_h, z_out, a_out = self._forward(X_train)\n",
    "            \n",
    "            cost = self._compute_cost(y_enc=y_train_enc, \n",
    "                                      output=a_out)\n",
    "            \n",
    "            y_train_pred = self.predict(X_train)\n",
    "            y_valid_pred = self.predict(X_valid)\n",
    "            \n",
    "            train_acc = ((np.sum(y_train == y_train_pred)).astype(np.float) / X_train.shape[0])\n",
    "            valid_acc = ((np.sum(y_valid == y_valid_pred)).astype(np.float) / X_valid.shape[0])\n",
    "            \n",
    "            sys.stderr.write('\\r%0*d/%d | Cost: %.2f '\n",
    "                             ' | Train/Valid Acc.: %.2f%%/%.2f%% '\n",
    "                             % (epoch_strlen, i+1, self.epochs, \n",
    "                                cost, train_acc*100, valid_acc*100))\n",
    "            sys.stderr.flush()\n",
    "            \n",
    "            self.eval_['cost'].append(cost)\n",
    "            self.eval_['train_acc'].append(train_acc)\n",
    "            self.eval_['valid_acc'].append(valid_acc)\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are done with executing this code, let's now initialize a new 784-100-10 MLP - a neural network with 784 input units (*n_features*), 100 hidden units (*n_hidden*), and 10 output units (*n_output*): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetMLP(n_hidden=100, \n",
    "                  l2=0.01, \n",
    "                  epochs=200, \n",
    "                  eta=0.0005, \n",
    "                  minibatch_size=100, \n",
    "                  shuffle=True, \n",
    "                  seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you read through the *NeuralNetMLP* code, you have probably already guessed what these parameters are for. Here, you find a short summary of these:\n",
    "\n",
    "* *l2*: This is the $\\lambda$ parameter for L2 regularization to decrease the degree of overfitting.\n",
    "* *epochs*: This is the number of passes over the training set.\n",
    "* *eta*: This is the learning rate $\\eta$.\n",
    "* *shuffle*: This is for shuffling the training set prior to every epoch to prevent that the algorithm gets stuck in circles.\n",
    "* *seed*: This is a random seed for shuffling and weigh initialization. \n",
    "* *minibatch_size*: This is the number of training samples in each mini-batch when splitting of the training data in each epoch for stochastic gradient descent. The gradient descent is computed for each mini-batch separately instead of the entire training data for faster learning.\n",
    "\n",
    "Next, we train the MLP using 55,000 samples from the already shuffled MNIST training dataset and used the remaining 5,000 samples for validation during training. Note that training the neural network may take up to 5 minutes on standard desktop computer hardware. \n",
    "\n",
    "As you may have noticed from the preceding code implementation, we implemented the *fit* method so that it takes four input arguments: training images, training labels, validation images, and validation labels. In neural network training, it is really useful to already compare training and validation accuracy during training, which helps us judge whether the network model performs well, given the architecture and hyperparameters. \n",
    "\n",
    "In general, training (deep) neural networks is relatively expensive compared with the other models we discussed so far. Thus, we want to stop it early in certain circumstances and start over with different hyperparameter setting. Alternatively, if we find that it increasingly tends to overfit the training data (noticeable by an increasing gap between training and validation set performance), we may want to stop the training early as well. \n",
    "\n",
    "Now, to start the training, we execute the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200/200 | Cost: 5047.08  | Train/Valid Acc.: 99.29%/97.94%  "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.NeuralNetMLP at 0x7f16158d4a20>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.fit(X_train=X_train[:55000], y_train=y_train[:55000], \n",
    "       X_valid=X_train[55000:], y_valid=y_train[55000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our *NeuralNetMLP* implementation, we also defined an *eval_* attribute that collects the cost, training, and validation accuracy for each epoch so that we can visualize the results using Matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEKCAYAAADaa8itAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nsi3ewAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(nn.epochs), nn.eval_['cost'])\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the cost decreased substantially during the first 100 epochs and seems to slowly converge in the last 100 epochs. However, the small slope between epoch 175 and epoch 200 indicates that the cost would further decrease with a training over additional epochs. \n",
    "\n",
    "Next, let's take a look at the training and validation accuracy: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(nn.epochs), nn.eval_['train_acc'], \n",
    "         label='training')\n",
    "plt.plot(range(nn.epochs), nn.eval_['valid_acc'], \n",
    "         label='validation', linestyle='--')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot reveals that the gap between training and validation accuracy increases the more epochs we train the network. At approximately the 50th epoch, the training and validation accuracy values are equal, and then, the network starts overfitting the training data. \n",
    "\n",
    "Note that this example was chosen deliberately to illustrate the effect of overfitting and demonstrate why it is useful to compare the validation and training accuracy values during training. One way to decrease the effect of overfitting is to increase the regularization strength - for example, by setting *l2=0.1*. Another useful technique to tackle overfitting in neural networks, dropout, will be covered later. \n",
    "\n",
    "Finally, let's evaluate the generalization performance of the model by calculating the prediction accuracy on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 97.53%\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = nn.predict(X_test)\n",
    "acc = (np.sum(y_test == y_test_pred).astype(np.float) / X_test.shape[0])\n",
    "print('Training accuracy: %.2f%%' % (acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the slight overfitting on the training data, our relatively simple one-hidden layer neural network achieved a relatively good performance on the test dataset, similar to the validation set accuracy (97.98 percent). \n",
    "\n",
    "To further fine-tune the model, we could change the number of hidden units, values of the regularization parameters, and the learning rate or use various other tricks that have been developed over the years but are beyond the scope of this book. In a next chapter, you will learn about a different neural network architecture that is known for its good performance on image datasets. Also, the chapter will introduce additional performance-enhancing tricks such as adaptive learning rates, momentum learning, and dropout. \n",
    "\n",
    "Lastly, let's take a look at some of the images that our MLP struggles with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaYAAAEYCAYAAAAXsVIGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFKbi/wP4oOcC3f+dQwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 25 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "miscl_img = X_test[y_test != y_test_pred][:25]\n",
    "correct_lab = y_test[y_test != y_test_pred][:25]\n",
    "miscl_lab = y_test_pred[y_test != y_test_pred][:25]\n",
    "\n",
    "fig, ax = plt.subplots(nrows=5, ncols=5, sharex=True, sharey=True)\n",
    "ax = ax.flatten()\n",
    "for i in range(25):\n",
    "    img = miscl_img[i].reshape(28, 28)\n",
    "    ax[i].imshow(img, cmap='Greys', interpolation='nearest')\n",
    "    ax[i].set_title('%d) t: %d p: %d'\n",
    "                    % (i+1, correct_lab[i], miscl_lab[i]))\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should now see a 5 x 5 subplot matrix where the first number of subtitles indicates the plot index, the second number represents the true class label (*t*), and the third number stands for the predicted class label (*p*). \n",
    "\n",
    "As we can see in the preceding figure, some of those images are even challenging for us humans to classify correctly. For example, the 6 in subplot 8 really looks like a carelessly drawn 0, and the 8 in subplot 23 could be a 9 due to the narrow lower part combined with the bold line. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an artificial neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have seen a neural network in action and have gained a basic understanding of how it works by looking over the code, let's dig a little bit deeper into some of the concepts, such as the logistic cost function and the backpropagation algorithm that we implemented to learn the weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the logistic cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic cost function that we implemented as the *_compute_cost* method is actually pretty simple to follow since it is the same cost function that we described in the logistic regression previously.\n",
    "\n",
    "$$J(w) = -\\sum_{i=1}^n y^{[i]}\\log(a^{[i]}) + (1 - y^{[i]})\\log(1-a^{[i]})$$\n",
    "\n",
    "Here, $a^{[i]}$ is the sigmoid activation of the $i$th sample in the dataset, which we compute in the forward propagation step:\n",
    "\n",
    "$$a^{[i]} = \\phi(z^{[i]})$$\n",
    "\n",
    "Again, note that in this context, the superscript *[i]* is an index for training samples, not layers.\n",
    "\n",
    "Now, let's add a regularization term, which allows us to reduce the degree of overfitting. As you recall from earlier chapters, the L2 regularization term is defined as follows (remember that we do not regularize the bias units):\n",
    "\n",
    "$$L2 = \\lambda||w||_2^2 = \\lambda\\sum_{j=1}^m w_j^2$$\n",
    "\n",
    "By adding the L2 regularization term to our logistic cost function, we obtain the following equation:\n",
    "\n",
    "$$J(w) = \\left[-\\sum_{i=1}^n y^{[i]}\\log(a^{[i]}) + (1 - y^{[i]})\\log(1-a^{[i]})\\right] + \\lambda||w||_2^2$$\n",
    "\n",
    "Since we implemented an MLP for multiclass classification that returns an output vector of $t$ elements that we need to compare to the $t \\times l$ dimensional target vector in the one-hot encoding representation, for, example, the activation of the third layer and the target class (here, class 2) for a particular sample may look like this:\n",
    "\n",
    "$$\n",
    "a^{out} = \n",
    "\\begin{bmatrix}\n",
    "0.1 \\\\\n",
    "0.9 \\\\\n",
    "\\vdots \\\\\n",
    "0.3\n",
    "\\end{bmatrix}, \n",
    "y = \n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "\\vdots \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Thus, we need to generalize the logistic cost function to all $t$ activation units in our network. Thus, the cost function (without the regularization term) becomes the following: \n",
    "\n",
    "$$J(w) = -\\sum_{i=1}^n\\sum_{j=1}^t y_j^{[i]}\\log(a_j^{[i]}) + (1 - y_j^{[i]})\\log(1-a_j^{[i]})$$\n",
    "\n",
    "Here, again, the superscript $[i]$ is the index of a particular sample in our training set.\n",
    "\n",
    "The following generalized regularization term may look a little bit complicated at first, but we are just calculating the sum of all weights in an $l$ layer (without the bias term) that we added to the first column:\n",
    "\n",
    "$$J(w) = -\\left[\\sum_{i=1}^n\\sum_{j=1}^t y_j^{[i]}\\log(a_j^{[i]}) + (1 - y_j^{[i]})\\log(1-a_j^{[i]})\\right] + \\frac{\\lambda}{2}\\sum_{l=1}^{L-1}\\sum_{i=1}^{u_l}\\sum_{j=1}^{u_{l+1}}(w_{j,i}^{(t)})^2$$\n",
    "\n",
    "Here, $u_l$ refers to the number of units of a given layer l, and the following expression represents the penalty term:\n",
    "\n",
    "$$\\frac{\\lambda}{2}\\sum_{l=1}^{L-1}\\sum_{i=1}^{u_l}\\sum_{j=1}^{u_{l+1}}(w_{j,i}^{(t)})^2$$\n",
    "\n",
    "Remember that our goal is to minimize the cost function $J(W)$; thus we need to calculate the partial derivative of the parameters $W$ with respect to each weight for every layer in the network:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial w_{j,i}^{(l)}} J(W)$$\n",
    "\n",
    "In the next section, we will talk about the backpropagation algorithm, which allows us to calculate those partial derivatives to minimize the cost function.\n",
    "\n",
    "Note that $W$ consists of multiple matrices. In a multilayer perceptron with one hidden unit, we have the weight matrix $W^{(h)}$, which connects the input to the hidden layer, and $W^{out}$, which connects the hidden layer to the output layer. An intuitive visualization of the three-dimensional tensor $W$ is provided in the following figure:\n",
    "\n",
    "<img src='images/12_10.png'>\n",
    "\n",
    "In this simplified figure, it may seem that both $W^{(h)}$ and $áº^{(out)}$ have the same number of rows and columns, which is typically not the case unless we initialize an MLP with the same number of hidden layers, output units, and input features. If this sounds confusing, stay tuned for the next section, where we will discuss the dimensionality of $W^{(h)}$ and $W^{(out)}$ in more detail in the context of the backpropagation algorithm. Also, I want to encourage you to read through the code of the *NeuralNetMLP* again, which I annotated with helpful comments about the dimensionality with regard to the different matrices and vector transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developing your intuition for backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although backpropagation was rediscovered and popularized more than 30 years ago, it still remains one of the most widely used algorithms to train artificial neural networks very efficiently. \n",
    "\n",
    "In this section, I intend to provide a short and intuitive summary and the bigger picture of how this fascinating algorithm works before we dive into more mathematical details. In essence, we can think of backpropagation as a very computationally efficient approach to compute the partial derivatives of a complex cost function in multilayer neural networks. Here, our goal is to use those derivatives to learn the weight coefficients for parametrizing such a multilayer artificial neural network. The challenge in the parametrization of neural networks is that we are typically dealing with a very large number of weight coefficients in a high-dimensional feature space. In contrast to cost functions of single-layer neural networks such as Adaline or logistic regression, which we have seen in previous chapter, the error surface of a neural network cost function is not convex or smooth with respect to the parameters. There are many bumps in this high-dimensional cost surface (local minima) that we have to overcome in order to find the global minimum of the cost function. \n",
    "\n",
    "You may recall the concept of the chain rule from your introductory calculus. The chain rule is an approach to compute the derivative of a complex, nested, function, such as $f(g(x))$. In the context of computer algebra, a set of techniques has been developed to solve such problems very efficiently, which is also known as **automatic differentiation**. \n",
    "\n",
    "Automatic differentiation comes with two modes, the forward and reverse modes; backpropagation is simply just a special case of reverse mode automatic differentiation. The key point is that applying the chain rule in the forward mode can be quite expensive since we would have to multiply large matrices for each layer (Jacobians) that we eventually multiply by a vector to obtain the output. The trick of reverse mode is that we start from right to left: we multiply a matrix by a vector, which yields another vector that is multiplied by the next matrix and so on. Matrix-vector multiplication is computationally much cheaper than matrix-matrix multiplication, which is why backpropagation is one of the most popular algorithms used in neural network training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training neural networks via backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will go through the math of backpropagation to understand how you can learn the weights in a neural network very efficiently. Depending on how comfortable you are with mathematical representations, the following equations may seem relatively complicated at first. \n",
    "\n",
    "In a previous section, we saw how to calculate the cost as the difference between the activation of the last layer and the target class label. Now, we will see how the backpropagation algorithm works to update the weights in our MLP model from a mathematical perspective, which we implemented in the *# Backpropagation* section inside the *fit* method. As we recall from the beginning of this chapter, we first need to apply forward propagation in order to obtain the activation of the output layer, which we formulated as follows:\n",
    "\n",
    "$$Z^{(h)} = A^{(in)}W^{(h)} \\quad \\text{net input of the hidden layer}$$\n",
    "\n",
    "$$A^{(h)} = \\phi(Z^{(h)}) \\quad \\text{activation of the hidden layer}$$\n",
    "\n",
    "$$Z^{(out)} = A^{(h)}W^{(out)} \\quad \\text{net input of the output layer}$$\n",
    "\n",
    "$$A^{(out)} = \\phi(Z^{(out)}) \\quad \\text{activation of the output layer}$$\n",
    "\n",
    "Concisely, we just forward-propagate the input features through the connection in the network, as shown in the following illustration:\n",
    "\n",
    "<img src='images/12_11.png'>\n",
    "\n",
    "In backpropagation, we propagate the error from right to left. We start by calculating the error vector of the output layer:\n",
    "\n",
    "$$\\delta^{(out)} = a^{(out)} - y$$\n",
    "\n",
    "Here, $y$ is the vector of the true class labels (the corresponding variable in the *NeuralNetMLP* code is $sigma_out$).\n",
    "\n",
    "Next, we calculate the error term of the hidden layer:\n",
    "\n",
    "$$\\delta^{(h)} = \\delta^{(out)}(W^{(out)})^T \\odot \\frac{\\partial \\phi(z^{(h)})}{\\partial z^{(h)}}$$\n",
    "\n",
    "Here, $\\frac{\\partial \\phi(z^{(h)})}{\\partial z^{(h)}}$ is simply the derivative of the sigmoid activation function, which we computed as *sigmoid_derivative_h = a_h (1. - a_h)* in the *fit* method of the *NeuralNetMLP*:\n",
    "\n",
    "$$\\frac{\\partial \\phi(z^{(h)})}{\\partial z^{(h)}} = (a^{(h)} \\odot (1 - a^{(h)}))$$\n",
    "\n",
    "Note that the $\\odot$ symbol means element-wise multiplication in this context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compute the $\\delta^{(h)}$ layer error matrix (*sigma_h*) as follows:\n",
    "\n",
    "$$\\delta^{(h)} = \\delta^{(out)} (W^{(out)})^T \\odot (a^{(h)} \\odot (1 - a^{(h)}))$$\n",
    "\n",
    "To better understand how we computed this $\\delta^{(h)}$ term, let's walk through it in more detail. In the preceding equation, we used the transpose $(W^{(out)})^T$ of the $h \\times t$-dimensional matrix $W^{(out)}$. Here, $t$ is the number of output class labels and $h$ is the number of hidden units. The matrix multiplication between the $n \\times t$-dimensional $\\delta^{(out)}$ matrix and the $t \\times h$-dimensional matrix $(W^{(out)})^T$, results in an $n \\times t$-dimensional matrix that we multiplied elementwise by the sigmoid derivative of the same dimension to obtain the $n \\times t$-dimensional matrix $\\delta^{(h)}$. Eventually, after obtaining the $\\delta$ terms, we can now write the derivation of the cost function as follows: \n",
    "\n",
    "$$\\frac{\\partial}{\\partial w_{i,j}^{out}} J(W) = a_j^{(h)} \\delta_i^{(out)}$$\n",
    "\n",
    "$$\\frac{\\partial}{\\partial w_{i,j}^{h}} J(W) = a_j^{(in)} \\delta_i^{(h)}$$\n",
    "\n",
    "Next, we need to accumulate the partial derivative of every node in each layer and the error of the node in the next layer. However, remember that we need to compute $\\Delta_{i,j}^{(l)}$ for every sample in the training set. Thus, it is easier to implement it as a vectorized version like in our *NeuralNetMLP* code implementation: \n",
    "\n",
    "$$\\Delta^{(h)} = \\Delta^{(h)} + (A^{(in)})^T \\delta^{(h)}$$\n",
    "\n",
    "$$\\Delta^{(out)} = \\Delta^{(out)} + (A^{(h)})^T \\delta^{(out)}$$\n",
    "\n",
    "And after we have accumulated the partial derivatives, we can add the regularization term:\n",
    "\n",
    "$$\\Delta^{(l)} := \\Delta^{(l)} + \\lambda^{(l)} \\quad \\text{except for the bias term}$$\n",
    "\n",
    "The two previous mathematical equations correspond to the code variables *delta_w_h*, *delta_b_h*, *delta_w_out*, and *delta_b_out* in *NeuralNetMLP*. \n",
    "\n",
    "Lastly, after we have computed the gradients, we can now update the weights by taking an opposite step towards the gradient for each layer $l$:\n",
    "\n",
    "$$W^{(l)} := W^{(l)} - \\eta \\delta^{(l)}$$\n",
    "\n",
    "To bring everything together, let's summarize backpropagation in the following figure:\n",
    "\n",
    "<img src='images/12_12.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the convergence in neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be wondering why we did not use regular gradient descent but instead used mini-batch learning to train our neural network for the handwrittten digit classification. You may recall our discussion on stochastic gradient descent that we used to implement online learning. In online learning, we compute the gradient based on a single training sample ($k=1$) at a time to perform the weight update. Although this is a stochastic approach, it often leads to very accurate solutions with a much faster convergence than regular gradient descent. Mini-batch learning is a special form of stochastic gradient descent where we compute the gradient based on a subset $k$ of the $n$ training samples with $l < k < n$. Mini-batch learning has the advantage over online learning that we can make use of our vectorized implementations to improve computational efficiency. However, we can update the weights much faster than in regular gradient descent. Intuitively, you can think of mini-batch learning as predicting the voter turnout of a presidential election from a poll asking only a representative subset of the population rather than asking the entire population (which would be equal to running to actual election). \n",
    "\n",
    "Multilayer neural networks are much harder to train than simpler algorithms such as Adaline, logistic regression, or support vector machines. In multilayer neural networks, we typically have hundreds, thousands, or even billions of weights that we need to optimize. Unfortunately, the output function has a rough surface and the optimization algorithm can easily become trapped in local minima, as shown in the following figure:\n",
    "\n",
    "<img src='images/12_13.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this representation is extremely simplified since our neural network has many dimensions; it makes it impossible to visualize the actual cost surface for the human eye. Here, we only show the cost surface for a single weight on the $x$-axis. However, the main message is that we do not want our algorithm to get trapped in local minima. By increasing the learning rate, we can more readily escape such local minima. On the other hand, we also increase the chance of overshooting the global optimum if the learning rate is too large. Since we initialize the weights randomly, we start with a solution to the optimization problem that is typically hopelessly wrong. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A few last words about the neural network implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may be wondering why we went through all of this theory just to implement a simple multilayer network that can classify handwritten digits instead of using an open source Python machine learning library. In fact, we will introduce more complex neural network models in the next chapters, which we will train using the open source TensorFlow library. Although the from scratch implementation in this chapter seems a bit tedious at first, it was a good exercise for understanding the basics behind backpropagation and neural network training, and a basic understanding of algorithms is crucial for applying machine learning techniques appropriately and successfully. \n",
    "\n",
    "Now that you have learned how feedforward neural network work, we are ready to explore more sophisticated deep neural networks, such as TensorFlow and Keras, which allow us to construct neural networks more efficiently. Over the past two years, since its release in November 2015, Tensorflow has gained a lot of popularity among machine learning researchers, who use it to construct deep neural networks because of its ability to optimize mathematical expressions for computations on multi dimensional arrays utilizing **Graphical Processing Units (GPUs)**. While TensorFlow can be considered a low-level deep learning library, simplifying API such as Keras have been developed that make the construction of common deep learning models even more convenient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter, you have learned the basic concepts behind multilayer artificial neural networks, which are currently the hottest topics in machine learning research. In a previous chapter, we started our journey with simple-layer neural network structures and now we have connected multiple neurons to a powerful neural network architecture to solve complex problems such as handwritten digit recognition. We demystified the popular backpropagation algorithm, which is one of the building blocks of many neural networks models that are used in deep learning. After learning about the backpropagation algorithm in this chapter, we are well-equipped for exploring more complex deep neural network architectures. In the remaining chapters, we will introduce TensorFlow, an open source library geared towards deep learning, which allows us to implement and train multilayer neural networks more efficiently. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
