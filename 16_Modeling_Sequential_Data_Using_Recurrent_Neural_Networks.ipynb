{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Sequential Data Using Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous chapter, we focused on **Convolutional Neural Networks (CNNs)** for image classification. In this chapter, we will explore **Recurrent Neural Networks (RNNs)** and see their application in modeling sequencial data and a specific subset of sequential data - time-series data. As an overview, in this chapter, we will cover the following topics:\n",
    "\n",
    "* Introducing sequential data\n",
    "* RNNs for modeling sequences\n",
    "* **Long Short-Term Memory (LSTM)**\n",
    "* **Truncated Backpropagation Through Time (T-BPTT)**\n",
    "* Implementing a multilayer RNN for sequence modeling in TensorFlow\n",
    "* Project one - RNN sentiment analysis of the IMDb movie review dataset\n",
    "* Project two - RNN character-level language modeling with LSTM cells, using text data from Shakespeare's Hamlet\n",
    "* Using gradient clipping to avoid exploring gradients. \n",
    "\n",
    "Since this chapter is the last in our *Python Machine Learning* journey, we will conclude with a summary of what we have learned about RNNs, and an overview of all the machine learning and deep learning topics that led us to RNNs across the journey of the book. We will then sign off by sharing with you links to some of our favorite people and initiatives in this wonderful field so that you can continue your journey into machine learning and deep learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing sequential data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin our discussion of RNNs by looking at the nature of sequential data, more commonly known as **sequences**. We will take a look at the unique properties of sequences that make them different from other kind of data. We will then see how we can represent sequential data, and explore the various categories of model ffor sequential data, which are based on the input and output of a model. This will help us explore relationship between RNNs and sequences a little bit later on in the chapter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling sequential data - order matters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What makes sequences unique, from other data types, is that elements in a sequence appear in a certain order, and are not independent of each other. \n",
    "\n",
    "If you recall from Chapter 6, we discussed that typical machine learning algorithms for supervised learning assume that the input data is **Independent and Identically Distributed (IID)**. For example, if we have $n$ data samples, $x^{(1)}, x^{(2)}, \\ldots, x^{(n)}$, the order in which we use the data for training our machine learning algorithm does not matter. \n",
    "\n",
    "However, this assumption is not valid anymore when we deal with sequences - by definition, order matters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representing sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have estabilished that sequences are a nonindependent order in our input data; we next need to find ways to leverage this valuable information in our machine learning model. \n",
    "\n",
    "Throughout this chapter, we will represent sequences as $(x^{(1)}, x^{(2)}, \\ldots, x^{(T)})$. The superscript indices indicate the order of the instances, and the length of the sequence is $T$. For a sensible example of sequences, consider time-series data, where each sample point $x^{(t)}$ belongs to a particular time $t$. \n",
    "\n",
    "The following figure shows an example of time-series data where both $x$'s and $y$'s naturally follow the order according to their time axis; therefore, both $x$'s and $y$'s are sequences: \n",
    "\n",
    "<img src='images/16_01.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard neural network models that we covered so far, such as MLP and CNNs, are not capable of handling *the order* of input samples. Intuitively, one can say that such models do not have a *memory* of the past seen samples. For instance, the samples are passed through the feedforward and backpropagation steps, and the weights are updated independent of the order in which the sample is processed. \n",
    "\n",
    "RNNs, by contrast, are designed for modeling sequences and are capable of remembering past information and processing new events accordingly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The different categories of sequence modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence modeling has many fascinating applications, such as language translation (perhaps from English to German), image captioning, and text generation. \n",
    "\n",
    "However, we need to understand the different types of sequence modeling tasks to develop an appropriate model. The following figure shows several different relationship categories of input and output data:\n",
    "\n",
    "<img src='images/16_02.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's consider the input and output data here. If neither the input or output data represents sequences, then we are dealing with standard data, and we can use any of the previous methods to model such data. But if either the input or output is a sequence, the data will form one of the following three different categories: \n",
    "\n",
    "* **Many-to-one**: The input data is a sequence, but the output is a fixed-size vector, not a sequence. For example, in sentiment analysis, the input is a text-based and the output is a class label.\n",
    "* **One-to-many**: The input data is in standard format, not a sequence, but the output is a sequence. An example of this category is image captioning - the input is an image; the output is an English phrase. \n",
    "* **Many-to-many**: Both the input and output arrays are sequences. This category can be further divided based on whether the input and output are synchronized or not. An example of a **synchronized** many-to-many modeling task is video classification, where each frame in a video is labeled. An example of a **delayed** many-to-many would be translating a language into another. For instance, an entire English sentence must be read and processed by a machine before producing its translation into German. \n",
    "\n",
    "Now, since we know the categories of sequence modeling, we can move forward to discuss the structure of an RNN. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNs for modeling sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, now that we understand sequences, we can look at the foundations of RNNs. We will start by introducing the typical structure of an RNN, and we will see how the data flows through it with one or more hidden layers. We will then examine how the neuron activations are computed in a typical RNN. This will create a context for us to discuss the common challenges in training RNNs, and explore the modern solution to these challenges - LSTM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the structure and flow of an RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by introducing the architecture of an RNN. The following figure shows a standard feedforward neural network and an RNN, in a side by side for comparison: \n",
    "\n",
    "<img src='images/16_03.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of these networks have only one hidden layer. In this representation, the units are not displayed, but we assume that the input layer ($x$), hidden layer ($h$), and output layer ($y$) are vectors which contain many units. \n",
    "\n",
    "This generic RNN architecture could correspond to the two sequence modeling categories where the input is a sequence. Thus, it could be either many-to-many if we consider $y^{(t)}$ as teh final output, or it could be many-to-one if, for example, we only use the last element of $y^{(t)}$ as the final output. \n",
    "\n",
    "Later, we will see how the output sequence $y^{(t)}$ can be converted into standard, nonsequential output. \n",
    "\n",
    "In a standard feedforward network, information flows from the input to the hidden layer, and then from the hidden layer to the output layer. On the other hand, in a recurrent network, the hidden layer gets its input from both the input layer and the hidden layer from the previous time step. \n",
    "\n",
    "The flow of information in adjacent time steps in the hidden layer allows the network to have a memory of past events. This flow of information is usually displayed as a loop, also known as a **recurrent edge** in graph notation, which is how this general architecture got its name. \n",
    "\n",
    "In the following figure, the single hidden layer network and the multilayer network illustrate two contrasting architectures: \n",
    "\n",
    "<img src='images/16_04.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to examine the architecture of RNNs and the flow of information, a compact representation with a recurrent edge can be unfolded, which you can see in the preceding figure. \n",
    "\n",
    "As we know, each hidden unit in a standard neural network receives only one input - the net preactivation associated with the input layer. Now, in contrast, each hidden layer unit in an RNN receives two *distinct* sets of input - the preactivation from the input layer and the activation of the same hidden layer from the previous time step $t-1$. \n",
    "\n",
    "At the first time step $t=0$, the hidden units are initialized to zeros or small random values. Then, at a time step where $t>0$, the hidden units get their input from the data point at the current time $x^{(t)}$ and the previous values of hidden units at $t-1$, indicated as $h^{(t-1)}$. \n",
    "\n",
    "Similarly, in the case of a multilayer RNN, we can summarize the information flow as follows: \n",
    "\n",
    "* *layer=1*: Here, the hidden layer is represented as $h_1^{(t)}$ and gets its input from the data point $x^{(t)}$ and the hidden values in the same layer, but the previous time step $h_1^{t-1}$. \n",
    "* *layer=2*: The second hidden layer, $h_2^{(t)}$ receives its inputs from the hidden units from the layer below at the current time step ($h_1^{(t)}$) and its own hidden values from the previous time step $h_2^{(t-1)}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing activations in an RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand the structure and general flow of information in an RNN, let's get more specific and compute the actual activations of the hidden layers as well as the output layer. For simplicity, we will consider just a single hidden layer; however, the same concept applies to multilayer RNNs. \n",
    "\n",
    "Each directed edge (the connections between boxes) in the representation of an RNN that we just looked at is associated with a weight matrix. Those weights do not depend on time $t$; therefore, they are shared across the time axis. The different weight matrices in a single layer RNN are as follows: \n",
    "\n",
    "* $W_{xh}$: The weight matrix between the input $x^{(t)}$ and the hidden layer $h$.\n",
    "* $W_{hh}$: The weight matrix associated with the recurrent edge. \n",
    "* $W_{hy}$: The weight matrix between the hidden layer and output layer. \n",
    "\n",
    "You can see these weights matrices in the following figure: \n",
    "\n",
    "<img src='images/16_05.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In certain implementations, you may observe that weight matrices $W_{xh}$ and $W_{hh}$ are concatenated to a combined matrix $W_h = [W_{xh}; W_{hh}]$. Later on, we will make use of this notation as well. \n",
    "\n",
    "Computing the activations is very similar to standard multilayer perceptrons and other types of feedforward neural networks. For the hidden layer, the net input $Z_h$ (preactivation) is computed through a linear combination. That is, we compute the sum of the multiplications of the weight matrices with the corresponding vectors and add the bias unit - $Z_h = W_{xh}x^{(t)} + W_{hh}h^{(t-1)} + b_h$. Then, the activations of the hidden units at the time step $t$ are calculated as follows:\n",
    "\n",
    "$$h^{(t)} = \\phi(Z_h^{(t)}) = \\phi_h (W_{xh}x^{(t)} + W){hh}h^{(t-1)} + b_h)$$\n",
    "\n",
    "Here, $b_h$ is the bias vector for the hidden units and $\\phi_h$ is the activation function of the hidden layer. \n",
    "\n",
    "In case you want to use the concatenated weight matrix $W_h = [W_{xh}; W_{hh}]$, the formula for computing hidden units will change as follows:\n",
    "\n",
    "$h^{(t)} = \\phi_h\\left([W_{xh}; W_{hh}][x^{(t)}; h^{(t-1)}]^{-1} + b_h\\right)$\n",
    "\n",
    "Once the activations of hidden units at the current time step are computed, then the activations of output units will be computed as follows: \n",
    "\n",
    "$$y^{(t)} = \\phi_y(w_{hy}h^{(t)} + b_y)$$\n",
    "\n",
    "To help clarify this further, the following figure shows the process of computing these activations with both formulations: \n",
    "\n",
    "<img src='images/16_06.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training RNNs using BPTT** \n",
    "\n",
    "The learning algorithm for RNNs was introduced in 1990s *Backpropagation Through Time: What it Does and How to Do It*.\n",
    "\n",
    "The derivation of the gradients might be a bit complicated, but the basic idea is that the overall loss $L$ is the sum of all the loss functions at time $t=1$ to $t=T$: \n",
    "\n",
    "$$L = \\sum_{t=1}^T L^{(t)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The challenges of learning long-range interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation through time, or BPTT, which we briefly mentioned in the previous information box, introduces some challenges. Because of the multiplicative factor $\\frac{\\partial h^{(t)}}{\\partial h^{(k)}}$ in the computing gradients of a loss function, the so-called **vanishing** or **exploding** gradient problem arises. This problem is explained through the examples in the following figure, which shows an RNN with only one hidden unit for simplicity: \n",
    "\n",
    "<img src='images/16_07.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, $\\frac{\\partial h^{(t)}}{\\partial h^{(k)}}$ has $t-k$ multiplications; therefore, multiplying the $w$ weight $t-k$ times results in a factor $w^{t-k}$. As a result, if $|w| < 1$, this factor becomes very small when $t-k$ is large. On the other hand, if the weight of the recurrent edge is $|w| > 1$, then $w^{t-k}$ becomes very large when $t-k$ is large. Note that large $t-k$ refers to long-range dependencies. \n",
    "\n",
    "Intuitively, we can see that a naive solution to avoid vanishing or exploring gradient can be accomplished by ensuring $|w| = 1$. \n",
    "\n",
    "In practice, there are two solutions to this problem: \n",
    "* Truncated backpropagation through time (TBPTT). \n",
    "* Long short-term memory (LSTM). \n",
    "\n",
    "TBPTT clips the gradients above a given threshold. While TBPTT can solve the exploding gradient problem, the truncation limits the number of steps that the gradient can effectively flow back and properly update the weights. \n",
    "\n",
    "On the other hand, LSTM, designed in 1997 by Hochreiter and Schmidhuber, has been more successful in modeling long-range sequences by overcoming the vanishing gradient problem. Let's discuss LSTM in more detail. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM were first introduced to overcome the vanishing gradient problem. The building block of an LSTM is a **memory cell**, which essentially represents the hidden layer. \n",
    "\n",
    "In each memory cell, there is a recurrent edge that has the desirable weight $w=1$, as we discussed previously, to overcome the vanishing and exploding gradient problems. The values associated with this recurrent edge is called **cell state**. The unfolded structure of a modern LSTM cell is shown in the following figure: \n",
    "\n",
    "<img src='images/16_08.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the cell state from the previous time step, $C^{(t-1)}$, is modified to get the cell state at the current time step, $C^{(t)}$, without being multiplied directly with any weight factor. \n",
    "\n",
    "The flow of information in this memory cell is controlled by some units of computation that we will describe here. In the previous figure, $\\odot$ refers to the **element-wise product** (element-wise multiplication) and $\\oplus$ means **element-wise summation** (element-wise addition). Furthermore, $x^{(t)}$ refers to the input data at time $t$, and $h^{(t-1)}$ indicates the hidden units at time $t-1$. \n",
    "\n",
    "Four boxes are indicated with an activation function, either the sigmoid function ($\\sigma$) or hyperbolic tangent (tanh), and a set of weights; these boxes apply linear combination by performing matrix-vector multiplications on their input. These units of computation with sigmoid activation functions, whose output units are passed through $\\odot$, are called **gates**. \n",
    "\n",
    "In an LSTM cell, there are three different types of gates, known as the forget gate, the input gate, and the output gate: \n",
    "\n",
    "* The **forget gate ($f_t$)** allows the memory cell to reset the cell state without growing indefinitely. In fact, the forget gate decides which information is allowed to go through and which information to suppress. Now, $f_t$ is computed as follows: $f_t = \\sigma(W_{xf}x^{(t)} + W_{hf}h^{(t-1)} + b_f)$. Note that the forget gate was not part of the original LSTM cell; it was added a few years later to improve the original model. \n",
    "* The **input gate ($i_t$)** and input nodes ($g_t$) are responsible for updating the cell state. They are computed as follows: $i_t = \\sigma(W_{xi}x^{(t)} + W_{hi}h^{(t-1)} + b_i)$ and $g_i = tanh(W_{xg}x^{(t)} + W_{hg}h^{(t-1)} + b_g)$. The cell state at time $t$ is computed as follows: $C^{(t)} = (C^{t-1} \\odot f_t) \\oplus (i_t \\odot g_t)$. \n",
    "* The **output gate ($o_t$)** decides how to update the values of hidden units: $o_t = \\sigma(W_{xo}x^{(t)} + W_{ho}h^{(t-1)} + b_o)$. Given this, the hidden units at the current time step are computed as follows: $h^{(t)} = o_t \\odot tanh(C^{(t)})$. \n",
    "\n",
    "The structure of an LSTM cell and its underlying computations might seem too complex. However, the good news is that TensorFlow has already implemented everything in wrapper functions that allows us to define our LSTM cell easily. We will see the real application of LSTMs in action when we use TensorFlow later in this chapter. \n",
    "\n",
    "We have introduced LSTMs in this section, which provide a basic approach for modeling long-range dependencies in sequences. Yet, it is important to note that there are many variations of LSTMs described in literature. \n",
    "\n",
    "Also, worth noting is a more recent approach, called **Gated Recurrent Unit (GRU)**, which was proposed in 2014. GRUs have a simpler architecture than LSTMs; therefore, they are computationally more efficient while their performance in some tasks, such as polyphonic music modeling, is comparable to LSTMs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
