
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{1\_Giving\_Computers\_The\_Ability\_To\_Learn\_From\_Data}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \section{Giving Computers the Ability to Learn from
Data}\label{giving-computers-the-ability-to-learn-from-data}

    \textbf{Machine Learning} is the application and science of algorithms
that make sense of data. We are living in an age where data comes in
abundance; using self-learning algorithms from the field of machine
learning, we can turn this data into knowledge.

    In this Jupyter Notebook, we will learn about the main concepts and
different types of machine learning. Together with a basic introduction
to the relevant terminology, we will lay the groundwork for successfully
using machine learning techniques for practical problem solving.

    We will cover the following topics: * The general concepts of machine
learning * The three types of learning and basic terminology * The
building blocks for successfully designing machine learning systems *
Installing and setting up Python for data analysis and machine learning

    \section{Building intelligent machines to transform data into
knowledge}\label{building-intelligent-machines-to-transform-data-into-knowledge}

    In this age of modern technology, there is one resource that we have in
abundance: a large amount of structured and unstructured data. In the
second half of the twentieth century, machine learning evolved as a
subfield of \textbf{Artificial Inteligence} that involved
\textbf{self-learning algorithms that derived knowledge from data in
order to make predictions}. Instead of requiring humans to manually
derive rules and build models from analyzing large amounts of data,
\textbf{machine learning offers a more efficient alternative for
capturing the knowledge in data to gradually improve the performance of
predictive models and make data-driven decisions}. Thanks to machine
learning, we enjoy robust email spam filters, convenient text and voice
recognition software, reliable web search engines, challenging
chess-playing programs, and, hopefully soon, safe and efficient
self-driving cars.

    \section{The three different types of machine
learning}\label{the-three-different-types-of-machine-learning}

    There are the types of machine learning: \textbf{Supervised Learning},
\textbf{Unsupervised Learning} and \textbf{Reinforcement Learning}.

    

    \subsection{Supervised Learning}\label{supervised-learning}

    Its main goal is to learn a model from \textbf{labeled training data}
that allows us to make predictions about unseen or future data. Here,
the term \textbf{supervised} refers to a set of samples where the
desired output signals (labels) are already known.

    Considering the example of email filtering, we can train a model using a
supervised machine learning algorithm on a corpus of labeled emails,
emails that are correctly marked as spam or not-spam, to predict whether
a new email belongs to either of the two categories.

    A supervised learning task with discrete class labels, such as in the
previous email spam filtering example, is also called a
\textbf{classification task}. Another subcategory of supervised learning
is \textbf{regression}, where the outcome signal is a continuous value.

    

    \subsubsection{Classification for predicting class
labels}\label{classification-for-predicting-class-labels}

    \textbf{Classification is a subcategory of supervised learning where the
goal is to predict the categorical class labels of new instances, based
on past observations}. Those class labels are discrete, unordered values
that can be understood as the group memberships of the instances. The
previously mentioned example of email spam detection represents a
typical example of a binary classification task, where the machine
learning algorithm learns a set of rules in order to distinguish between
two possible classes: spam and not-spam emails.

    However, the set of class labels does not have to be of a binary nature.
The predictive model learned by a supervised learning algorithm can
assign any class label what was presented in the training dataset to a
new, unlabeled instance. A typical example of \textbf{multiclass
classification} task is handwritten character recognition. Here, we
could collect a training dataset that consists of multiple handwritten
examples of each letter in the alphabet. Now, if a user provides a new
handwritter character via an input device, our predictive model will be
able to predict the correct letter in the alphabet with certain
accuracy. However, our machine learning system would be unable to
correctly recognize any of the digits zero to nine, for example, if they
were not part of our training dataset.

    The following figure illustrates the concept of a binary classification
task given 30 training samples; 15 samples are labeled as negative class
(minus signs) and 15 training samples are labeled as positive class
(plus signs). In this scenario, our dataset is two-dimensional, which
means that each sample has two values associated with it: \(x_1\) and
\(x_2\). Now, we can use a supervised machine learning algorithm to
learn a rule, the \textbf{decison boundary} represented as a dashed
line, that can separate those two calsses and classify new data into
each of those two categories given its \(x_1\) and \(x_2\) values.

    

    \subsubsection{Regression for predicting continuous
outcomes}\label{regression-for-predicting-continuous-outcomes}

    A second type of supervised learning is the prediction of continuous
outcomes, which is also called \textbf{regression analysis}. In the
regression analysis, we are given a number of predictor
(\textbf{explanatory}) variables and a continuous response variable
(\textbf{outcome} or \textbf{target}), and \textbf{we try to find a
relationship between those variables that allows us to predict an
outcome}. For example, let's assume that we are interested in predicting
the math SAT scores of our students. If there is a relationship between
the time spent studying for the test and the final scores, we could use
it as training data to learn a model that uses the study time to predict
the test scores of future students who are planning to make this test.

    

    \subsection{Solving interative problems with reinforcement
learning}\label{solving-interative-problems-with-reinforcement-learning}

    Another type of machine learning is \textbf{reinforcement learning}. In
reinforcement learning, \textbf{the goal is to develop a system
(\emph{agent}) that improves its performance based on interactions with
the environment}. Since the information about the current state of the
environment typically also includes a so-called \textbf{reward signal},
\textbf{we can think of reinforcement learning as a field related to
supervised learning}. However, \textbf{in reinforcement learning this
feedback is not the correct ground truth label or value, but a measure
of how well the action was measured by a reward function}. Through its
interaction with the environment, \textbf{an agent can then use
reinforcement learning to learn a series of actions that maximizes this
reward via an exploratory trial-and-error aproach or deliberative
planning}.

    A popular example of reinforcement learning is a chess engine. Here, the
agent decides upon a series of moves depending on the state of the board
(the environment), and the reward can be defined as \textbf{win} or
\textbf{lose} at the end of the game.

    

    There are many different subtypes of reinforcement learning. However, a
general scheme is that the agent in reinforcement learning tries to
maximize the reward by a series of interactions with the environment.
Each state can be associated with a positive or negative reward, and a
reward can be defined as accomplishing an overall goal, such as winning
or losing a game of chess. For instance, in chess the outcome of each
move can be thought of as a different state of the environment. To
explore the chess example further, let's think of visiting certain
locations on the chess board as being associated with a positive event,
for instance, removing an opponent's chess piece from the board or
threatening the queen. Other positions, however, are associated with a
negative event, such as losing a chess piece to the opponent in the
following turn. Now, not every turn results in the removal of a chess
piece, and reinforcement learning is concerned with learning the series
of steps by maximizing a reward based on immediate and delayed feedback.

    \subsection{Discovering hidden structures with unsupervised
learning}\label{discovering-hidden-structures-with-unsupervised-learning}

    In \textbf{supervised learning}, we know the right answer beforehand
when we train our model, and in \textbf{reinforcement learning}, we
define a measure of reward for particular actions by the agent. In
\textbf{unsupervised learning}, however, we are dealing with
\textbf{unlabeled data} of unknown structure. Using unsupervised
learning techniques, \textbf{we are able to explore the structure of our
data to extract meaningful information without the guidance of a known
outcome variable or reward function}.

    \subsubsection{Finding subgroups with
clustering}\label{finding-subgroups-with-clustering}

    \textbf{Clustering} is an exploratory data analysis technique that
\textbf{allows us to organize a pile of information into meaningful
subgroups (\emph{clusters}) without having any prior knowledge of their
group memberships}. Each cluster that arises during the analysis defines
a group of objects that share a certain degree of similarity but are
more dissimiliar to objects in other clusters, which is why clustering
is also sometimes called \textbf{unsupervised classification}.
Clustering is a \textbf{great technique for structuring information and
deriving meaningful relationships from data}. For example, it allows
marketers to discover customer groups based on their interests, in order
to develop distinct marketing programs.

    The following figure illustrates how clustering can be applied to
organizing unlabeled data into three distinct groups based on the
similarity of their features \(x_1\) and \(x_2\):

    

    \subsubsection{Dimensionality reduction for data
compression}\label{dimensionality-reduction-for-data-compression}

    Another subfield of unsupervised learning is \textbf{dimensionality
reduction}. Often we are working with data of high dimensionality,
\textbf{each observation comes with a high number of measurements}, that
can present a challenge for limited storage space and the computational
performance of machine learning algorithms. Unsupervised dimensionality
reduction is a commonly used approach in feature preprocessing to
\textbf{remove noise from data}, which can also degrade the predictive
performance of certain algorithms, and \textbf{compress the data onto
the smaller dimensional subspace while retaining most of the relevant
information}.

    Sometimes, \textbf{dimensionality reduction can also be useful for
visualizing data}, for example, a high-dimensional feature set can be
projected onte one-, two-, or three-dimensional feature spaces in order
to visualize it via 3D or 2D scatterplots or histograms. The following
figure shows an example where nonlinear dimensionality reduction was
applied to compress a 3D Swiss Roll onto a new 2D feature subspace:

    

    \section{Introduction of the basic terminology and
notations}\label{introduction-of-the-basic-terminology-and-notations}

    Now that we have discussed the three broad categories of machine
learning, \textbf{supervised}, \textbf{unsupervised} and
\textbf{reinforcement learning}, let us have a look at the basic
terminology that we will be using thoughout this material. The following
table depicts an excerpt of the \textbf{Iris dataset}, which is a
classic example in the field of machine learning. The iris dataset
contains the measurements of 150 Iris flowers from three different
species, Setosa, Versicolor, and Virginica. Here, each \textbf{flower
sample represents one row in our dataset}, and \textbf{the flower
measurements in centimeters are stored as columns}, which we also call
the \textbf{features} of the dataset:

    

    To keep the notation and implementation simple yet efficient, we will
make use of some of the basics of linear algebra. In the following
chapters, we will use a matrix and vector notation to refer to our data.
We will follow the common convention to represent \textbf{each sample as
a separate row} in a \textbf{feature matrix X}, where \textbf{each
feature is stored as a separate column}. The Iris dataset consisting of
150 samples and four features can then be written as a \textbf{\(150x4\)
matrix X}.

    \section{A roadmap for building machine learning
systems}\label{a-roadmap-for-building-machine-learning-systems}

    In previous sections, we discussed the basic concepts of machine
learning and the three types of learning. In this section, we will
discuss the other important parts of a machine learning system
accompanying the learning algorithm. The following diagram shows a
typical workflow for using machine learning in predictive modeling,
which we will discuss in the following subsections:

    

    \subsection{Preprocessing - getting data into
shape}\label{preprocessing---getting-data-into-shape}

    Let's begin with discussing the roadmap for building machine learning
systems. \textbf{Raw data rarely comes in the form and shape that is
necessary for the optimal performance of a learning algorithm}. Thus,
\textbf{the preprocessing of the data is one of the most crucial steps
in any machine learning application}. If we take the Iris flower dataset
from the previous section as an example, we can think of the raw data as
a series of flower images from which we want to extract meaningful
features. Useful features could be the color, the hue, the intensity of
the flowers, the height, and the flower lenghts and widths. \textbf{Many
machine learning algorithms also require that the selected features are
on the same scale for optimal performance}, which is often achieved by
transforming the features in the range {[}0, 1{]}, or a \textbf{standard
normal distribution with zero mean and unit variance}, as we will see in
later chapters.

    \textbf{Some of the selected features may be highly correlated and
therefore redundant to a certain degree}. In those cases,
\textbf{dimensionality reduction techniques are useful for compressing
the features onto a lower dimensional subspace.} Reducing the
dimensionality of our feature space has the advantage that \textbf{less
storage space is required}, and \textbf{the learning algorithm can run
much faster}. In certain cases, \textbf{dimensionality reduction can
also improve the predictive performance of a model if the dataset
contains a large number of irrelevant features, or noise, that is, if
the dataset has a low signal-to-noise ratio}.

    To determine whether our machine learning algorithm \textbf{not only
performs well on the training set but also generalizes well to new
data}, we also want to \textbf{randomly divide the dataset into a
separate training and test set}. We use the training set to train and
optimize our machine learning model, while we keep the test set until
the very end to evaluate the final model.

    \subsection{Training and selecting a predictive
model}\label{training-and-selecting-a-predictive-model}

    Many different machine learning algorithms have been developed to solve
different problem tasks. An important point that can be summarized from
David Wolpert's famous \textbf{No free lunch theorems} is that we can't
get learning for free. Intuitively, we can relate this concept to the
popular saying, \textbf{\emph{I suppose it is tempting, if the only tool
you have is a hammer, to treat everything as if it were a nail}}. For
example, \textbf{each classification algorithm has its inherent biases,
and no single classification model enjoys superiority if we do not make
any assumptions about the task}. In practice, it is therefore essential
to compare at least a handful of different algorithms in order to train
and select the best performing model. But before we can compare
different models, we first have to decide upon a metric to measure
performance. One commonly used metric is classification
\textbf{accuracy}, which is defined as \textbf{the proportion of
correctly classified instances}.

    One legitimate question to ask is this: \textbf{how do we know which
model performs well on the final test dataset and real-world data if we
do not use this test set for the model selection, but keep it for the
final model evaluation?} In order to address the issue embedded in this
question, \textbf{different cross-validation techniques} can be used
where \textbf{the training dataset is further divided into training and
validation subsets in order to estimate the generalization performance
of the model}. Finally, \textbf{we also cannot expect that the default
parameters of the different learning algorithms provided by the software
libraries are optimal for our specific problem task}. Therefore, we will
make frequent use of \textbf{hyperparameter optimization techniques}
that help us to \textbf{fine-tune the performance of our model} in later
chapters. Intuitively, \textbf{we can think of those hyperparameters as
parameters that are not learned from the data but represent the knobs of
a model that we can turn to improve its performance}.

    \subsection{Evaluating models and predicting unseen data
instances}\label{evaluating-models-and-predicting-unseen-data-instances}

    After we have selected a model that has been fitted on the training
dataset, we can use the test dataset to estimate how well it performs on
this unseen data to estimate the generalization error. If we are
satisfied with its performance, we can now use this model to predict
new, future data. It is important to note that \textbf{the parameters
for the previously mentioned procedures, such as feature scaling and
dimensionality reduction, are solely obtained from the training dataset,
and the same parameters are later reapplied to transform the test
dataset, as well as any new samples, the performance measure on the test
data may be overly optimistic otherwise}.

    \section{Installing Python and packages from the Python Package
Index}\label{installing-python-and-packages-from-the-python-package-index}

    Its is recommended you use the most recent version of Python 3 that is
currently available, although most of the code examples may also be
compatible with Python 2.7 or higher.

    The additional packages that we will be using thoughout this material
can be installed via the \emph{pip} installer program, which has been
part of the Python standard library since Python 3.3.

    After we have successfully installed Python, we can execute \emph{pip}
from the terminal to install additional Python packages:

\textbf{pip install SomePackage}

    \subsection{Using the Anaconda Python distribution and package
manager}\label{using-the-anaconda-python-distribution-and-package-manager}

    A highly recommended alternative Python distribution for scientific
computing is Anaconda. Anaconda is a free, including for commercial use,
interprise-ready Python distribution that bundles all the essential
Python packages for data science, math, and engineering in one
user-friendly cross-plataform distribution.

    After successfully installing Anaconda, we can install new Python
packages using the following command:

\textbf{conda install SomePackage}

    \subsection{Packages for scientific computing, data science and machine
learning}\label{packages-for-scientific-computing-data-science-and-machine-learning}

    Throughout this material, we will mainly use \textbf{NumPy}'s
multidimensional arrays to store and manipulate data. Occasionally, we
will make use of \textbf{Pandas}, which is a library built on top of
NumPy that provides additional higher-level data manipulation tools that
make working with tabular data even more convenient. To augment our
learning experience and visualize quantitative data, which is often
extremely useful to intuitively make sense of it, we will use the very
customizable \textbf{Matplotlib} library. To machine learning models and
implementations we will use the library \textbf{scikit-learn}.

    \section{Summary}\label{summary}

    In this chapter, we explored machine learning at a very high level and
familiarized ourselves with the big picture and major concepts that we
are going ot explore in the following chapters in more detail. We
learned that supervised learning is composed of two important subfields:
classification and regression. While classification models allow us to
categorize objects into known classes, we can use regression analysis to
predict the continuous outcomes of target variables. Unsupervised
learning not only offers useful techniques for discovering structures in
unlabeled data, but it can also be useful for data compression in
feature preprocessing steps. We briefly went over the typical roadmap
for applying machine learning to problem tasks, which we will use as a
foundation for deeper discussions and hands-on examples in the following
chapters. Eventually, we set up our Python environment and installed the
required packages to get ready to see machine learning in action.

    Later in this material, in addition to machine learning itself, we will
also introduce different techniques to preprocess our dataset, which
will help us to get the best performance out of different machine
learning algorithms. While we will cover classification algorithms quite
extensively throughout the book, we will also explore different
techniques for regression analysis and clustering.

    In the following chapter, we will start our journey by implementing one
of the earliest machine learning algorithms for classification, which
will prepare us for Chapter 3, where we cover more advanced machine
learning algorithms using the scikit-learn open source machine learning
library.


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
