{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding a Machine Learning Model into a Web Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous chapters, you learned about the many different machine learning concepts and algorithms that can help us with better and more efficient decision-making. However, machine learning techniques are not limited to offline applications and analysis, and they can be the predictive engine of your web services. For example, popular and useful applications of machine learning models in web applications include spam detection in submission forms, search engines, recommendation systems for media or shopping portals, and many more. \n",
    "\n",
    "In this chapter, you will learn how to embed a machine learning model into a web application that can not only classify, but also learn from data in real time. The topics that we will cover are as follows: \n",
    "\n",
    "* Saving the current state of a trained machine learning model\n",
    "* Using SQLite databases for data storage\n",
    "* Developing a web application using the popular Flask web framework\n",
    "* Deploying a machine learning application to a public web server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serializing fitted scikit-learn estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a machine learning model can be computationally quite expensive, as we have seen previously. Surely we do not want to train our model every time we close our Python interpreter and want to make a new prediction or reload our web application? One option for model persistence is Python's in-built *pickle* module, which allows us to serialize and deserialize Python object structures to compact bytecode so that we can save our classifier in its current state and reload it if we want to classify new samples, without needing the model to learn from the training data all over again. Before you execute the following code, please make sure that you have trained the out-of-core logistic regression model from the last section of the previous chapter and have it ready in your current Python session: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import pickle \n",
    "import os\n",
    "\n",
    "dest = os.path.join('movieclassifier', 'pkl_objects')\n",
    "if not os.path.exists(dest):\n",
    "    os.makedirs(dest)\n",
    "    \n",
    "pickle.dump(stop, open(os.path.join(dest, 'stopwords.pkl'), 'wb'), protocol=4)\n",
    "pickle.dump(clf, open(os.path.join(dest, 'classifier.pkl'), 'wb'), protocol=4)\n",
    "''';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the preceding code, we create a *movieclassifier* directory where we will later store the files and data for our web application. Within this *movieclassifier* directory, we created a *pkl_objects* subdirectory to save the serialized Python objects to our local drive. Via the *dump* method of the *pickle* module, we then serialized the trained logistic regression model as well as the stop word set from the **Natural Language Toolkit (NLTK)** library, so that we do not have to install the NLTK vocabulary on our server. \n",
    "\n",
    "The *dump* method takes as its first argument the object that we want to pickle, and for the second argument we provied an open file object that the Python object will be written to. Via the *wb* argument inside the *open* function, we opened the file in binary mode for pickle, and we set *protocol=4* to choose the latest and most efficient pickle protocol that has been added to Python 3.4, which is compatible with Python 3.4 or newer. \n",
    "\n",
    "We do not need to pickle *HashingVectorizer*, since it does not need to be fitted. Instead, we can create a new Python script file from which we can import the vectorizer into our current Python session. Now, copy the following code and save it as *vectorizer.py* in the *movieclassifier* directory: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "cur_dir = os.path.dirname(__file__)\n",
    "stop = pickle.load(open(os.path.join(cur_dir,'pkl_objects','stopwords.pkl'), 'rb'))\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized\n",
    "\n",
    "vect = HashingVectorizer(decode_error='ignore',n_features=2**21,preprocessor=None,tokenizer=tokenizer)\n",
    "''';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have pickled the Python objects and created the *vectorizer.py* file, it would now be a good idea to restart our Python interpreter or IPython Notebook kernel to test if we can deserialize the objects without error. \n",
    "\n",
    "From your terminal, navigate to the *movieclassifier* directory, start a new Python session and execute the following code to verify that you can import the *vectorizer* and unpickle the classifier: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import os\n",
    "from movieclassifier.vectorizer import vect\n",
    "\n",
    "clf = pickle.load(open(os.path.join('movieclassifier', 'pkl_objects', 'classifier.pkl'), 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have successfully loaded the *vectorizer* and unpickled the classifier, we can now use these objects to preprocess documents samples and make predictions about their sentiment: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: positive\n",
      "Probability: 86.25%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "label = {0:'negative', 1:'positive'}\n",
    "\n",
    "example = ['I love this movie']\n",
    "X = vect.transform(example)\n",
    "print('Prediction: %s\\nProbability: %.2f%%'\n",
    "      % (label[clf.predict(X)[0]], np.max(clf.predict_proba(X))*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our classifier returns the class labels as integers, we defined a simple Python dictionary to map these integers to their sentiment. We then used *HashingVectorizer* to transform the simple example document into a word vector *x*. Finally, we used the *predict* method of the logist regression classifier to predict the class label, as well as the *predict_proba* method to return the corresponding probability of our prediction. Note that the *predict_proba* method call returns an array with a probability value for each unique class label. Since the class label with the largest probability corresponds to the class that is returned by the *predict* call, we used the *np.max* function to return the probability of the predicted class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up an SQLite database for data storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we wil set up a simple SQLite database to collect optional feedback about the predictions from users of the web application. We can use this feedback to update our classification model. SQLite is a open source SQL database engine that does not require a separate server to operate, which makes it ideal for smaller projects and simple web applications. Essentially, a SQLite dataset can be understood as a single, self-contained database file that allows us to directly access storage files. \n",
    "\n",
    "Furthermore, SQLite does not require any system-specific configuration and is support by all common operating systems. It has gained a reputation for being very reliable as it is used by popular companies, such as Google, Mozilla, Adobe, Apple, Microsoft, and many more. \n",
    "\n",
    "Fortunately, following Python's *batteries included* philosophy, there is already an API in the Python standard library, *sqlite3*, which allows us to work with SQLite databases. \n",
    "\n",
    "By executing the following code, we will create a new SQLite database inside the *movieclassifier* directory and store two example movie reviews: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "\n",
    "bd_file = os.path.join('movieclassifier', 'reviews.sqlite')\n",
    "\n",
    "if os.path.exists(bd_file):\n",
    "    os.remove(bd_file)\n",
    "\n",
    "conn = sqlite3.connect(bd_file)\n",
    "c = conn.cursor()\n",
    "c.execute(\"CREATE TABLE review_db\"\\\n",
    "          \" (review TEXT, sentiment INTEGER, date TEXT)\")\n",
    "\n",
    "example1 = \"I love this movie\"\n",
    "c.execute(\"INSERT INTO review_db\"\\\n",
    "          \" (review, sentiment, date) VALUES\"\\\n",
    "          \" (?, ?, DATETIME('now'))\", (example1, 1))\n",
    "\n",
    "example2 = \"I disliked this movie\"\n",
    "c.execute(\"INSERT INTO review_db\"\\\n",
    "          \" (review, sentiment, date) VALUES\"\\\n",
    "          \" (?, ?, DATETIME('now'))\", (example2, 0))\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the preceding code example, we created a connection (*conn*) to a SQLite database file by calling the *connect* method of the *sqlite3* library, which created the new database file *reviews.sqlite* in the *movieclassifier* directory if it did not already exist. Please note that SQLite does not implement a replace function for existing tables; you need to delete the database file manually from your file browser if you want to execute the code a second time. \n",
    "\n",
    "Next, we created a cursor via the *cursor* method, which allows us to transverse over the database records using the versatile SQL syntax. Via the first *execute* call, we then created a new database table, *review_db*. We used this to store and access database entries. Along with *review_db*, we also created three columns in this database table: *review*, *sentiment*, and *date*. We used these to store two example movie reviews and respective class labels (sentiments). \n",
    "\n",
    "Using the *DATETIME('now')* SQL command, we also added date and timestamps to our entries. In addition to the timestamps, we used the question mark symbol (*?*) to pass the movie review texts (*example1* and *example2*) and the corresponding class label (1 and 0) as positional arguments to the *execute* method, as members of a tuple. Lastly, we called the *commit* method to save the changes that we made to the database and closed the connection via the *close* method. \n",
    "\n",
    "To check if the entries have been stored in the database table correctly, we will now reopen the connection to the database and use the SQL *SELECT* command to fetch all rows in the database table that have been commited between the beginning of the year 2017 and today: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I love this movie', 1, '2018-04-13 09:50:09'), ('I disliked this movie', 0, '2018-04-13 09:50:09')]\n"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect(bd_file)\n",
    "c = conn.cursor()\n",
    "\n",
    "c.execute(\"SELECT * FROM review_db WHERE date\"\\\n",
    "          \" BETWEEN '2017-01-01 00:00:00' AND DATETIME('now')\")\n",
    "results = c.fetchall()\n",
    "\n",
    "conn.close()\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
