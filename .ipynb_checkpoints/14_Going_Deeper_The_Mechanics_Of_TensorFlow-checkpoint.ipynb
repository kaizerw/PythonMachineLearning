{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Going Deeper - The Mechanics of TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous chapter, we trained a multilayer perceptron to classify MNIST digits, using various aspects of the TensorFlow Python API. That was a great way to dive us straight into some hands-on experience with TensorFlow neural network training and machine learning.\n",
    "\n",
    "In this chapter, we will now shift our focus squarely on to TensorFlow itself, and explore in detail the impressive mechanics and features that TensorFlow offers:\n",
    "\n",
    "* Key features and advantages of TensorFlow\n",
    "* TensorFlow ranks and tensors\n",
    "* Understanding and working with TensorFlow graphs\n",
    "* Working with TensorFlow variables\n",
    "* TensorFlow operations with different scopes\n",
    "* Common tensor transformations: working with ranks, shapes, and types\n",
    "* Transforming tensors as multidimensional arrays\n",
    "* Saving and restoring a model in TensorFlow\n",
    "* Visualizing neural network graphs with TensorBoard\n",
    "\n",
    "We will stay hands-on in this chapter, of course, and implement graphs throughout the chapter to explore the main TensorFlow features and concepts. Along the way, we will also revisit a regression model, explore neural network graph visualization with TensorBoard, and suggest some ways that you could explore visualizing more of the graphs that you will make through this chapter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keys features of TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow gives us a scalable, multiplataform programming interface for implementing and running machine learning algorithms. The TensorFlow API has been relatively stable and mature since its 1.0 release in 2017. There are other deep learning libraries available, but they are still very experimental by comparison. \n",
    "\n",
    "A key feature of TensorFlow that we already noted is its ability to work with single or multiple GPUs. This allows users to train machine learning models very efficiently on large-scale systems. \n",
    "\n",
    "TensorFlow has strong growth drivers. Its development in funded and supported by Google, and so a large team of software engineers work on improvements continuosly. TensorFlow also has strong support from open source developers, who avidly contribute and provide user feedback. This has made the TensorFlow library more useful to both academic researchers and developers in their industry. A further consequence of these factors is that TensorFlow has extensive documentation and tutorials to help new users. \n",
    "\n",
    "Last but not least among these key features, TensorFlow supports mobile deployment, which makes it a very suitable tool for production. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow ranks and tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TensorFlow library lets users define operations and functions over tensors as computational graphs. Tensors are a generalizable mathematical notation for multidimensional arrays holding data values, where the dimensionality of a tensor is typically referred to as its **rank**. \n",
    "\n",
    "We have worked mostly, so far, with tensors of rank zero to two. For instance, a scalar, a single number such as an integer or float, is a tensor of rank 0. A vector is a tensor of rank 1, and a matrix is a tensor of rank 2. But, it does not stop here. The tensor notation can be generalized to higher dimensions - as we will see in the next chapter, when we work with an input of rank 3 and weight tensors of rank 4 to support images with multiple color channels. \n",
    "\n",
    "To make the concept of a **tensor** more intuitive, consider the following figure, which represents tensors of ranks 0 and 1 in the first row, and tensors of ranks 2 and 3 in the second row:\n",
    "\n",
    "<img src='images/14_01.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to get the rank and shape of a tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the *tf.rank* function to get the rank of a tensor. It is important to note that *tf.rank* will return a tensor as output, and in order to get the actual value, we will need to evaluate that tensor. \n",
    "\n",
    "In addition to the tensor rank, we can also get the shape of a TensoFlow tensor (similar to the shape of a NumPy array). For example, if $x$ is a tensor, we can get its shape using *x.get_shape()*, which will return an object of a special class called *TensorShape*.\n",
    "\n",
    "See the following examples on how to use the *tf.rank* function and the *get_shape* method of a tensor. The following code example illustrates how to retrieve the rank and shape of the tensor objects in a TensorFlow session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: () (4,) (2, 2)\n",
      "Ranks: 0 1 2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "g = tf.Graph()\n",
    "\n",
    "## define the computation graph\n",
    "with g.as_default():\n",
    "    ## define tensors t1, t2, t3\n",
    "    t1 = tf.constant(np.pi)\n",
    "    t2 = tf.constant([1, 2, 3, 4])\n",
    "    t3 = tf.constant([[1, 2], [3, 4]])\n",
    "    \n",
    "    ## get their ranks\n",
    "    r1 = tf.rank(t1)\n",
    "    r2 = tf.rank(t2)\n",
    "    r3 = tf.rank(t3)\n",
    "    \n",
    "    ## get their shapes\n",
    "    s1 = t1.get_shape()\n",
    "    s2 = t2.get_shape()\n",
    "    s3 = t3.get_shape()\n",
    "    \n",
    "    print('Shapes:', s1, s2, s3)\n",
    "    \n",
    "with tf.Session(graph=g) as sess:\n",
    "    print('Ranks:', r1.eval(), r2.eval(), r3.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the rank of the *t1* tensor is 0 since it is just a scalar (corresponding to the *[]* shape). The rank of the *t2* vector is 1, and since it has four elements, its shape is the one-element tuple *(4, )*. Lastly, the shape of the $2 \\times 2$ matrix *t3* is  2; thus, its corresponding shape is given by the *(2, 2)* tuple. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding TensorFlow's computation graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow relies on building a computation graph at its core, and it uses this computation graph to derive relationships between tensors from the input all the way to the output. Let's say, we have rank 0 (scalars) and tensors *a*, *b*, and *c* and we want to evaluate $z = 2 \\times (a - b) + c$. This evaluation can be represented as a computation graph, as shown in the following figure: \n",
    "\n",
    "<img src='images/14_02.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the computation graph is simply a network of nodes. Each node resembles an operation, which applies a function to its input tensor or tensors and returns zero or more tensors as the output. \n",
    "\n",
    "TensorFlow builds this computation graph and uses it to compute the gradients accordingly. The individual steps for building and compiling such a computation graph in TensorFlow are as follows: \n",
    "\n",
    "1. Instantiate a new, empty computation graph. \n",
    "2. Add nodes (the tensors and operations) to the computation graph. \n",
    "3. Execute the graph:\n",
    "    1. Start a new session\n",
    "    2. Initialize the variables in the graph\n",
    "    3. Run the computation graph in this session\n",
    "   \n",
    "So, let's create a graph for evaluating $z = 2 \\times (a - b) + c$, as shown in the previous figure, where *a*, *b*, and *c* are scalars (single numbers). Here, we define them as TensorFlows constants. A graph can be created by calling *tf.Graph()*, then nodes can be added to it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = tf.Graph()\n",
    "\n",
    "with g.as_default():\n",
    "    a = tf.constant(1, name='a')\n",
    "    b = tf.constant(2, name='b')\n",
    "    c = tf.constant(3, name='c')\n",
    "    \n",
    "    z = 2*(a-b) + c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, we added nodes to the *g* graph using *with g.as_default()*. If we do not explicitly create a graph, there is always a default graph, and therefore, all the nodes are added to the default graph. In this book, we try to avoid working with the default graph for clarity. This approach is specially useful when we are developing code in a Jupyter notebook, as we avoid pilling up unwanted nodes in the default graph by accident. \n",
    "\n",
    "A TensorFlow session is an environment in which the operations and tensors of a graph can be executed. A session object is created by calling *tf.Session* that can receive an existing graph (here, *g*) as an argument, as in *tf.Session(graph=g)*, otherwise, it will launch the default graph, which might be empty. \n",
    "\n",
    "After launching a graph in a TensorFlow session, we can execute it nodes; that is, evaluating its tensors or executing its operators. Evaluating each individual tensor involves calling its *eval* method inside the current session. When evaluating a specific tensor in the graph, TensorFlow has to execute all the preceding nodes in the graph until it reaches that particular one. In case there are one or more placeholders, they would need to be fed, as we will see later in the next section. \n",
    "\n",
    "Quite similarly, executing operations can be done using a session's *run* method. In the previous example, *train_op* is an operator that does not return any tensor. This operator can be executed as *train_op.run()*. Furthermore, there is a universal way of runnning both tensors and operators: *tf.Session().run()*. Using this method, as we will see later on as well, multiple tensors and operators can be placed in a list or tuple. As a result, *tf.Session().run()* will return a list or tuple of the same size. \n",
    "\n",
    "Here, we will launch the previous graph in a TensorFlow session and evaluate the tensor *z* as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2*(a-b)+c =>  1\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=g) as sess:\n",
    "    print('2*(a-b)+c => ', sess.run(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we define tensors and operations in a computation graph context within TensorFlow. A TensorFlow session is then used to execute the operations in the graph and fetch and evaluate the results. \n",
    "\n",
    "In this section, we saw how to define a computation graph, how to add nodes to it, and how to evaluate the tensors in a graph within a TensorFlow session. We will now take a deeper look into the different types of nodes that can appear in a computation graph, including placeholders and variables. Along the way, we will see some other operators that do not return a tensor as the output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Placeholders in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow has special mechanisms for feeding data. One of these mechanisms is the use of placeholders, which are predefined tensors with specific types and shapes. \n",
    "\n",
    "These tensors are added to the computation graph using the *tf.placeholder* function, and they do not contain any data. However, upon the execution of certain nodes in the graph, these placeholders need to be fed with data arrays. \n",
    "\n",
    "In the following sections, we will see how to define placeholders in a graph and how to feed them with data values upon execution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining placeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you now know, placeholders are defined using the *tf.placeholder* function. When we define placeholders, we need to decide what their shape ant type should be, according to the shape and type of the data that will be fed through them upn execution. \n",
    "\n",
    "Let's start with a simple example. In the following code, we will define the same graph that was shown in the previous section for evaluating $z = 2 \\times (a-b) + c$. This times, however, we use placeholders for the scalars *a*, *b*, and *c*. Also, we store the intermediate tensors associated with *r1* and *r2*, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    tf_a = tf.placeholder(tf.int32, shape=[], name='tf_a')\n",
    "    tf_b = tf.placeholder(tf.int32, shape=[], name='tf_b')\n",
    "    tf_c = tf.placeholder(tf.int32, shape=[], name='tf_c')\n",
    "    \n",
    "    r1 = tf_a - tf_b\n",
    "    r2 = 2*r1\n",
    "    z = r2 + tf_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, we defined three placeholders, named *tf_a*, *tf_b*, and *tf_c*, using type *tf.int32* (32-bit integers) and set their shape via *shape=[]* since they are scalars (tensors of rank 0). In the current book, we always precede the placeholder objects with *tf_* for clarity and to be able to distinguish them from the other tensors. \n",
    "\n",
    "Note that in the previous code example, we were dealing with scalars, and therefore, their shapes were specified as *shape=[]* However, it is very straightforward to define placeholders of higher dimensions. For example, a rank 3 placeholder of type *float* and shape $3 \\times 4 \\times 5$ can be defined as *tf.placeholder(dtype=tf.float32, shape=[2, 3, 4])*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feeding placeholders with data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we execute a node in the graph, we need to create a Python **dictionary** to feed the values of placeholders with data arrays. We do this according to the type and shape of the placeholders. This dictionary is passed as the input argument *feed_dict* to a session's *run* method. \n",
    "\n",
    "In the previous graph, we added three placeholders of the type *tf.int32* to feed scalars for computing *z*. Now, in order to evaluate the result tensor *z*, we can feed arbitrary integer values (here, 1, 2, and 3) to the placeholders, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z: 1\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=g) as sess:\n",
    "    feed = {tf_a: 1, tf_b: 2, tf_c: 3}\n",
    "    print('z:', sess.run(z, feed_dict=feed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that having extra arrays for placeholders does not cause any error; it is just redundant to do so. However, if a placeholder is needed for the execution of a particular node, and is not provided via the *feed_dict* argument, it will cause a runtime error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining placeholders for data arrays with varying batchsizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, when we are developing a neural network model, we may deal with mini-batches of data that have different sizes. For example, we may train a neural network with a specific mini-batch size, but we want to use the network to make predictions on one or more data point. \n",
    "\n",
    "A useful feature of placeholders is that can specify *None* for the dimension that is varying in size. For example, we can create a placeholder of rank 2, where the first dimension is unknown (or may vary), as shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "g = tf.Graph()\n",
    "\n",
    "with g.as_default():\n",
    "    tf_x = tf.placeholder(tf.float32, \n",
    "                          shape=[None, 2], \n",
    "                          name='tf_x')\n",
    "    \n",
    "    x_mean = tf.reduce_mean(tf_x, axis=0, name='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can evaluate *x_mean* with two different input, *x1* and *x2*, which are NumPy arrays of shape *(5, 2)* and *(10, 2)*, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feeding data with shape  (5, 2)\n",
      "Result: [0.62 0.47]\n",
      "Feeding data with shape  (10, 2)\n",
      "Result: [0.46 0.49]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(123)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    x1 = np.random.uniform(low=0, high=1, size=(5, 2))\n",
    "    print('Feeding data with shape ', x1.shape)\n",
    "    print('Result:', sess.run(x_mean, feed_dict={tf_x: x1}))\n",
    "    \n",
    "    x2 = np.random.uniform(low=0, high=1, size=(10, 2))\n",
    "    print('Feeding data with shape ', x2.shape)\n",
    "    print('Result:', sess.run(x_mean, feed_dict={tf_x: x2}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, if we try printing the object *tf_x*, we will get *Tensor(\"tf_x:0\", shape=(?, 2), dtype=float32)*, which shows that the shape of this tensor is *(?, 2)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of TensorFlow, variables are a special type of tensor objects that allow us to store and update the parameters of our models in a TensorFlow session during training. The following sections explain how we can define variables in a graph, initialize those variables in a session, organize variables via the so-called variable scope, and reuse existing variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow variables store the parameters of a model than can be updated during training, for example, the weights in the input, hidden and output layers of a neural network. When we define a variable, we need to initialize it with a tensor of values. \n",
    "\n",
    "TensorFlow provides two ways for dealing with variables: \n",
    "* *tf.Variable(<initial-value>, name='variable-name')*\n",
    "* *tf.get_variable(name, ...)*\n",
    "\n",
    "The first one, *tf.Variable*, is a class that creates an object for a new variable and adds it to the graph. Note that *tf.Variable* does not have an explicit way to determine *shape* and *dtype*; the shape and type are set to be the same as those of the initial values. \n",
    "\n",
    "The second option, *tf.get_variable*, can be used to **reuse** an existing variable with a given name (if the name exists in the graph) or create a new one if the name does not exist. In this case, the name becomes critical; that is probably why it has to be placed as the first argument to this function. Furthermore, *tf.get_variable* provides an explicit way to set *shape* and *dtype*; these parameters are only required when creating a new variable, not reusing existing ones. \n",
    "\n",
    "The advantage of *tf.get_variable* over *tf.Variable* is twofold: *tf.get_variable* allows us to reuse existing variables and it already uses the popular Xavier/Glorot initialization scheme by default. Besides the initializer, the *get_variable* function provides other parameters to control the tensor, such as adding a regularizer for the variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Xavier/Glorot initialization**\n",
    "\n",
    "In the early development of deep learning, it was observed that random uniform or random normal weight initialization could often result in a poor performance of the model during training. \n",
    "\n",
    "In 2010, Xavier Glorot and Yoshua Bengio investigated the effect of initialization and proposed a novel, more robust initialization scheme to facilitate the training of deep networks. \n",
    "\n",
    "The general idea behind Xavier initialization is to roughly balance the variance of the gradients across different layers. Otherwise, one layer may get too much attention during training while the other layer lags behind. \n",
    "\n",
    "According to the research paper by Glorot and Bengio, if we want to initialize the weights from uniform distribution, we should choose the interval of this uniform distribution as follows: \n",
    "\n",
    "$$W ~ Uniform \\left(-\\frac{\\sqrt{6}}{\\sqrt{n_{in} + n_{out}}}, \\frac{\\sqrt{6}}{\\sqrt{n_{in} + n_{out}}} \\right)$$\n",
    "\n",
    "Here, $n_{in}$ is the number of input neurons that are multiplied with the weights, and $n_{out}$ is the number of output neurons that feed into the next layer. For initializing the weights from Gaussian (normal) distribution, the authors recommended choosing the standard deviation of this Gaussian to be $\\sigma = \\frac{\\sqrt{2}}{\\sqrt{n_{in} + n_{out}}}$.\n",
    "\n",
    "TensorFlow support Xavier initialization in both uniform and normal distributions of weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In either initialization technique, it is important to note that the initial values are not set until we launch the graph in *tf.Session* and explicitly run the initializer operator in that session. In fact, the required memory for a graph is not allocated until we initialize the variables in a TensorFlow session. \n",
    "\n",
    "Here is an example of creating a variable object where the initial values are created from a NumPy array. The *dtype* data type of this tensor is *tf.int64*, which is automatically **inferred** from its NumPy array input: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'w:0' shape=(2, 4) dtype=int64_ref>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "g1 = tf.Graph()\n",
    "\n",
    "with g1.as_default():\n",
    "    w = tf.Variable(np.array([[1, 2, 3, 4], \n",
    "                              [5, 6, 7, 8]]), name='w')\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, it is critical to understand that tensors defined as variables are not allocated in memory and contain no values until they are initialized. Therefore, before executing any node in the computation graph, we *must* initialize the variables that are within the path to the node that we want to execute. \n",
    "\n",
    "This initialization process refers to allocating memory for the associated tensors and assigning their initial values. TensorFlow provides a function named *tf.global_variables_initializer* that returns an operator for initializing all the variables that exist in a computation graph. Then, executing this operator will initialize the variables as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3 4]\n",
      " [5 6 7 8]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=g1) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also store this operator in an object such as *init_op = tf.global_variables_initializer()* and execute this operator later using *sess.run(init_op)* or *init_op.run()*. However, we need to make sure that this operator is created after we define all the variables. \n",
    "\n",
    "For example, in the following code, we define the variable *w1*, then we define the operator *init_op*, followed by the variable *w2*: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "g2 = tf.Graph()\n",
    "\n",
    "with g2.as_default():\n",
    "    w1 = tf.Variable(1, name='w1')\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    w2 = tf.Variable(2, name='w2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's evaluate *w1* as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1: 1\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=g2) as sess:\n",
    "    sess.run(init_op)\n",
    "    print('w1:', sess.run(w1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works fine. Now, let's try evaluating *w2*: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FailedPreconditionError",
     "evalue": "Attempting to use uninitialized value w2\n\t [[Node: _retval_w2_0_0 = _Retval[T=DT_INT32, index=0, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](w2)]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    517\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value w2\n\t [[Node: _retval_w2_0_0 = _Retval[T=DT_INT32, index=0, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](w2)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-c10f8d6b307c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mg2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'w2:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value w2\n\t [[Node: _retval_w2_0_0 = _Retval[T=DT_INT32, index=0, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](w2)]]"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=g2) as sess:\n",
    "    sess.run(init_op)\n",
    "    print('w2:', sess.run(w2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the code example, executing the graph raises an error because *w2* was not initialized via *sess.run(init_op)*, and therefore, could not be evaluated. The operator *init_op* was defined prior to adding *w2* to the graph; thus, executing *init_op* will not initialize *w2*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this subsection, we are going to discuss *scoping*, which is an important concept in TensorFlow, and especially useful if we are constructing large neural network graphs. \n",
    "\n",
    "With variable scopes, we can organize the variables into separate subparts. When we create a variable scope, the name of operations and tensors that are created within that scope are prefixed with that scope, and those scopes can further be nested. For example, if we have two subnetworks, where each subnetwork has several layers, we can define two scopes named *'net_A'* and *'net_B'*, respectively. Then, each layer will be defined within one of these scopes. \n",
    "\n",
    "Let's see how the variable names will turn out in the following code example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'net_A/layer-1/weights:0' shape=(10, 4) dtype=float32_ref>\n",
      "<tf.Variable 'net_A/layer-2/weights:0' shape=(20, 10) dtype=float32_ref>\n",
      "<tf.Variable 'net_B/layer-1/weights:0' shape=(10, 4) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "g = tf.Graph()\n",
    "\n",
    "with g.as_default():\n",
    "    with tf.variable_scope('net_A'):\n",
    "        with tf.variable_scope('layer-1'):\n",
    "            w1 = tf.Variable(tf.random_normal(shape=(10, 4)), name='weights')\n",
    "        with tf.variable_scope('layer-2'):\n",
    "            w2 = tf.Variable(tf.random_normal(shape=(20, 10)), name='weights')\n",
    "    with tf.variable_scope('net_B'):\n",
    "        with tf.variable_scope('layer-1'):\n",
    "            w3 = tf.Variable(tf.random_normal(shape=(10, 4)), name='weights')\n",
    "        \n",
    "    print(w1)\n",
    "    print(w2)\n",
    "    print(w3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the variables names are now prefixed with their nested scopes, separated by the forward slash *'/'* symbol. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's imagine that we are developing a somewhat complex neural network model that has a classifier whose input data comes from more than once source. For example, we will assume that we have data $(X_A, y_A)$ coming from source $A$ and data $(X_B, y_B)$ comes from the source $B$. In this example, we will design our graph in such a way that it will use the data from only one source as input tensor to build the network. Then, we can feed the data from the other source to the same classifier. \n",
    "\n",
    "In the following example, we assume that data from source $A$ is fed through placeholder, and source $B$ is the output of a generator network. We will build by calling the *build_generator* function within the *generator* scope, then we will add a classifier by calling *build_classifier* within the *classifier* scope: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def build_classifier(data, labels, n_classes=2):\n",
    "    data_shape = data.get_shape().as_list()\n",
    "    weights = tf.get_variable(name='weights', \n",
    "                              shape=(data_shape[1], \n",
    "                                     n_classes), \n",
    "                              dtype=tf.float32)\n",
    "    bias = tf.get_variable(name='bias', \n",
    "                           initializer=tf.zeros(shape=n_classes))\n",
    "    logits = tf.add(tf.matmul(data, weights), bias, name='logits')\n",
    "    return logits, tf.nn.softmax(logits)\n",
    "\n",
    "def build_generator(data, n_hidden):\n",
    "    data_shape = data.get_shape().as_list()\n",
    "    w1 = tf.Variable(tf.random_normal(shape=(data_shape[1], n_hidden)), name='w1')\n",
    "    b1 = tf.Variable(tf.zeros(shape=n_hidden), name='b1')\n",
    "    hidden = tf.add(tf.matmul(data, w1), b1, name='hidden_pre-activation')\n",
    "    hidden = tf.nn.relu(hidden, 'hidden_activation')\n",
    "    \n",
    "    w2 = tf.Variable(tf.random_normal(shape=(n_hidden, data_shape[1])), name='w2')\n",
    "    b2 = tf.Variable(tf.zeros(shape=data_shape[1]), name='b2')\n",
    "    \n",
    "    output = tf.add(tf.matmul(hidden, w2), b2, name='output')\n",
    "    return output, tf.nn.sigmoid(output)\n",
    "\n",
    "batch_size = 64\n",
    "g = tf.Graph()\n",
    "\n",
    "with g.as_default():\n",
    "    tf_X = tf.placeholder(shape=(batch_size, 100), \n",
    "                          dtype=tf.float32, \n",
    "                          name='tf_X')\n",
    "    with tf.variable_scope('generator'):\n",
    "        gen_out1 = build_generator(data=tf_X, n_hidden=50)\n",
    "    \n",
    "    with tf.variable_scope('classifier') as scope:\n",
    "        cls_out1 = build_classifier(data=tf_X, \n",
    "                                    labels=tf.ones(shape=batch_size))\n",
    "        scope.reuse_variables()\n",
    "        cls_out2 = build_classifier(data=gen_out1[1], labels=tf.zeros(shape=batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we have called the *build_classifier* function two times. The first call causes the building of the network. Then, we call *scope.reuse_variables()* and call that function again. As a result, the second call does not create new variables; instead, it reuses the same variables. Alternatively, we could reuse the variables by specifying the *reuse=True* parameter, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = tf.Graph()\n",
    "\n",
    "with g.as_default():\n",
    "    tf_X = tf.placeholder(shape=(batch_size, 100), \n",
    "                          dtype=tf.float32, \n",
    "                          name='tf_X')\n",
    "    with tf.variable_scope('generator'):\n",
    "        gen_out1 = build_generator(data=tf_X, n_hidden=50)\n",
    "    \n",
    "    with tf.variable_scope('classifier'):\n",
    "        cls_out1 = build_classifier(data=tf_X, \n",
    "                                    labels=tf.ones(shape=batch_size))\n",
    "        \n",
    "    with tf.variable_scope('classifier', reuse=True):\n",
    "        cls_out2 = build_classifier(data=gen_out1[1], labels=tf.zeros(shape=batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we have discussed how to define computational graphs and variables in TensorFlow, a detailed discussion of how we can compute gradients in a computational graph is beyond the scope of this book, where we use TensorFlow's convenient optimizer classes that perform backpropagation automatically for us. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
