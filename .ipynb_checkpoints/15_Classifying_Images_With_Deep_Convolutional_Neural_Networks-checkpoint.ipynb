{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Images with Deep Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous chapter, we looked in depth at different aspects of the TensorFlow API, became familiar with tensors, naming variables, and operators, and learned how to work with variable scopes. In this chapter, we will now learn about **Convolutional Neural Networks (CNNs)**, and how we can implement CNNs in TensorFlow. We will also take an interesting journey in this chapter and we apply this type of deep neural network architecture to image classification. \n",
    "\n",
    "So we will start by discussing the basic building blocks of CNNs, using a bottom-up approach. Then we will take a deeper dive into the CNN architecture and how to implement deep CNNs in TensorFlow. Along the way we will be covering the following topics: \n",
    "\n",
    "* Understanding convolution operations in one or two dimensions\n",
    "* Learning about the building blocks of CNN architectures\n",
    "* Implementing deep convolutional neural networks in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building blocks of convolutional neural networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional neural networks, or CNNs, are a family of models that were inspired by how the visual cortex of human brain works when recognizing objects. \n",
    "\n",
    "The development of CNNs goes back to the 1990's, when Yann LeCun and his colleagues proposed a novel neural network architecture for classifying handwritten digits from images. \n",
    "\n",
    "Due to the outstanding performance of CNNs for image classification tasks, they have gained a lot of attention and this led to tremendous improvements in machine learning and computer vision applications. \n",
    "\n",
    "In the following sections, we next see how CNNs are used as feature extraction engines, and then we will delve into the theoretical definition of convolution and computing convolution in one and two dimensions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding CNNs and learning feature hierarchies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Successfully extrating **salient (relevant) features** is key to the performance of any machine learning algorithm, of course, and traditional machine learning models rely on input features that may come from a domain expert, or are based on computational feature extraction techniques. Neural networks are able to automatically learn the features from raw data that are most useful for a particular task. For this reason, it is common to consider a neural network as a feature extraction engine: the early layers (those right after the input layer) extract **low-level features**. \n",
    "\n",
    "Multilayer neural networks, and in particular, deep convolutional neural networks, construct a so-called **feature hierarchy** by combining the low-level features in a layer-wise fashion to form high-level features. For example, if we are dealing with images, then low-level features, such as edges and blobs, are extracted from the earlier layers, which are combined together to form high-level features - as object shapes like a building, a car, or a dog. \n",
    "\n",
    "As you can see in the following image, a CNN computes **feature maps** from an input image, where each element comes from a local patch of pixel in the input image:\n",
    "\n",
    "<img src='images/15_01.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This local patch of pixels is referred to as the **local receptive field**. CNNs will usually perform very well for image-related tasks, and that is largely due to two important ideas: \n",
    "\n",
    "* **Sparse-connectivity**: A single element in the feature map is connected to only a small patch of pixels. (This is very different from connecting to the whole input image, in the case of perceptrons. You may find it useful to look back and compare how we implemented a fully connected network that connected to the whole image). \n",
    "* **Parameter-sharing**: The same weights are used for different patches of the input image. \n",
    "\n",
    "As a direct consequence of these two ideas, the number of weights (parameters) in the network decreases dramatically, and we see an improvement in the ability to capture **salient** features. Intuitively, it makes sense that nearby pixels are probably more relevant to each other than pixels that are far away from each other. \n",
    "\n",
    "Typically, CNNs are composed for several **Convolutional (conv)** layers and subsampling (also known as **Pooling (P)**) layers that are followed by one or more **Fully Connected (FC)** layers at the end. The fully connected layers are essentially a multilayer perceptron, where every input unit $i$ is connected to every output unit $j$ with weight $w_{ij}$. \n",
    "\n",
    "Please note that subsampling layers, commonly known as **pooling layers**, do not have any learnable parameters; for instance, there are no weights or bias units in pooling layers. However, both convolution and fully connected layers have such weights and biases. \n",
    "\n",
    "In the following sections, we will study convolutional and pooling layers in more detail and see how they work. To understand how convolution operations work ,let's start with a convolution in one dimension before working through the typical two-dimensional cases as applications for two-dimensional images later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing discrete convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **discrete convolution** (or simply **convolution**) is a fundamental operation in a CNN. Therefore, it is important to understand how this operation works. In this section, we will learn the mathematical definition and discuss some of the **naive** algorithms to compute convolutions of two one-dimensional vectors or two two-dimensional matrices. \n",
    "\n",
    "Please note that this description is solely for understanding how a convolution works. Indeed, much more efficient implementations of convolutional operations already exist in packages such as TensorFlow, as we will see later in this chapter. \n",
    "\n",
    "In this chapter, we will use subscripts to denote the size of a multidimensional array; for example, $A_{n_1 \\times n_2}$ is a two-dimensional array of size $n_1 \\times n_2$. We use brackets $[]$ to denote the indexing of a multidimensional array. For example, $A[i, j]$ means the element at index $i, j$ of matrix $A$. Furthermore, note that we use a special symbol $*$ to denote the convolution operation between two vectors or matrices, which is not to be confused with the multiplication operator $*$ in Python. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing a discrete convolution in one dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with some basic definitions and notations we are going to use. A discrete convolution for two one-dimensional vectors $x$ and $w$ is denoted by $y = x * w$, in which vector $x$ is our input (sometimes called **signal**) and $w$ is called the **filter** or **kernel**. A discrete convolution is mathematically defined as follows:\n",
    "\n",
    "$$y = x * w \\rightarrow y[i] = \\sum_{k=-\\infty}^{+\\infty} x[i-k]w[k]$$\n",
    "\n",
    "Here, the brackets $[]$ are used to denote the indexing for vector elements. The index $i$ runs through each element of the output vector $y$. There are two odds things in the preceding formula that we need to clarify: $-\\infty$ to $+\\infty$ indices and negative indexing for $x$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-correlation (or simply correlation) between the input vector and a filter is denoted by $y = x * w$ and is very much like a sibling for a convolution with a small difference; the difference is that in cross-correlation, the multiplication is performed in the same direction. Therefore, it is not required to rotate the filter matrix $w$ in each dimension. Mathematically, cross-correlation is defined as follows: \n",
    "\n",
    "$$y = x * w \\rightarrow y[i] = \\sum_{k=-\\infty}^{+\\infty} x[i+k]w[k]$$\n",
    "\n",
    "The same rules for padding and stride may be applied to cross-correlation as well. \n",
    "\n",
    "The first issue where the sum runs through indices from $-\\infty$ to $+\\infty$ seems odd mainly because in machine learning applications, we always deal with finite feature vectors. For example, if $x$ has 10 features with indices $0, 1, \\ldots, 9$, then indices $-\\infty: -1$ and $10: +\\infty$ are out of bounds for $x$. Therefore, to correctly compute the summation shown in the preceding formula, it is assumed that $x$ and $w$ are filled with zeros. This will result in an output vector $y$ that also has infinite size with lots of zeros as well. Since this is not useful in practical situations, $x$ is padded only with a finite number of zeros. \n",
    "\n",
    "This process is called **zero-padding** or simply **padding**. Here, the number of zeros padded on each side is denoted by $p$. An example padding of a one-dimensional vector $x$ is shown in the following figure:\n",
    "\n",
    "<img src='images/15_02.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume that the original input $x$ and filter $w$ have $n$ and $m$ elements, respectively, where $m \\le n$. Therefore, the padded vector $x^p$ has size $n + 2p$. Then, the practical formula for computing a discrete convolution will change to the following:\n",
    "\n",
    "$$y = x * w \\rightarrow y[i] = \\sum_{k=0}^{k=m-1} x^p[i+m-k]w[k]$$\n",
    "\n",
    "Now that we have solved the infinite index issue, the second issue is indexing $x$ with $i+m-k$. The important point to notice here is that $x$ and $w$ are indexed in different directions in this summation. For this reason, we can flip one of those vectors, $x$ or $w$, after they are padded. Then, we can simply compute their dot product. \n",
    "\n",
    "Let's assume we flip the filter $w$ to get the rotated filter $w^r$. Then, the dot product $x[i:i+m].w^r$ is computed to get one element $y[i]$, where $x[i:i+m]$ is a patch of $x$ with size $m$. \n",
    "\n",
    "This operation is repeated like in a sliding window approach to get all the output elements. The following figure provides an example with $x = [3, 2, 1, 7, 1, 2, 5, 4]$ and $w = [\\frac{1}{2}, \\frac{3}{4}, 1, \\frac{1}{4}]$ so that the first three output elements are computed:\n",
    "\n",
    "<img src='images/15_03.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see in the preceding example that the padding size is zero ($p=0$). Notice that the rotated filter $w^r$ is shifted by two cells each time we shift. This **shift** is another hyperparameter of a convolution, the **stride** s. In this example, the stride is two, $s=2$. Note that the stride has to be a positive number smaller than the size of the input vector. We will talk more about padding and strides in the next section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The effect of zero-padding in a convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far here, we have used zero-padding in convolutions to compute finite-sized output vectors. Technically, padding can be applied with any $p \\ge 0$. Depending on the choice $p$, boundary cells may be treated differently than the cells localed in the middle of $x$. \n",
    "\n",
    "Now consider an example where $n=5, m=3$. Then, $p=0, x[0]$ is only used in computing one output element (for instance, $y[0]$), while $x[1]$ is used in the computation of two output elements (for instance, $y[0]$ and $y[1]$). So, you can see that this different treatment of elements of $x$ can artificially put more emphasis on the middle element, $x[2]$, since it has appeared in most computations. We can avoid this issue if we choose $p=2$, in which case, each element of $x$ will be involved in computing three elements of $y$. \n",
    "\n",
    "Furthermore, the size of the output $y$ also depends on the choice of the padding strategy we use. There are three modes of padding that are commonly used in practice: **full**, **same**, and **valid**: \n",
    "\n",
    "* In the **full** mode, the padding parameter $p$ is set to $p=m-1$. Full padding increases the dimensions of the output; thus, it is rarely used in convolutional neural network architectures.\n",
    "* **Same** padding is usually used if you want to have the size of the output the same as the input vector $x$. In this case, the padding parameter $p$ is computed according to the filter size, along with the requirement that the input size and output size are the same. \n",
    "* Finally, computing a convolution in the **valid** mode refers to the case where $p=0$ (no padding). \n",
    "\n",
    "The following figure illustrates the three different padding modes for a simple $5 \\times 5$ pixel input with a kernel size of $3 \\times 3$ and a stride of 1:\n",
    "\n",
    "<img src='images/15_11.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most commonly used padding mode in convolutional neural networks is **same** padding. One of its advantage over the other padding modes is that same padding preserves the height and width of the input images or tensors, which makes designing a network architecture more convenient. \n",
    "\n",
    "One big disadvantage of the **valid** padding versus **full** and **same** padding, for example, is that the volume of the tensors would decrease substantially in neural networks with many layers, which can be detrimental to the network performance. \n",
    "\n",
    "In practice, it is recommended that you preserve the spatial size using same padding for the convolutional layers and decrease the spatial size via pooling layers instead. As for the full padding, its size results in an output larger than the input size. Full padding is usually used in signal processing applications where it is important to minimize boundary effects. However, in deep learning context, boundary effect is not usually an issue, so we rarely see full padding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determining the size of the convolution output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output size of a convolution is determined by the total number of times hat we shift the filter $w$ along the input vector. Let's assume that the input vector has size $n$ and the filter is of size $m$. Then, the size of the output resulting from $x*m$ with padding $p$ and stride $s$ is determined as follows:\n",
    "\n",
    "$$o = \\left\\lfloor \\frac{n+2p-m}{s} \\right\\rfloor + 1$$\n",
    "\n",
    "Here, $\\lfloor\\rfloor$ denotes the floor operation.\n",
    "\n",
    "Consider the following two cases:\n",
    "\n",
    "* Compute the output size for an input vector of size 10 with a convolution kernel of size 5, padding 2, and stride 1: \n",
    "$$n=10,m=5,p=2,s=1 \\rightarrow o = \\left\\lfloor \\frac{10+2.2-5}{1} \\right\\rfloor + 1 = 10$$\n",
    "(Note that in this case, the output size turns out to be the smae as the input; therefore, we conclude this as **mode='same'**).\n",
    "* How can the output size change for the same input vector, but have a kernel of size 3, and stride 2?\n",
    "$$n=10,m=3,p=2,s=2 \\rightarrow o = \\left\\lfloor \\frac{10+2.2-3}{2} \\right\\rfloor + 1 = 6$$\n",
    "\n",
    "Finally, in order to learn how to compute convolutions in one dimension, a naive implementation is shown in the following code block, and the results are compared with the *numpy.convolve* function. The code is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1d implementation: [ 5. 14. 16. 26. 24. 34. 19. 22.]\n",
      "Numpy results: [ 5 14 16 26 24 34 19 22]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def conv1d(x, w, p=0, s=1):\n",
    "    w_rot = np.array(w[::-1])\n",
    "    x_padded = np.array(x)\n",
    "    if p > 0:\n",
    "        zero_pad = np.zeros(shape=p)\n",
    "        x_padded = np.concatenate([zero_pad, x_padded, zero_pad])\n",
    "    res = []\n",
    "    for i in range(0, int(len(x)/s), s):\n",
    "        res.append(np.sum(x_padded[i:i+w_rot.shape[0]] * w_rot))\n",
    "    return np.array(res)\n",
    "\n",
    "## testing \n",
    "x = [1, 3, 2, 4, 5, 6, 1, 3]\n",
    "w = [1, 0, 3, 1, 2]\n",
    "\n",
    "print('Conv1d implementation:', conv1d(x, w, p=2, s=1))\n",
    "print('Numpy results:', np.convolve(x, w, mode='same'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, here, we have explored the convolution in 1D. We started with 1D case to make the concepts easier to understand. In the next section, we will extend this to two dimensions, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing a discrete convolution in 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concepts you learned in the previous sections are easily extendible to two dimensions. When we deal with two-dimensional input, such as matrix $X_{n_1 \\times n_2}$ and the filter matrix $W_{m_1 \\times m_2}$, where $m_1 \\le n1$ and $m_2 \\le n_2$, then the matrix $Y = X * W$ is the result of 2D convolution of $X$ and $W$. This is mathematically defined as follows: \n",
    "\n",
    "$$Y = X * W \\rightarrow Y[i, j] = \\sum_{k1=-\\infty}^{+\\infty}\\sum_{k2=-\\infty}^{+\\infty}X[i-k_1, j-k_2]W[k_1, k_2]$$\n",
    "\n",
    "Notice that if you omit one of the dimensions, the remaining formula is exactly the same as the one we used previously to compute the convolution in 1D. In fact, all the previously mentioned techniques, such as zero-padding, rotating the filter matrix, and the use of strides, are also applicable to 2D convolutions, provided that they are extended to both the dimensions independently. The following example illustrates the computation of a 2D convolution between an input matrix $X_{3\\times3}$, a kernel matrix $W_{3\\times3}$, padding $p=(1, 1)$, and stride $s=(2, 2)$. According to the specified padding, one layer of zeros are padded on each side of the input matrix, which results in the padded matrix $X_{5 \\times 5}^{padded}$, as follows:\n",
    "\n",
    "<img src='images/15_04.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the preceding filter, the rotate filter will be:\n",
    "\n",
    "$$\n",
    "W^r = \n",
    "\\begin{bmatrix}\n",
    "0.5 & 1.0 & 0.5 \\\\\n",
    "0.1 & 0.4 & 0.3 \\\\\n",
    "0.4 & 0.7 & 0.5\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Note that this rotation is not the same as the transpose matrix. To the the rotated filter in NumPy, we can write *W_rot = W[::-1, ::-1]*. Next, we can shift the rotated filter matrix along the padded input matrix $X^{padded}$ like a sliding window and compute the sum of the element-wise product, which is denoted by the $\\odot$ operator in the following picture:\n",
    "\n",
    "<img src='images/15_05.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result will be the $2 \\times 2$ matrix $Y$. \n",
    "\n",
    "Let's also implement the 2D convolution according to the **naive** algorithm described. The *scipy.signal* package provides a way to compute 2D convolution via the *scipy.signal.convolve2d* function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d implementation:\n",
      " [[11. 25. 32. 13.]\n",
      " [19. 25. 24. 13.]\n",
      " [13. 28. 25. 17.]\n",
      " [11. 17. 14.  9.]]\n",
      "SciPy results:\n",
      " [[11 25 32 13]\n",
      " [19 25 24 13]\n",
      " [13 28 25 17]\n",
      " [11 17 14  9]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.signal\n",
    "\n",
    "def conv2d(X, W, p=(0, 0), s=(1, 1)):\n",
    "    W_rot = np.array(W)[::-1, ::-1]\n",
    "    X_orig = np.array(X)\n",
    "    n1 = X_orig.shape[0] + 2*p[0]\n",
    "    n2 = X_orig.shape[1] + 2*p[1]\n",
    "    X_padded = np.zeros(shape=(n1, n2))\n",
    "    X_padded[p[0]:p[0]+X_orig.shape[0], \n",
    "             p[1]:p[1]+X_orig.shape[1]] = X_orig\n",
    "    res = []\n",
    "    for i in range (0, int((X_padded.shape[0] - W_rot.shape[0])/s[0])+1, s[0]):\n",
    "        res.append([])\n",
    "        for j in range(0, int((X_padded.shape[1] - W_rot.shape[1])/s[1])+1, s[1]):\n",
    "            X_sub = X_padded[i:i+W_rot.shape[0], j:j+W_rot.shape[1]]\n",
    "            res[-1].append(np.sum(X_sub * W_rot))\n",
    "    return np.array(res)\n",
    "\n",
    "X = [[1, 3, 2, 4], [5, 6, 1, 3], [1, 2, 0, 2], [3, 4, 3, 2]]\n",
    "W = [[1, 0, 3], [1, 2, 1], [0, 1, 1]]\n",
    "\n",
    "print('Conv2d implementation:\\n', conv2d(X, W, p=(1, 1), s=(1, 1)))\n",
    "print('SciPy results:\\n', scipy.signal.convolve2d(X, W, mode='same'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provided a naive implementation to compute a 2D convolution for the purpose of understand the concepts. However, this implementation is very inefficient in terms of memory requirements and computational complexity. Therefore, it should not be used in real-world neural network applications. \n",
    "\n",
    "In recent years, much more efficient algorithms have been developed that use the Fourier transformation for computing convolutions. It is also important to note that in the context of neural networks, the size of a convolutional kernel is usually much smaller than the size of the input image. For example, modern CNNs usually use kernel sizes such as $1 \\times 1$, $3 \\times 3$ or $5 \\times 5$, for which efficient algorithms have been designed that can carry out the convolutional operations much more efficiently, such as the **Winograd's Minimal Filtering** algorithm.\n",
    "\n",
    "In the next section, we will discuss subsampling, which is another important operation often used in CNNs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
