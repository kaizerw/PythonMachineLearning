{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelizing Neural Network Training with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter, we will move on from the mathematical foundations of machine learning and deep learning to introduce TensorFlow. TensorFlow is one of the most popular deep learning libraries currently available, and it can let us implement neural networks much more efficiently than any of our previous NumPy implementations. In this chapter, we will start using TensorFlow and see how it brings significant benefits to training performance. \n",
    "\n",
    "This chapter begins the next stage of our journey into training machine learning and deep learning, and we will explore the following topics:\n",
    "\n",
    "* How TensorFlow improves training performance\n",
    "* Working with TensorFlow to write optimized machine learning code\n",
    "* Using TensorFlow high-level APIs to build a multilayer neural network\n",
    "* Choosing activation functions for artificial neural networks\n",
    "* Introducing Keras, a high-level wrapper around TensorFlow, for implementing common deep learning architectures most conveniently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow and training performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow can speep up our machine learning tasks significantly. To understand how it can do this, let's begin by discussing some of the performance challenges we typically run into when we run expensive calculations on our hardware. \n",
    "\n",
    "The performance of computer processors has, of course, been improving continuously over recent years, and that has allowed us to train more powerful and complex learning systems, and so to improve the predictive performance of our machine learning models. Even the cheapest desktop computer hardware that is available right now comes with processing units that have multiple cores. \n",
    "\n",
    "Also, in the previous chapter, we saw that many functions in scikit-learn allowed us to spread those computations over multiple processing units. However, by default, Python is limited to execution on one core due to the **Global Interpreter Lock (GIL)**. So, although we, indeed, take advantage of its multiprocessing library to distribute our computations over multiple cores, we still have to consider that the most advanced desktop hardware rarely comes with more than 8 or 16 such cores. \n",
    "\n",
    "If we recall from the previous chapter, where we implemented a very simple multilayer perceptron with only one hidden layer consisting of 100 units, we had to optimize approximately 80,000 weight parameters ([784\\*100 + 100] + [100 \\* 10] + 10 = 79,510) to learn a model for a very simple image classification task. The images in MNIST are rather small (28 x 28 pixels), and we can only imagine the explosion in the number of parameters if we want to add additional hidden layers or work with images that have higher pixel densities. \n",
    "\n",
    "Such a task would quickly become unfeasible for a single processing unit. The question then becomes - how can we tackle such problems more effectively?\n",
    "\n",
    "The obvious solution to this problem is to use GPUs, which are real work horses. You can think of a graphics card as a small computer cluster inside your machine. Another advantage is that modern GPUs are relatively cheap compared to the state-of-the-art CPUs, as we can see in the following overview:\n",
    "\n",
    "<img src='images/13_05.png'>\n",
    "\n",
    "At 70 percent of the price of a modern CPU, we can get a GPU that has 450 times more cores and is capable of around 15 times more floating-point calculations per second. So, what is holding us back from utilizing GPUs for our machine learning tasks?\n",
    "\n",
    "The challenge is that writing code to target GPU is not as simple as executing Python code in our interpreter. There are special packages, such as CUDA and OpenCL, that allows us to target the GPU. However, writing code in CUDA or OpenCL is probably not the most convenient environment for implementing and running machine learning algorithms. The good news is that this is what TensorFlow was developed for!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is TensorFlow?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow is a scalable and multiplataform programming interface for implementing and running machine learning algorithms, including convenience wrappers for deep learning. \n",
    "\n",
    "TensorFlow was developed by the researchers and engineers of the Google Brain team; and while the main development is led by a team of researchers and software engineers at Google, its development also involves many contributions from the open source community. TensorFlow was initially built for only internal use at Google, but it was subsequently released in November 2015 under a permissive open source license. \n",
    "\n",
    "To improve the performance of training machine learning models, TensorFlow allows execution on both CPUs and GPUs. However, its greatest performance capabilities can be discovered when using GPUs. TensorFlow supports CUDA-enabled GPUs officially. Support for OpenCL-enabled devices is still experimental. However, OpenCL will likely be officially supported in near future. \n",
    "\n",
    "TensorFlow currently supports frontend interfaces for a number of programming languages. Luckly for us as Python users, TensorFlow's Python API is currently the most complete API, thereby attracting many machine learning and deep learning practitioners. Furthermore, TensorFlow has an official API in C++.\n",
    "\n",
    "The APIs in other languages, such as Java, Haskell, Node.js, and Go, are not stable yet, but the open source community and TensorFlow developers are constantly improving them. TensorFlow computations rely on constructing a directed graph for representing the data flow. Even though building the graph may sound complicated, TensorFlow comes with high-level APIs that has made it very easy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How we will learn TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will learn first of all about the low-level TensorFlow API. While implementing models at this level can be a little bit cumbersome at first, the advantage of the low-level API is that it gives us more flexibility as programmers to combine the basic operations and develop complex machine learning models. Starting from TensorFlow version 1.1.0, high-level APIs are added on top of the low-level API (for instance, the so-called Layers and Estimators API), which allow building and prototyping models much faster.\n",
    "\n",
    "After learning about the low-level API, we will move forward to explore two high-level APIs, namely TensorFlow **Layers** and **Keras**. However, let's begin by taking our first steps with TensorFlow low-level API, and ease ourselves into how everything works. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First steps with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will take our first steps in using the low-level TensorFlow API. Depending on how your system is set up ,you can typically just use Python's *pip* installed and install TensorFlow from PyPI by executing *pip install tensorflow* command from your terminal. \n",
    "\n",
    "In case you want to use GPUs, the CUDA toolkit as well as the NVIDIA cuDNN library need to be installed; then you can install TensorFlow with GPU support by executing *pip install tensorflow-gpu*.\n",
    "\n",
    "TensorFlow is built around a computation graph composed of a set of nodes. Each node represents an operation that may have zero or more input or output. The values that flow through the edges of the computation graph are called **tensors**. \n",
    "\n",
    "Tensors can be understood as a generalization of scalars, vectors, matrices, and so on. More concretely, a scalar can be defined as a rank-0 tensor, a vector as a rank-1 tensor, a matrix as a rank-2 tensor, and matrices stacked in a third dimension as rank-3 tensors. \n",
    "\n",
    "Once a computation graph is built, the graph can be launched in a TensorFlow *Session* for executing different nodes of the graph. In a next chapter, we will cover the steps in building the computation graph and launching the graph in a session in more detail. \n",
    "\n",
    "As a warm-up exercise, we will start with the use of simple scalars from TensorFlow to compute a net input $z$ of a sample point $x$ in a one-dimensional dataset with weight $w$ and bias $b$:\n",
    "\n",
    "$z = w \\times x + b$\n",
    "\n",
    "The following code shows the implementation of this equation in the low-level TensorFlow API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= 1.0 --> z= 2.7\n",
      "x= 0.6 --> z= 1.9\n",
      "x=-1.8 --> z=-2.9\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "## create a graph\n",
    "g = tf.Graph()\n",
    "\n",
    "with g.as_default():\n",
    "    x = tf.placeholder(dtype=tf.float32, shape=(None), \n",
    "                       name='x')\n",
    "    w = tf.Variable(2.0, name='weight')\n",
    "    b = tf.Variable(0.7, name='bias')\n",
    "    \n",
    "    z = w*x + b\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "## create a session and pass in graph g\n",
    "with tf.Session(graph=g) as sess:\n",
    "    ## initialize w anb b:\n",
    "    sess.run(init)\n",
    "    ## evaluate z:\n",
    "    for t in [1.0, 0.6, -1.8]:\n",
    "        print('x=%4.1f --> z=%4.1f'\n",
    "              % (t, sess.run(z, feed_dict={x:t})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was pretty straightforward, right? In general, when we develop for a model in the TensorFlow low-level API, we need to define placeholders for input data ($x$, $y$, and sometimes other tunable parameters); then, define the weight matrices and build the model from input to output. If this is an optimization problem, we should define the loss or cost function and determine which optimization algorithm to use. TensorFlow will create a graph that contains all the symbols that we have defined as nodes in this graph. \n",
    "\n",
    "Here, we created a placeholder for $x$ with *shape=(None)*. This allows us to feed the values in an element-by-element form and as a batch of input data at once, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.7 4.7 6.7]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=g) as sess:\n",
    "    sess.run(init)\n",
    "    print(sess.run(z, feed_dict={x: [1., 2., 3.]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with array structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's discuss how to use array structures in TensorFlow. By executing the following code, we will create a simple rank-3 tensor of size $\\text{batchsize} \\times 2 \\times 3$, reshape it, and calculate the column sums using TensorFlow's optimized expressions. Since we do not know the batch size a priori, we specify *None* for the batch size in the argument of the *shape* parameter of the placeholder $x$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:  (3, 2, 3)\n",
      "Reshaped:\n",
      " [[ 0.  1.  2.  3.  4.  5.]\n",
      " [ 6.  7.  8.  9. 10. 11.]\n",
      " [12. 13. 14. 15. 16. 17.]]\n",
      "Column Sums:\n",
      " [18. 21. 24. 27. 30. 33.]\n",
      "Column Means:\n",
      " [ 6.  7.  8.  9. 10. 11.]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    x = tf.placeholder(dtype=tf.float32, \n",
    "                       shape=(None, 2, 3), \n",
    "                       name='input_x')\n",
    "    \n",
    "    x2 = tf.reshape(x, shape=(-1, 6), name='x2')\n",
    "    \n",
    "    ## calculate the sum of each column\n",
    "    xsum = tf.reduce_sum(x2, axis=0, name='col_sum')\n",
    "    \n",
    "    ## calculate the mean of each column\n",
    "    xmean = tf.reduce_mean(x2, axis=0, name='col_mean')\n",
    "    \n",
    "with tf.Session(graph=g) as sess:\n",
    "    x_array = np.arange(18).reshape(3, 2, 3)\n",
    "    \n",
    "    print('input shape: ', x_array.shape)\n",
    "    print('Reshaped:\\n', \n",
    "          sess.run(x2, feed_dict={x: x_array}))\n",
    "    print('Column Sums:\\n', \n",
    "          sess.run(xsum, feed_dict={x: x_array}))\n",
    "    print('Column Means:\\n', \n",
    "          sess.run(xmean, feed_dict={x: x_array}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we worked with three functions - *tf.reshape*, *tf.reduce_sum*, and *tf.reduce_mean*. Note that for reshaping, we used the value $-1$ for the first dimension. This is because we do not know the value of the batch size; when reshaping a tensor, if you use $-1$ for a specific dimension, the size of that dimension will be computed according to the total size of the tensor and the remaining dimension. Therefore, *tf.reshape(tensor, shape=(-1,))* can be used to flatten a tensor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing a simple model with the low-level TensorFlow API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have familiarized ourselves with TensorFlow, let's take a look at a really practical example and implement **Ordinary Least Squares (OLS)** regression. \n",
    "\n",
    "Let's start by creating a small one-dimensional toy dataset with 10 training samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "X_train = np.arange(10).reshape((10, 1))\n",
    "y_train = np.array([1.0, 1.3, 3.1, 2.0, 5.0, \n",
    "                    6.3, 6.6, 7.4, 8.0, 9.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this dataset, we want to train a linear regression model to predict the ouput $y$ from the input $x$. Let's implement this model in a class, which we name *TfLinreg*. For this, we would need two placeholders - one for the input $x$ and one for $y$ for feeding the data into our model. Next, we need to define the trainable variables - weights $w$ and bias $b$. \n",
    "\n",
    "Then, we can define the linear regression model as $z = w \\times x + b$, followed by defining the cost function to be the **Mean of Squared Error (MSE)**. To learn the weight parameters of the model, we use the gradient descent optimizer. The code is as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfLinreg(object):\n",
    "    \n",
    "    def __init__(self, x_dim, learning_rate=0.01, \n",
    "                 random_seed=None):\n",
    "        self.x_dim = x_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.g = tf.Graph()\n",
    "        ## build the model\n",
    "        with self.g.as_default():\n",
    "            ## set graph-level random-seed\n",
    "            tf.set_random_seed(random_seed)\n",
    "            \n",
    "            self.build()\n",
    "            \n",
    "            ## create initializer\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "            \n",
    "    def build(self):\n",
    "        ## define placeholders for inputs\n",
    "        self.X = tf.placeholder(dtype=tf.float32, \n",
    "                                shape=(None, self.x_dim), \n",
    "                                name='x_input')\n",
    "        self.y = tf.placeholder(dtype=tf.float32, \n",
    "                                shape=(None), \n",
    "                                name='y_input')\n",
    "        print(self.X)\n",
    "        print(self.y)\n",
    "        \n",
    "        ## define weight matrix and bias vector\n",
    "        w = tf.Variable(tf.zeros(shape=(1)), \n",
    "                        name='weight')\n",
    "        b = tf.Variable(tf.zeros(shape=(1)), \n",
    "                        name='bias')\n",
    "        print(w)\n",
    "        print(b)\n",
    "        \n",
    "        self.z_net = tf.squeeze(w*self.X + b, name='z_net')\n",
    "        print(self.z_net)\n",
    "        \n",
    "        sqr_errors = tf.square(self.y - self.z_net, \n",
    "                               name='sqr_errors')\n",
    "        print(sqr_errors)\n",
    "        self.mean_cost = tf.reduce_mean(sqr_errors, \n",
    "                                        name='mean_cost')\n",
    "        \n",
    "        optimizer = tf.train.GradientDescentOptimizer(\n",
    "                        learning_rate=self.learning_rate, \n",
    "                        name='GradientDescent')\n",
    "        self.optimizer = optimizer.minimize(self.mean_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have defined a class to construct our model. We will create an instance of this class and call it *lrmodel*, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"x_input:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"y_input:0\", dtype=float32)\n",
      "<tf.Variable 'weight:0' shape=(1,) dtype=float32_ref>\n",
      "<tf.Variable 'bias:0' shape=(1,) dtype=float32_ref>\n",
      "Tensor(\"z_net:0\", dtype=float32)\n",
      "Tensor(\"sqr_errors:0\", dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "lrmodel = TfLinreg(x_dim=X_train.shape[1], learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *print* statements that we wrote in the *build* method will display information about six nodes in the graph - $X$, $y$, $w$, $b$, $z\\_net$, and $sqr\\_errors$ - with their names and shapes. \n",
    "\n",
    "The next step is to implement a training function to learn the weights of the linear regression model. Note that $b$ is the bias unit (the $y$-axis intercept at $x=0$). \n",
    "\n",
    "For training, we implement a separate function that need a TensorFlow session, a model instance, training data, and the number of epochs as input arguments. In this function, first we initialize the variables in the TensorFlow session using the *init_op* operation defined in the mode. Then, we iterate and call the *optimizer* operation of the model while feeding the training data. This function will return a list of training costs as a side product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linreg(sess, model, X_train, y_train, num_epochs=10):\n",
    "    ## initialize all variables: W and b\n",
    "    sess.run(model.init_op)\n",
    "    \n",
    "    training_costs = []\n",
    "    for i in range(num_epochs):\n",
    "        _, cost = sess.run([model.optimizer, model.mean_cost], \n",
    "                           feed_dict={model.X: X_train, \n",
    "                                      model.y: y_train})\n",
    "        training_costs.append(cost)\n",
    "    \n",
    "    return training_costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now we can create a new TensorFlow session to launch the *lrmodel.g* graph and pass all the required arguments to the *train_linreg* function for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session(graph=lrmodel.g)\n",
    "training_costs = train_linreg(sess, lrmodel, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the training costs these 10 epochs to see whether the model is converged or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbYAAAEmCAYAAAAOb7UzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWFGwiYhIrCjYREQkVv4/sMQ32vPL/DMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(1, len(training_costs) + 1), training_costs)\n",
    "plt.tight_layout()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Cost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the following plot, this simple model converges very quickly after a few epochs. \n",
    "\n",
    "So far so good. Looking at the cost function, it seems that we built a working regression model from this particular dataset. Now, let's compile a new function to make predictions based on the input features. For this function, we need the TensorFlow session, the model, and the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_linreg(sess, model, X_test):\n",
    "    y_pred = sess.run(model.z_net, \n",
    "                      feed_dict={model.X: X_test})\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing a predict function was pretty straightforward; just running *z_net* defined in the graph computes the predicted output values. Next, let's plot the linear regression fit on the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUUxiQBERUUz6/4eq33vvYTPgAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X_train, y_train, marker='s', s=50, \n",
    "            label='Training Data')\n",
    "plt.plot(range(X_train.shape[0]), \n",
    "         predict_linreg(sess, lrmodel, X_train), \n",
    "         color='gray', marker='o', markersize=16, \n",
    "         linewidth=3, label='LinReg Model')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the resulting plot, our model fits the training data points appropriately. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training neural networks efficiently with high-level TensorFlow APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will take a look at two high-level TensorFlow APIs - the Layers API (*tensorflow.layers* or *tf.layers*) and the Keras API (*tensorflow.contrib.keras*). Keras can be installed as a separate package. It supports Theano or TensorFlow as backend. \n",
    "\n",
    "However, after the release of TensorFlow 1.1.0, Keras has been added to the TensorFlow *contrib* submodule. It is very likely that the Keras subpackage will be moved outside the experimental *contrib* submodule and become one of the main TensorFlow submodules soon. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building multilayer neural networks using TensorFlow's Layers API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what neural network training via the *tensorflow.layers (tf.layers)* high-level API looks like, let's implement a multilayer perceptron to classify the handwritten digits from the MNIST dataset, which we introduced in the previous chapter. \n",
    "\n",
    "Note that TensorFlow also provides the same dataset as follows: \n",
    "\n",
    "*import tensorflow as tf*\n",
    "*from tf.examples.tutorials.mnist import input_data*\n",
    "\n",
    "However, we work with the MNIST dataset as an external dataset to learn all the steps of data preprocessing separately. This way, you would learn what you need to do with your own dataset. \n",
    "\n",
    "After downloading and unzipping the archives, we place the files in the *mnist* directory in our current working directory so that we can load the training as well as the test dataset, using the *load_mnist(path, kind)* function we implemented before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import struct\n",
    "import numpy as np\n",
    "\n",
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"Load MNIST data from 'path'\"\"\"\n",
    "    labels_path = os.path.join(path, '%s-labels-idx1-ubyte' % kind)\n",
    "    images_path = os.path.join(path, '%s-images-idx3-ubyte' % kind)\n",
    "    \n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II', lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath, dtype=np.uint8)\n",
    "        \n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", imgpath.read(16))\n",
    "        images = np.fromfile(imgpath, dtype=np.uint8).reshape(len(labels), 784)\n",
    "        images = ((images / 255.) - .5) * 2\n",
    "    \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 60000, Columns: 784\n",
      "Rows: 10000, Columns: 784\n",
      "(60000, 784) (60000,)\n",
      "(10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "## loading the data\n",
    "X_train, y_train = load_mnist('./mnist/', kind='train')\n",
    "print('Rows: %d, Columns: %d' % (X_train.shape[0], \n",
    "                                 X_train.shape[1]))\n",
    "X_test, y_test = load_mnist('./mnist/', kind='t10k')\n",
    "print('Rows: %d, Columns: %d' % (X_test.shape[0], \n",
    "                                 X_test.shape[1]))\n",
    "## mean centering and normalization:\n",
    "mean_vals = np.mean(X_train, axis=0)\n",
    "std_val = np.std(X_train)\n",
    "\n",
    "X_train_centered = (X_train - mean_vals) / std_val\n",
    "X_test_centered = (X_test - mean_vals) / std_val\n",
    "\n",
    "del X_train, X_test\n",
    "\n",
    "print(X_train_centered.shape, y_train.shape)\n",
    "print(X_test_centered.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start building our model. We will start by creating two placeholders, named *tf_x* and *tf_y*, and then build a multilayer perceptron as done before, but with three fully connected layers. \n",
    "\n",
    "However, we will replace the logistic units in the hidden layers with hyperbolic tangent activation functions (*tanh*), replace the logistic function in the output layer with *softmax*, and add an additional hidden layer. \n",
    "\n",
    "The *tanh* and *softmax* functions are new activation functions. We will learn more about these activation functions in the next section: *Choosing activation functions for multilayer neural networks*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "n_features = X_train_centered.shape[1]\n",
    "n_classes = 10\n",
    "random_seed = 123\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    tf.set_random_seed(random_seed)\n",
    "    tf_x = tf.placeholder(dtype=tf.float32, \n",
    "                          shape=(None, n_features), \n",
    "                          name='tf_x')\n",
    "    tf_y = tf.placeholder(dtype=tf.int32, \n",
    "                          shape=None, \n",
    "                          name='tf_y')\n",
    "    y_onehot = tf.one_hot(indices=tf_y, depth=n_classes)\n",
    "    \n",
    "    h1 = tf.layers.dense(inputs=tf_x, units=50, \n",
    "                         activation=tf.tanh, \n",
    "                         name='layer1')\n",
    "    \n",
    "    h2 = tf.layers.dense(inputs=h1, units=50, \n",
    "                         activation=tf.tanh, \n",
    "                         name='layer2')\n",
    "    \n",
    "    logits = tf.layers.dense(inputs=h2, units=10, \n",
    "                             activation=None, \n",
    "                             name='layer3')\n",
    "    \n",
    "    predictions = {\n",
    "        'classes': tf.argmax(logits, axis=1, \n",
    "                             name='predicted_classes'), \n",
    "        'probabilities': tf.nn.softmax(logits, \n",
    "                                       name='softmax_tensor')\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the cost functions and add an operator for initializing the model variables as well as an optimization operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define cost function and optimizer\n",
    "with g.as_default():\n",
    "    cost = tf.losses.softmax_cross_entropy(onehot_labels=y_onehot, \n",
    "                                           logits=logits)\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "    \n",
    "    train_op = optimizer.minimize(loss=cost)\n",
    "    \n",
    "    init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start training the network, we need a way to generate batches of data. For this, we implement the following function that returns a generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch_generator(X, y, batch_size=128, shuffle=False):\n",
    "    X_copy = np.array(X)\n",
    "    y_copy = np.array(y)\n",
    "    \n",
    "    if shuffle:\n",
    "        data = np.column_stack((X_copy, y_copy))\n",
    "        np.random.shuffle(data)\n",
    "        X_copy = data[:, :-1]\n",
    "        y_copy = data[:, -1].astype(int)\n",
    "        \n",
    "    for i in range(0, X.shape[0], batch_size):\n",
    "        yield (X_copy[i:i+batch_size, :], y_copy[i:i+batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can create a new TensorFlow session, initialize all the variables in our network, and train it. We also display the average training los after each epoch to monitors the learning process later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- Epoch  1  Avg. Training Loss: 1.5380\n",
      " -- Epoch  2  Avg. Training Loss: 0.9406\n",
      " -- Epoch  3  Avg. Training Loss: 0.7420\n",
      " -- Epoch  4  Avg. Training Loss: 0.6328\n",
      " -- Epoch  5  Avg. Training Loss: 0.5622\n",
      " -- Epoch  6  Avg. Training Loss: 0.5129\n",
      " -- Epoch  7  Avg. Training Loss: 0.4761\n",
      " -- Epoch  8  Avg. Training Loss: 0.4476\n",
      " -- Epoch  9  Avg. Training Loss: 0.4250\n",
      " -- Epoch 10  Avg. Training Loss: 0.4060\n",
      " -- Epoch 11  Avg. Training Loss: 0.3902\n",
      " -- Epoch 12  Avg. Training Loss: 0.3765\n",
      " -- Epoch 13  Avg. Training Loss: 0.3646\n",
      " -- Epoch 14  Avg. Training Loss: 0.3539\n",
      " -- Epoch 15  Avg. Training Loss: 0.3445\n",
      " -- Epoch 16  Avg. Training Loss: 0.3361\n",
      " -- Epoch 17  Avg. Training Loss: 0.3282\n",
      " -- Epoch 18  Avg. Training Loss: 0.3210\n",
      " -- Epoch 19  Avg. Training Loss: 0.3145\n",
      " -- Epoch 20  Avg. Training Loss: 0.3084\n",
      " -- Epoch 21  Avg. Training Loss: 0.3028\n",
      " -- Epoch 22  Avg. Training Loss: 0.2973\n",
      " -- Epoch 23  Avg. Training Loss: 0.2923\n",
      " -- Epoch 24  Avg. Training Loss: 0.2876\n",
      " -- Epoch 25  Avg. Training Loss: 0.2831\n",
      " -- Epoch 26  Avg. Training Loss: 0.2788\n",
      " -- Epoch 27  Avg. Training Loss: 0.2746\n",
      " -- Epoch 28  Avg. Training Loss: 0.2708\n",
      " -- Epoch 29  Avg. Training Loss: 0.2671\n",
      " -- Epoch 30  Avg. Training Loss: 0.2636\n",
      " -- Epoch 31  Avg. Training Loss: 0.2602\n",
      " -- Epoch 32  Avg. Training Loss: 0.2570\n",
      " -- Epoch 33  Avg. Training Loss: 0.2536\n",
      " -- Epoch 34  Avg. Training Loss: 0.2507\n",
      " -- Epoch 35  Avg. Training Loss: 0.2478\n",
      " -- Epoch 36  Avg. Training Loss: 0.2450\n",
      " -- Epoch 37  Avg. Training Loss: 0.2422\n",
      " -- Epoch 38  Avg. Training Loss: 0.2395\n",
      " -- Epoch 39  Avg. Training Loss: 0.2369\n",
      " -- Epoch 40  Avg. Training Loss: 0.2345\n",
      " -- Epoch 41  Avg. Training Loss: 0.2320\n",
      " -- Epoch 42  Avg. Training Loss: 0.2297\n",
      " -- Epoch 43  Avg. Training Loss: 0.2275\n",
      " -- Epoch 44  Avg. Training Loss: 0.2252\n",
      " -- Epoch 45  Avg. Training Loss: 0.2230\n",
      " -- Epoch 46  Avg. Training Loss: 0.2209\n",
      " -- Epoch 47  Avg. Training Loss: 0.2190\n",
      " -- Epoch 48  Avg. Training Loss: 0.2170\n",
      " -- Epoch 49  Avg. Training Loss: 0.2151\n",
      " -- Epoch 50  Avg. Training Loss: 0.2131\n"
     ]
    }
   ],
   "source": [
    "## create a session to launch the graph\n",
    "sess = tf.Session(graph=g)\n",
    "## run the variable initialization operator\n",
    "sess.run(init_op)\n",
    "\n",
    "## 50 epochs of training:\n",
    "for epoch in range(50):\n",
    "    training_costs = []\n",
    "    batch_generator = create_batch_generator(X_train_centered, \n",
    "                                             y_train, \n",
    "                                             batch_size=64, \n",
    "                                             shuffle=True)\n",
    "    for batch_X, batch_y in batch_generator:\n",
    "        ## prepare a dict to feed data to our network:\n",
    "        feed = {tf_x: batch_X, tf_y: batch_y}\n",
    "        _, batch_cost = sess.run([train_op, cost], feed_dict=feed)\n",
    "        training_costs.append(batch_cost)\n",
    "    print(' -- Epoch %2d  '\n",
    "          'Avg. Training Loss: %.4f' % (epoch+1, np.mean(training_costs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training process may take a couple of minutes. Finally, we can use the trained model to do predictions on the test dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 93.53%\n"
     ]
    }
   ],
   "source": [
    "## do prediction on the test set:\n",
    "feed = {tf_x: X_test_centered}\n",
    "y_pred = sess.run(predictions['classes'], feed_dict=feed)\n",
    "print('Test Accuracy: %.2f%%' \n",
    "       % (100*np.sum(y_pred==y_test)/y_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that by leveraging high-level APIs, we can quickly build a model and test it. Therefore, a high-level API is very useful for prototyping our ideas and quickly checking the results. \n",
    "\n",
    "Next, we will develop a similar classification model for MNIST using Keras, which is another high-level TensorFlow API. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing a multilayer neural network with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The development of Keras started in the early months of 2015. As of today, it has evolved into one of the most popular and widely used libraries that is built on top of Theano and TensorFlow. \n",
    "\n",
    "Similar to TensorFlow, the Keras allows us to utilize our GPUs to accelerate neural network training. One of its prominent features is that it has a very intuitive and use-friendly API, which allows us to implement neural networks in only a few lines of code. \n",
    "\n",
    "Keras was first released as a standalone API that could leverage Theano as a backend, and the support for TensorFlow was added later. Keras is also integrated into TensorFlow from version 1.1.0. Therefore, if you have TensorFlow version 1.1.0, no more installation is needed for Keras. \n",
    "\n",
    "Currently, Keras is part of the *contrib* module (which contains packages developed by contributors to TensorFlow and is considered experimental code). In future releases of TensorFlow, it may be moved to become a separate module in the TensorFlow main API. \n",
    "\n",
    "Note that you may have to change the code from *import tensorflow.contrib.keras as keras* to *import tensorflow.keras as keras* in future versions of TensorFlow in the following code examples. \n",
    "\n",
    "On the following pages, we will walk through the code examples for using Keras step by step. Using the same functions described in the previous section, we need to load the data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 60000, Columns: 784\n",
      "Rows: 10000, Columns: 784\n",
      "(60000, 784) (60000,)\n",
      "(10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "## loading the data\n",
    "X_train, y_train = load_mnist('./mnist/', kind='train')\n",
    "print('Rows: %d, Columns: %d' % (X_train.shape[0], \n",
    "                                 X_train.shape[1]))\n",
    "X_test, y_test = load_mnist('./mnist/', kind='t10k')\n",
    "print('Rows: %d, Columns: %d' % (X_test.shape[0], \n",
    "                                 X_test.shape[1]))\n",
    "## mean centering and normalization:\n",
    "mean_vals = np.mean(X_train, axis=0)\n",
    "std_val = np.std(X_train)\n",
    "\n",
    "X_train_centered = (X_train - mean_vals) / std_val\n",
    "X_test_centered = (X_test - mean_vals) / std_val\n",
    "\n",
    "del X_train, X_test\n",
    "\n",
    "print(X_train_centered.shape, y_train.shape)\n",
    "print(X_test_centered.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's set the random seed for NumPy as TensorFlow so that we get consistent results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.keras as keras\n",
    "\n",
    "np.random.seed(123)\n",
    "tf.set_random_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To continue with the preparation of the training data, we need to convert the class labels (integers 0-9) into the one-hot format. Fortunately, Keras provides a convenient tool for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 3 labels:  [5 0 4]\n",
      "\n",
      "First 3 labels (one-hot):\n",
      " [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "y_train_onehot = keras.utils.to_categorical(y_train)\n",
    "print('First 3 labels: ', y_train[:3])\n",
    "print('\\nFirst 3 labels (one-hot):\\n', y_train_onehot[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can get to the interesting part and implement a neural network. Briefly, we will have three layers, where the first two layers each have 50 hidden units with the *tanh* activation functions and the last layer has 10 layers for the 10 class labels and uses *softmax* to give the probability of each class. Keras makes these tasks very simple, as you can see in the following code implementation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Dense(units=50, \n",
    "                             input_dim=X_train_centered.shape[1], \n",
    "                             kernel_initializer='glorot_uniform', \n",
    "                             bias_initializer='zeros', \n",
    "                             activation='tanh'))\n",
    "\n",
    "model.add(keras.layers.Dense(units=50, \n",
    "                             input_dim=50, \n",
    "                             kernel_initializer='glorot_uniform', \n",
    "                             bias_initializer='zeros', \n",
    "                             activation='tanh'))\n",
    "\n",
    "model.add(keras.layers.Dense(units=y_train_onehot.shape[1], \n",
    "                             input_dim=50, \n",
    "                             kernel_initializer='glorot_uniform', \n",
    "                             bias_initializer='zeros', \n",
    "                             activation='softmax'))\n",
    "\n",
    "sgd_optimizer = keras.optimizers.SGD(lr=0.001, decay=1e-7, \n",
    "                                     momentum=0.9)\n",
    "\n",
    "model.compile(optimizer=sgd_optimizer, \n",
    "              loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we initialize a new model using the *Sequential* class to implement a feedforward neural network. Then, we can add as many layers to it as we like. However, since the first layer that we add is the input layer, we have to make sure that the *input_dim* attribute matches the number of features (columns) in the training (784 features or pixels in the neural network implementation). \n",
    "\n",
    "Also, we have to make sure that the number of output units (*units*) and input units (*input_dim*) of two consecutive layers match. In the preceding example, we added two hidden layers with 50 hidden units plus one bias unit each. The number of units in the output layer should be equal to the number of unique class labels - the number of columns in the one-hot-encoded class label array. \n",
    "\n",
    "Note that we used a new initialization algorithm for weight matrices by setting *kernel_initializer='glorot_uniform'*. Glorot initialization (also known as Xavier initialization) is a more robust way of initialization for deep neural networks. The biases are initialized to zero, which is more common, and in fact the default in Keras. \n",
    "\n",
    "Before we can compile our model, we also have to define an optimizer. In the preceding example, we chose a stochastic gradient descent optimization, which we are already familiar with from previous chapters. Furthermore, we can set values for the weight decay constant and momentum learning to adjust the learning rate at each epoch. Lastly, we set the cost (or loss) function to *categorial_crossentropy*. \n",
    "\n",
    "The binary cross-entropy is just a technical term of the cost function in the logistic regression, and the categorical cross-entropy is its generalization for multiclass predictions via softmax, which we will cover in the section *Estimating class probabilities in multiclass classification via the softmax function* later in this chapter. \n",
    "\n",
    "After compiling the model, we can now train it by calling the *fit* method. Here, we are using mini-batch stochastic gradient with a batch size of 64 training samples per batch. We train the MLP over 50 epochs, and we can follow the optimization of the cost function during training by setting *verbose=1*.\n",
    "\n",
    "The *validation_split* parameter is especially handy since it will reserve 10 percent of the training data (here, 6,000 samples) for validation after each epoch so that we can monitor whether the model is overfitting during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/50\n",
      "54000/54000 [==============================] - 2s 38us/step - loss: 0.7528 - val_loss: 0.3674\n",
      "Epoch 2/50\n",
      "54000/54000 [==============================] - 2s 33us/step - loss: 0.3765 - val_loss: 0.2757\n",
      "Epoch 3/50\n",
      "54000/54000 [==============================] - 2s 34us/step - loss: 0.3068 - val_loss: 0.2353\n",
      "Epoch 4/50\n",
      "54000/54000 [==============================] - 2s 33us/step - loss: 0.2681 - val_loss: 0.2105\n",
      "Epoch 5/50\n",
      "54000/54000 [==============================] - 2s 34us/step - loss: 0.2416 - val_loss: 0.1924\n",
      "Epoch 6/50\n",
      "54000/54000 [==============================] - 2s 33us/step - loss: 0.2215 - val_loss: 0.1796\n",
      "Epoch 7/50\n",
      "54000/54000 [==============================] - 2s 33us/step - loss: 0.2052 - val_loss: 0.1690\n",
      "Epoch 8/50\n",
      "54000/54000 [==============================] - 2s 34us/step - loss: 0.1917 - val_loss: 0.1602\n",
      "Epoch 9/50\n",
      "54000/54000 [==============================] - 2s 34us/step - loss: 0.1798 - val_loss: 0.1533\n",
      "Epoch 10/50\n",
      "54000/54000 [==============================] - 2s 34us/step - loss: 0.1696 - val_loss: 0.1476\n",
      "Epoch 11/50\n",
      "54000/54000 [==============================] - 2s 34us/step - loss: 0.1606 - val_loss: 0.1431\n",
      "Epoch 12/50\n",
      "54000/54000 [==============================] - 2s 33us/step - loss: 0.1526 - val_loss: 0.1376\n",
      "Epoch 13/50\n",
      "54000/54000 [==============================] - 2s 33us/step - loss: 0.1453 - val_loss: 0.1340\n",
      "Epoch 14/50\n",
      "54000/54000 [==============================] - 2s 33us/step - loss: 0.1386 - val_loss: 0.1307\n",
      "Epoch 15/50\n",
      "54000/54000 [==============================] - 2s 34us/step - loss: 0.1325 - val_loss: 0.1272\n",
      "Epoch 16/50\n",
      "54000/54000 [==============================] - 2s 34us/step - loss: 0.1268 - val_loss: 0.1254\n",
      "Epoch 17/50\n",
      "54000/54000 [==============================] - 2s 34us/step - loss: 0.1218 - val_loss: 0.1229\n",
      "Epoch 18/50\n",
      "54000/54000 [==============================] - 2s 34us/step - loss: 0.1170 - val_loss: 0.1207\n",
      "Epoch 19/50\n",
      "54000/54000 [==============================] - 2s 34us/step - loss: 0.1125 - val_loss: 0.1184\n",
      "Epoch 20/50\n",
      "54000/54000 [==============================] - 2s 34us/step - loss: 0.1083 - val_loss: 0.1180\n",
      "Epoch 21/50\n",
      "54000/54000 [==============================] - 2s 33us/step - loss: 0.1044 - val_loss: 0.1156\n",
      "Epoch 22/50\n",
      "54000/54000 [==============================] - 2s 33us/step - loss: 0.1008 - val_loss: 0.1147\n",
      "Epoch 23/50\n",
      "54000/54000 [==============================] - 2s 33us/step - loss: 0.0973 - val_loss: 0.1135\n",
      "Epoch 24/50\n",
      "54000/54000 [==============================] - 2s 33us/step - loss: 0.0940 - val_loss: 0.1118\n",
      "Epoch 25/50\n",
      "54000/54000 [==============================] - 2s 33us/step - loss: 0.0909 - val_loss: 0.1108\n",
      "Epoch 26/50\n",
      "54000/54000 [==============================] - 2s 33us/step - loss: 0.0881 - val_loss: 0.1094\n",
      "Epoch 27/50\n",
      "54000/54000 [==============================] - 2s 33us/step - loss: 0.0851 - val_loss: 0.1103\n",
      "Epoch 28/50\n",
      "54000/54000 [==============================] - 2s 33us/step - loss: 0.0824 - val_loss: 0.1098\n",
      "Epoch 29/50\n",
      "54000/54000 [==============================] - 2s 33us/step - loss: 0.0798 - val_loss: 0.1086\n",
      "Epoch 30/50\n",
      "54000/54000 [==============================] - 2s 33us/step - loss: 0.0775 - val_loss: 0.1069\n",
      "Epoch 31/50\n",
      "54000/54000 [==============================] - 2s 33us/step - loss: 0.0751 - val_loss: 0.1071\n",
      "Epoch 32/50\n",
      "54000/54000 [==============================] - 2s 33us/step - loss: 0.0728 - val_loss: 0.1076\n",
      "Epoch 33/50\n",
      "54000/54000 [==============================] - 2s 34us/step - loss: 0.0707 - val_loss: 0.1061\n",
      "Epoch 34/50\n",
      "54000/54000 [==============================] - 2s 33us/step - loss: 0.0685 - val_loss: 0.1064\n",
      "Epoch 35/50\n",
      "54000/54000 [==============================] - 2s 33us/step - loss: 0.0666 - val_loss: 0.1049\n",
      "Epoch 36/50\n",
      "54000/54000 [==============================] - 2s 33us/step - loss: 0.0647 - val_loss: 0.1049\n",
      "Epoch 37/50\n",
      "54000/54000 [==============================] - 2s 33us/step - loss: 0.0627 - val_loss: 0.1056\n",
      "Epoch 38/50\n",
      "54000/54000 [==============================] - 2s 33us/step - loss: 0.0609 - val_loss: 0.1058\n",
      "Epoch 39/50\n",
      "54000/54000 [==============================] - 2s 32us/step - loss: 0.0593 - val_loss: 0.1056\n",
      "Epoch 40/50\n",
      "54000/54000 [==============================] - 2s 33us/step - loss: 0.0575 - val_loss: 0.1049\n",
      "Epoch 41/50\n",
      "54000/54000 [==============================] - 2s 33us/step - loss: 0.0560 - val_loss: 0.1043\n",
      "Epoch 42/50\n",
      "54000/54000 [==============================] - 2s 33us/step - loss: 0.0545 - val_loss: 0.1046\n",
      "Epoch 43/50\n",
      "54000/54000 [==============================] - 2s 33us/step - loss: 0.0528 - val_loss: 0.1050\n",
      "Epoch 44/50\n",
      "54000/54000 [==============================] - 2s 33us/step - loss: 0.0516 - val_loss: 0.1047\n",
      "Epoch 45/50\n",
      "54000/54000 [==============================] - 2s 33us/step - loss: 0.0501 - val_loss: 0.1046\n",
      "Epoch 46/50\n",
      "54000/54000 [==============================] - 2s 33us/step - loss: 0.0488 - val_loss: 0.1045\n",
      "Epoch 47/50\n",
      "54000/54000 [==============================] - 2s 33us/step - loss: 0.0474 - val_loss: 0.1052\n",
      "Epoch 48/50\n",
      "54000/54000 [==============================] - 2s 33us/step - loss: 0.0461 - val_loss: 0.1043\n",
      "Epoch 49/50\n",
      "54000/54000 [==============================] - 2s 32us/step - loss: 0.0450 - val_loss: 0.1048\n",
      "Epoch 50/50\n",
      "54000/54000 [==============================] - 2s 33us/step - loss: 0.0437 - val_loss: 0.1055\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_centered, y_train_onehot, \n",
    "                    batch_size=64, epochs=50, verbose=1, \n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the value of the cost function is extremely useful during training. This is because we can quickly spot whether the cost is decreasing during training and stop the algorithm earlier, if otherwise, to tune the hyperparameters values. \n",
    "\n",
    "To predict the class labels, we can then use the *predict_classes* method to return the class labels directly as integers: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 3 predictions:  [5 0 4]\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = model.predict_classes(X_train_centered, \n",
    "                                     verbose=0)\n",
    "print('First 3 predictions: ', y_train_pred[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's print the model accuracy on training and test sets: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 99.03%\n",
      "Test accuracy: 96.55%\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = model.predict_classes(X_train_centered, \n",
    "                                     verbose=0)\n",
    "correct_preds = np.sum(y_train==y_train_pred, axis=0)\n",
    "train_acc = correct_preds / y_train.shape[0]\n",
    "print('Training accuracy: %.2f%%' % (train_acc * 100))\n",
    "\n",
    "y_test_pred = model.predict_classes(X_test_centered, \n",
    "                                    verbose=0)\n",
    "correct_preds = np.sum(y_test==y_test_pred, axis=0)\n",
    "test_acc = correct_preds / y_test.shape[0]\n",
    "print('Test accuracy: %.2f%%' % (test_acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is just a very simple neural network without optimized tuning parameters. If you are interested in playing more with Keras, feel free to further tweak the learning rate, momentum, weight decay, and number of hidden units. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing activation functions for multilayer networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we have only discussed the sigmoid activation function in the context of multilayer feedforward neural network so far; we used it in the hidden layer as well as the output layer in the multilayer implementation in the previous chapter. \n",
    "\n",
    "Although we referred to this activation function as a sigmoid function - as it is commonly called in literature - the more precise definition would be a *logistic function* or *negative log-likelihood function*. In the following subsections, you will learn more about alternative sigmoidal functions that are useful for implementing multilayer neural networks. \n",
    "\n",
    "Technically, we can use any function as an activation function in multilayer neural network as long as it is differenciable. We can even use linear activation functions, such as Adaline. However, in practice, it would not be very useful to use linear activation functions for both hidden and output layer since we want to introduce nonlinearity in a typical artificial neural network to be able to tackle complex problems. The sum of linear functions yields a linear function after all. \n",
    "\n",
    "The logistic activation function that we used in previous chapter, probably mimics the concept of a neuron in a brain most closely - we can think of it as the probability of whether a neuron fires or not. \n",
    "\n",
    "However, logistic activation functions can be problematic if we have highly negative input since the ouput of the sigmoid would be close to zero in this case. If the sigmoid function returns output that are close to zero, the neural network would learn very slowly and it becomes more likely that it gets trapped in the local minima during training. This is why people often prefer a hyperbolic tangent as an activation function in hidden layers. \n",
    "\n",
    "Before we discuss what a hyperbolic tangent looks like, let's briefly recapitulate some of the basics of the logistic function and look at a generalization that makes it more useful for multilabel classification problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic function recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned in the introduct of this section, the logistic function, often just called the sigmoid function, is in fact a special case of a sigmoid function. Recall from the section on logistic regression that we can use a logistic function to model the probability that sample $x$ belongs to the positive class (class 1) in a binary classification task. The given net input $z$ is shown in the following equation:\n",
    "\n",
    "$$z = w_0x_0 + w_1x_1 + \\ldots + w_mx_m = \\sum_{i=0}^m w_ix_i = w^Tx$$\n",
    "\n",
    "The logistic function will compute the following:\n",
    "\n",
    "$\\phi_{logistic}(z) = \\frac{1}{1 + e^{-z}}$\n",
    "\n",
    "Note that $w_0$ is the bias unit ($y$-axis intercept, which means $x_0 = 1$). To provide a more concrete example, let's assume a model for a two-dimensional data point $x$ and a model with the following weight coefficients assigned to thw $w$ vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(y=1|x) = 0.888\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([1, 1.4, 2.5]) ## first value must be 1\n",
    "w = np.array([0.4, 0.3, 0.5])\n",
    "\n",
    "def net_input(X, w):\n",
    "    return np.dot(X, w)\n",
    "\n",
    "def logistic(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def logistic_activation(X, w):\n",
    "    z = net_input(X, w)\n",
    "    return logistic(z)\n",
    "\n",
    "print('P(y=1|x) = %.3f' % logistic_activation(X, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we calculate the net input and use it to activate a logistic neuron with those particular feature values and weight coefficients, we get a value of 0.888, which we can interpret as 88.8 percent probability that this particular sample $x$ belongs to the positive class. \n",
    "\n",
    "In previous chapter, we used the one-hot-encoding technique to compute the values in the output layer consisting of multiple logistic activation units. However, as we will demonstrate with the following code example, an output layer consisting of multiple logistic activation units does not produce meaningful, interpretable probabilities values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net Input: \n",
      " [1.78 0.76 1.65]\n",
      "Output Units: \n",
      " [0.85569687 0.68135373 0.83889105]\n"
     ]
    }
   ],
   "source": [
    "# W: array with shape = (n_output_units, n_hidden_units+1)\n",
    "#    note that the first column are the bias units\n",
    "W = np.array([[1.1, 1.2, 0.8, 0.4], \n",
    "              [0.2, 0.4, 1.0, 0.2], \n",
    "              [0.6, 1.5, 1.2, 0.7]])\n",
    "# A: data array with shape = (n_hidden_units+1, n_samples)\n",
    "#    note that the first column of this array must be 1\n",
    "A = np.array([[1, 0.1, 0.4, 0.6]])\n",
    "\n",
    "Z = np.dot(W, A[0])\n",
    "y_probas = logistic(Z)\n",
    "print('Net Input: \\n', Z)\n",
    "print('Output Units: \\n', y_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the output, the resulting values cannot be interpreted as probabilities for a three-class problem. The reason of this is that they do not sum up to 1. However, this is in fact not a big concern if we only use our model to predict the class labels, not the class membership probabilities. One way to predict the class label from the output units obtained earlier is to use the maximum value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class label: 0\n"
     ]
    }
   ],
   "source": [
    "y_class = np.argmax(Z, axis=0)\n",
    "print('Predicted class label: %d' % y_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In certain contexts, it can be useful to compute meaningful class probabilities for multiclass predictions. In the next section, we will take a look at a generalization of the logistic function, the *softmax* function, which can help us with this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating class probabilities in multiclass classification via the softmax function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, we saw how could obtain a class label using the *argmax* function. The *softmax* function is in fact a soft form of the *argmax* function; instead of given a single class index, it provides the probability of each class. Therefore, it allows us to compute meaningful class probabilities in multiclass settings (multinomial logistic regression). \n",
    "\n",
    "In *softmax*, the probability of a particular sample with net input $z$ belonging to the $i$th class can be computed with a normalization term in the denominator, that is, the sum of all $M$ linear functions:\n",
    "\n",
    "$$p(y=i|z) = \\phi(z) = \\frac{e^{z_j}}{\\sum_{i=1}^M e^{z_j}}$$\n",
    "\n",
    "To see *softmax* in action, let's code it up in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities:\n",
      " [0.44668973 0.16107406 0.39223621]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(z):\n",
    "    return np.exp(z) / np.sum(np.exp(z))\n",
    "\n",
    "y_probas = softmax(Z)\n",
    "print('Probabilities:\\n', y_probas)\n",
    "np.sum(y_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the predicted class probabilities now sum up to 1, as we would expect. It is also notable that the predicted class label is the same as when we applied the *argmax* function to the logistic output. Intuitively, it may help to think of the *softmax* function as a *normalized* ouput that is useful to obtain meaningful class-membership predictions in multiclass settings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadening the output spectrum using hyperbolic tangent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another sigmoid function that is often used in the hidden layers of artificial networks in the **hyperbolic tangent** (commonly known as **tanh**), which can be interpreted as a rescaled version of the logistic function: \n",
    "\n",
    "$$\\phi_{logistic}(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "$$\\phi_{tanh}(z) = 2 \\times \\phi_{logistic}(2z)-1 = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$$\n",
    "\n",
    "The advantage of the hyperbolic tangent over the logistic function is that it has a broader output spectrum and ranges in the open interval (-1, 1), which can improve the convergence of the back propagation algorithm.\n",
    "\n",
    "In contrast, the logistic function returns an output signal that ranges in the open interval (0, 1). For an intuitive comparison of the logistic function and the hyperbolic tangent, let's plot the two sigmoid functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQCmllPJK/w/slX6Xp1d9uQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def tanh(z):\n",
    "    e_p = np.exp(z)\n",
    "    e_m = np.exp(-z)\n",
    "    return (e_p - e_m) / (e_p + e_m)\n",
    "\n",
    "z = np.arange(-5, 5, 0.005)\n",
    "log_act = logistic(z)\n",
    "tanh_act = tanh(z)\n",
    "plt.ylim([-1.5, 1.5])\n",
    "plt.xlabel('net input $z$')\n",
    "plt.ylabel('activation $\\phi(z)$')\n",
    "plt.axhline(1, color='black', linestyle=':')\n",
    "plt.axhline(0.5, color='black', linestyle=':')\n",
    "plt.axhline(0, color='black', linestyle=':')\n",
    "plt.axhline(-0.5, color='black', linestyle=':')\n",
    "plt.axhline(-1, color='black', linestyle=':')\n",
    "plt.plot(z, tanh_act, linewidth=3, linestyle='--', \n",
    "         label='tanh')\n",
    "plt.plot(z, log_act, linewidth=3, label='logistic')\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the shapes of the two sigmoidal curves look very similar; however, the *tanh* function has $2\\times$ larger output space than the *logistic* function. \n",
    "\n",
    "Note that we implemented the *logistic* and *tanh* functions verbosely for the purpose of illustration. In practice, we can use NumPy's *tanh* function to achive the same results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.9999092  -0.99990829 -0.99990737 ...  0.99990644  0.99990737\n",
      "  0.99990829]\n"
     ]
    }
   ],
   "source": [
    "tanh_act = np.tanh(z)\n",
    "print(tanh_act)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, the logistic function is available in SciPy's special module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00669285 0.00672617 0.00675966 ... 0.99320669 0.99324034 0.99327383]\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import expit\n",
    "log_act = expit(z)\n",
    "print(log_act)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rectified linear unit activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rectified Linear Unit (ReLU)** is another activation function that is often used in deep neural networks. Before we understand ReLU, we should step back and understand the vanishing gradient problem of tanh and logistic activations. \n",
    "\n",
    "To understand this problem, let's assume that we initially have the net input $z_1 = 20$, which changes to $z_2 = 25$. Computing the tanh activation, we get $\\phi(z_1) \\approx 1.0$ and $\\phi(z_2) \\approx 1.0$, which shows no change in the output. \n",
    "\n",
    "This means the derivative of activations with respect to net input diminishes as $z$ becomes large. As a result, learning weights during the training phase become very slow because the gradient terms may be very close to zero. ReLU activation addresses this issue. Mathematically, ReLU is defined as follows:\n",
    "\n",
    "$$\\phi(z) = max(0, z)$$\n",
    "\n",
    "ReLU is still a nonlinear function that is good for learning complex functions with neural networks. Besides this, the derivative of ReLU, with respect to its input, is always 1 for positive input values. Therefore, it solves the problem of vanishing gradients, making it suitable for deep neural networks. We will use the ReLU activation function in the next chapter as an activation function for multilayer convolutional neural networks. \n",
    "\n",
    "Now that we know more about the different activation functions that are commonly used in artificial neural networks, let's conclude this section with an overview of the different activation functions that we encountered in this book: \n",
    "\n",
    "<img src='images/13_04.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter, you learned how to use TensorFlow, an open source library for numerical computations with a special focus on deep learning. While TensorFlow is more convenient to use compared to NumPy, due to its additional complexity to support GPUs, it allows us to define and train large, multilayer neural networks very efficiently. \n",
    "\n",
    "Also, you learned about the TensorFlow API to build complex machine learning and neural network models and run them efficiently. First, we explored programming in the low-level TensorFlow API. Implementing model at this level may be tedious when we have to program at the level of matrix-vector multiplications and define every detail of each operation. However, the advantage is that this allows us as developers to combine such basic operations and build more complex models. Furthermore, we discussed how TensorFlow allows us to utilize the GPUs for training and testing big neural networks to speed up the computations. Without the use of GPUs, training some networks would typically need months of computation. \n",
    "\n",
    "We then explored two high-level APIs that make building neural network models a lot easier compared to the low-level API. Specifically, we used TensorFlow Layers and Keras to build the multilayer neural network and learned how to build models using those APIs. \n",
    "\n",
    "Finally, you learned about different activation functions and understood their behaviors and applications. Specifically, in this chapter, we saw tanh, softmax, and ReLU. In previous chapter, we started with implementing a simple **Multilayer Perceptron (MLP)** to classify a handwritten image in the MNIST dataset. While the low-level implementation from scratch was helpful to illustrate the core concepts of a multilayer neural network, such as the forward pass and backpropagation, training neural networks using NumPy is very inefficient and impractical for large networks. \n",
    "\n",
    "In the next chapter, we wil continue our journey and dive deeper into TensorFlow, and we will find ourselves working with graph and session objects. Along the way, we will learn many new concepts, such as placeholders, variables, and saving and restoring models in TensorFlow. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
