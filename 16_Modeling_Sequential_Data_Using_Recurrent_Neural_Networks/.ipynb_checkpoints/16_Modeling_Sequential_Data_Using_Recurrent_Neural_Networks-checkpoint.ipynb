{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Sequential Data Using Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous chapter, we focused on **Convolutional Neural Networks (CNNs)** for image classification. In this chapter, we will explore **Recurrent Neural Networks (RNNs)** and see their application in modeling sequencial data and a specific subset of sequential data - time-series data. As an overview, in this chapter, we will cover the following topics:\n",
    "\n",
    "* Introducing sequential data\n",
    "* RNNs for modeling sequences\n",
    "* **Long Short-Term Memory (LSTM)**\n",
    "* **Truncated Backpropagation Through Time (T-BPTT)**\n",
    "* Implementing a multilayer RNN for sequence modeling in TensorFlow\n",
    "* Project one - RNN sentiment analysis of the IMDb movie review dataset\n",
    "* Project two - RNN character-level language modeling with LSTM cells, using text data from Shakespeare's Hamlet\n",
    "* Using gradient clipping to avoid exploring gradients. \n",
    "\n",
    "Since this chapter is the last in our *Python Machine Learning* journey, we will conclude with a summary of what we have learned about RNNs, and an overview of all the machine learning and deep learning topics that led us to RNNs across the journey of the book. We will then sign off by sharing with you links to some of our favorite people and initiatives in this wonderful field so that you can continue your journey into machine learning and deep learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing sequential data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin our discussion of RNNs by looking at the nature of sequential data, more commonly known as **sequences**. We will take a look at the unique properties of sequences that make them different from other kind of data. We will then see how we can represent sequential data, and explore the various categories of model ffor sequential data, which are based on the input and output of a model. This will help us explore relationship between RNNs and sequences a little bit later on in the chapter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling sequential data - order matters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What makes sequences unique, from other data types, is that elements in a sequence appear in a certain order, and are not independent of each other. \n",
    "\n",
    "If you recall from Chapter 6, we discussed that typical machine learning algorithms for supervised learning assume that the input data is **Independent and Identically Distributed (IID)**. For example, if we have $n$ data samples, $x^{(1)}, x^{(2)}, \\ldots, x^{(n)}$, the order in which we use the data for training our machine learning algorithm does not matter. \n",
    "\n",
    "However, this assumption is not valid anymore when we deal with sequences - by definition, order matters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representing sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have estabilished that sequences are a nonindependent order in our input data; we next need to find ways to leverage this valuable information in our machine learning model. \n",
    "\n",
    "Throughout this chapter, we will represent sequences as $(x^{(1)}, x^{(2)}, \\ldots, x^{(T)})$. The superscript indices indicate the order of the instances, and the length of the sequence is $T$. For a sensible example of sequences, consider time-series data, where each sample point $x^{(t)}$ belongs to a particular time $t$. \n",
    "\n",
    "The following figure shows an example of time-series data where both $x$'s and $y$'s naturally follow the order according to their time axis; therefore, both $x$'s and $y$'s are sequences: \n",
    "\n",
    "<img src='images/16_01.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard neural network models that we covered so far, such as MLP and CNNs, are not capable of handling *the order* of input samples. Intuitively, one can say that such models do not have a *memory* of the past seen samples. For instance, the samples are passed through the feedforward and backpropagation steps, and the weights are updated independent of the order in which the sample is processed. \n",
    "\n",
    "RNNs, by contrast, are designed for modeling sequences and are capable of remembering past information and processing new events accordingly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The different categories of sequence modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence modeling has many fascinating applications, such as language translation (perhaps from English to German), image captioning, and text generation. \n",
    "\n",
    "However, we need to understand the different types of sequence modeling tasks to develop an appropriate model. The following figure shows several different relationship categories of input and output data:\n",
    "\n",
    "<img src='images/16_02.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's consider the input and output data here. If neither the input or output data represents sequences, then we are dealing with standard data, and we can use any of the previous methods to model such data. But if either the input or output is a sequence, the data will form one of the following three different categories: \n",
    "\n",
    "* **Many-to-one**: The input data is a sequence, but the output is a fixed-size vector, not a sequence. For example, in sentiment analysis, the input is a text-based and the output is a class label.\n",
    "* **One-to-many**: The input data is in standard format, not a sequence, but the output is a sequence. An example of this category is image captioning - the input is an image; the output is an English phrase. \n",
    "* **Many-to-many**: Both the input and output arrays are sequences. This category can be further divided based on whether the input and output are synchronized or not. An example of a **synchronized** many-to-many modeling task is video classification, where each frame in a video is labeled. An example of a **delayed** many-to-many would be translating a language into another. For instance, an entire English sentence must be read and processed by a machine before producing its translation into German. \n",
    "\n",
    "Now, since we know the categories of sequence modeling, we can move forward to discuss the structure of an RNN. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNs for modeling sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, now that we understand sequences, we can look at the foundations of RNNs. We will start by introducing the typical structure of an RNN, and we will see how the data flows through it with one or more hidden layers. We will then examine how the neuron activations are computed in a typical RNN. This will create a context for us to discuss the common challenges in training RNNs, and explore the modern solution to these challenges - LSTM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the structure and flow of an RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by introducing the architecture of an RNN. The following figure shows a standard feedforward neural network and an RNN, in a side by side for comparison: \n",
    "\n",
    "<img src='images/16_03.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of these networks have only one hidden layer. In this representation, the units are not displayed, but we assume that the input layer ($x$), hidden layer ($h$), and output layer ($y$) are vectors which contain many units. \n",
    "\n",
    "This generic RNN architecture could correspond to the two sequence modeling categories where the input is a sequence. Thus, it could be either many-to-many if we consider $y^{(t)}$ as teh final output, or it could be many-to-one if, for example, we only use the last element of $y^{(t)}$ as the final output. \n",
    "\n",
    "Later, we will see how the output sequence $y^{(t)}$ can be converted into standard, nonsequential output. \n",
    "\n",
    "In a standard feedforward network, information flows from the input to the hidden layer, and then from the hidden layer to the output layer. On the other hand, in a recurrent network, the hidden layer gets its input from both the input layer and the hidden layer from the previous time step. \n",
    "\n",
    "The flow of information in adjacent time steps in the hidden layer allows the network to have a memory of past events. This flow of information is usually displayed as a loop, also known as a **recurrent edge** in graph notation, which is how this general architecture got its name. \n",
    "\n",
    "In the following figure, the single hidden layer network and the multilayer network illustrate two contrasting architectures: \n",
    "\n",
    "<img src='images/16_04.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to examine the architecture of RNNs and the flow of information, a compact representation with a recurrent edge can be unfolded, which you can see in the preceding figure. \n",
    "\n",
    "As we know, each hidden unit in a standard neural network receives only one input - the net preactivation associated with the input layer. Now, in contrast, each hidden layer unit in an RNN receives two *distinct* sets of input - the preactivation from the input layer and the activation of the same hidden layer from the previous time step $t-1$. \n",
    "\n",
    "At the first time step $t=0$, the hidden units are initialized to zeros or small random values. Then, at a time step where $t>0$, the hidden units get their input from the data point at the current time $x^{(t)}$ and the previous values of hidden units at $t-1$, indicated as $h^{(t-1)}$. \n",
    "\n",
    "Similarly, in the case of a multilayer RNN, we can summarize the information flow as follows: \n",
    "\n",
    "* *layer=1*: Here, the hidden layer is represented as $h_1^{(t)}$ and gets its input from the data point $x^{(t)}$ and the hidden values in the same layer, but the previous time step $h_1^{t-1}$. \n",
    "* *layer=2*: The second hidden layer, $h_2^{(t)}$ receives its inputs from the hidden units from the layer below at the current time step ($h_1^{(t)}$) and its own hidden values from the previous time step $h_2^{(t-1)}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing activations in an RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand the structure and general flow of information in an RNN, let's get more specific and compute the actual activations of the hidden layers as well as the output layer. For simplicity, we will consider just a single hidden layer; however, the same concept applies to multilayer RNNs. \n",
    "\n",
    "Each directed edge (the connections between boxes) in the representation of an RNN that we just looked at is associated with a weight matrix. Those weights do not depend on time $t$; therefore, they are shared across the time axis. The different weight matrices in a single layer RNN are as follows: \n",
    "\n",
    "* $W_{xh}$: The weight matrix between the input $x^{(t)}$ and the hidden layer $h$.\n",
    "* $W_{hh}$: The weight matrix associated with the recurrent edge. \n",
    "* $W_{hy}$: The weight matrix between the hidden layer and output layer. \n",
    "\n",
    "You can see these weights matrices in the following figure: \n",
    "\n",
    "<img src='images/16_05.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In certain implementations, you may observe that weight matrices $W_{xh}$ and $W_{hh}$ are concatenated to a combined matrix $W_h = [W_{xh}; W_{hh}]$. Later on, we will make use of this notation as well. \n",
    "\n",
    "Computing the activations is very similar to standard multilayer perceptrons and other types of feedforward neural networks. For the hidden layer, the net input $Z_h$ (preactivation) is computed through a linear combination. That is, we compute the sum of the multiplications of the weight matrices with the corresponding vectors and add the bias unit - $Z_h = W_{xh}x^{(t)} + W_{hh}h^{(t-1)} + b_h$. Then, the activations of the hidden units at the time step $t$ are calculated as follows:\n",
    "\n",
    "$$h^{(t)} = \\phi(Z_h^{(t)}) = \\phi_h (W_{xh}x^{(t)} + W){hh}h^{(t-1)} + b_h)$$\n",
    "\n",
    "Here, $b_h$ is the bias vector for the hidden units and $\\phi_h$ is the activation function of the hidden layer. \n",
    "\n",
    "In case you want to use the concatenated weight matrix $W_h = [W_{xh}; W_{hh}]$, the formula for computing hidden units will change as follows:\n",
    "\n",
    "$h^{(t)} = \\phi_h\\left([W_{xh}; W_{hh}][x^{(t)}; h^{(t-1)}]^{-1} + b_h\\right)$\n",
    "\n",
    "Once the activations of hidden units at the current time step are computed, then the activations of output units will be computed as follows: \n",
    "\n",
    "$$y^{(t)} = \\phi_y(w_{hy}h^{(t)} + b_y)$$\n",
    "\n",
    "To help clarify this further, the following figure shows the process of computing these activations with both formulations: \n",
    "\n",
    "<img src='images/16_06.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training RNNs using BPTT** \n",
    "\n",
    "The learning algorithm for RNNs was introduced in 1990s *Backpropagation Through Time: What it Does and How to Do It*.\n",
    "\n",
    "The derivation of the gradients might be a bit complicated, but the basic idea is that the overall loss $L$ is the sum of all the loss functions at time $t=1$ to $t=T$: \n",
    "\n",
    "$$L = \\sum_{t=1}^T L^{(t)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The challenges of learning long-range interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation through time, or BPTT, which we briefly mentioned in the previous information box, introduces some challenges. Because of the multiplicative factor $\\frac{\\partial h^{(t)}}{\\partial h^{(k)}}$ in the computing gradients of a loss function, the so-called **vanishing** or **exploding** gradient problem arises. This problem is explained through the examples in the following figure, which shows an RNN with only one hidden unit for simplicity: \n",
    "\n",
    "<img src='images/16_07.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, $\\frac{\\partial h^{(t)}}{\\partial h^{(k)}}$ has $t-k$ multiplications; therefore, multiplying the $w$ weight $t-k$ times results in a factor $w^{t-k}$. As a result, if $|w| < 1$, this factor becomes very small when $t-k$ is large. On the other hand, if the weight of the recurrent edge is $|w| > 1$, then $w^{t-k}$ becomes very large when $t-k$ is large. Note that large $t-k$ refers to long-range dependencies. \n",
    "\n",
    "Intuitively, we can see that a naive solution to avoid vanishing or exploring gradient can be accomplished by ensuring $|w| = 1$. \n",
    "\n",
    "In practice, there are two solutions to this problem: \n",
    "* Truncated backpropagation through time (TBPTT). \n",
    "* Long short-term memory (LSTM). \n",
    "\n",
    "TBPTT clips the gradients above a given threshold. While TBPTT can solve the exploding gradient problem, the truncation limits the number of steps that the gradient can effectively flow back and properly update the weights. \n",
    "\n",
    "On the other hand, LSTM, designed in 1997 by Hochreiter and Schmidhuber, has been more successful in modeling long-range sequences by overcoming the vanishing gradient problem. Let's discuss LSTM in more detail. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM were first introduced to overcome the vanishing gradient problem. The building block of an LSTM is a **memory cell**, which essentially represents the hidden layer. \n",
    "\n",
    "In each memory cell, there is a recurrent edge that has the desirable weight $w=1$, as we discussed previously, to overcome the vanishing and exploding gradient problems. The values associated with this recurrent edge is called **cell state**. The unfolded structure of a modern LSTM cell is shown in the following figure: \n",
    "\n",
    "<img src='images/16_08.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the cell state from the previous time step, $C^{(t-1)}$, is modified to get the cell state at the current time step, $C^{(t)}$, without being multiplied directly with any weight factor. \n",
    "\n",
    "The flow of information in this memory cell is controlled by some units of computation that we will describe here. In the previous figure, $\\odot$ refers to the **element-wise product** (element-wise multiplication) and $\\oplus$ means **element-wise summation** (element-wise addition). Furthermore, $x^{(t)}$ refers to the input data at time $t$, and $h^{(t-1)}$ indicates the hidden units at time $t-1$. \n",
    "\n",
    "Four boxes are indicated with an activation function, either the sigmoid function ($\\sigma$) or hyperbolic tangent (tanh), and a set of weights; these boxes apply linear combination by performing matrix-vector multiplications on their input. These units of computation with sigmoid activation functions, whose output units are passed through $\\odot$, are called **gates**. \n",
    "\n",
    "In an LSTM cell, there are three different types of gates, known as the forget gate, the input gate, and the output gate: \n",
    "\n",
    "* The **forget gate ($f_t$)** allows the memory cell to reset the cell state without growing indefinitely. In fact, the forget gate decides which information is allowed to go through and which information to suppress. Now, $f_t$ is computed as follows: $f_t = \\sigma(W_{xf}x^{(t)} + W_{hf}h^{(t-1)} + b_f)$. Note that the forget gate was not part of the original LSTM cell; it was added a few years later to improve the original model. \n",
    "* The **input gate ($i_t$)** and input nodes ($g_t$) are responsible for updating the cell state. They are computed as follows: $i_t = \\sigma(W_{xi}x^{(t)} + W_{hi}h^{(t-1)} + b_i)$ and $g_i = tanh(W_{xg}x^{(t)} + W_{hg}h^{(t-1)} + b_g)$. The cell state at time $t$ is computed as follows: $C^{(t)} = (C^{t-1} \\odot f_t) \\oplus (i_t \\odot g_t)$. \n",
    "* The **output gate ($o_t$)** decides how to update the values of hidden units: $o_t = \\sigma(W_{xo}x^{(t)} + W_{ho}h^{(t-1)} + b_o)$. Given this, the hidden units at the current time step are computed as follows: $h^{(t)} = o_t \\odot tanh(C^{(t)})$. \n",
    "\n",
    "The structure of an LSTM cell and its underlying computations might seem too complex. However, the good news is that TensorFlow has already implemented everything in wrapper functions that allows us to define our LSTM cell easily. We will see the real application of LSTMs in action when we use TensorFlow later in this chapter. \n",
    "\n",
    "We have introduced LSTMs in this section, which provide a basic approach for modeling long-range dependencies in sequences. Yet, it is important to note that there are many variations of LSTMs described in literature. \n",
    "\n",
    "Also, worth noting is a more recent approach, called **Gated Recurrent Unit (GRU)**, which was proposed in 2014. GRUs have a simpler architecture than LSTMs; therefore, they are computationally more efficient while their performance in some tasks, such as polyphonic music modeling, is comparable to LSTMs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a multilayer RNN for sequence modeling in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we introduced the underlying theory behind RNNs, we are ready to move on to the more practical part to implement RNNs in TensorFlow. During the rest of this chapter, we will apply RNNs to two common problems tasks:\n",
    "\n",
    "1. Sentiment analysis\n",
    "2. Language modeling\n",
    "\n",
    "These two projects, which we will build together in the following pages, are both fascinating but also quite involved. Thus, instead of providing all the code all at once, we will break the implementation up into several steps and discuss the code in detail. \n",
    "\n",
    "Note, before we start coding in this chapter, that since we are using a very modern build of TensorFlow, we will be using code from the *contrib* submodule of TensorFlow's Python API, in the latest version of TensorFlow (1.3.0) form August 2017. These *contrib* functions and classes, as well as their documentation references used in this chapter, may change in the future versions of TensorFlow, or they may be integrated into the *tf.nn* submodule. We therefore advise you to keep and eye on the TensorFlow API documentation to be updated with the latest version details, in particular, if you have any problems using the *tf.contrib* code described in this chapter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project one - performing sentiment analysis of IMDb movie reviews using multilayer RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may recall from Chapter 8 that sentiment analysis is concerned with analyzing the expressed opinion of a sentence or text document. In this section and the following subsections, we will implement a multilayer RNN for sentiment analysis using a many-to-one architecture. \n",
    "\n",
    "In the next section, we will implement many-to-many RNN for an application language modeling. While the chosen examples are purposefully simple to introduce the main concepts of RNNs, language modeling has a wide range of interesting applications such as building chatbot - giving computers the ability to directly talk and interact with a human. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preprocessing steps in Chapter 8, we created a clean dataset named *movie_data.csv*, which we will use again now. So, first let's import the necessary modules and read the data into a *DataFrame* pandas, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyprind\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('movie_data.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that this *df* data frame has two columns, namely *'review'* and *'sentiment'*, where *'review'* contains the text of movie reviews and *'sentiment'* contains the 0 or 1 labels. The text component of these movie reviews are sequences of words; therefore, we want to build an RNN model to process the words in each sequence, and at the end, classify the entire sequence to 0 or 1 classes. \n",
    "\n",
    "To prepare the data for input to a neural network, we need to encode it into numeric values. To do this, we first find the unique words in the entire dataset, which can be using sets in Python. However, I found that using sets for finding unique words in such a large dataset is not efficient. A more efficient way is to use *Counter* from the collections package. \n",
    "\n",
    "In the following code, we will define a *counts* objects from the *Counter* class that collects the counts of occurrence of each unique word in the text. Note that in this particular application (and in contrast to the bag-of-words models), we are only interested in the set of unique words and won't require the word counts, which are created as a side product. \n",
    "\n",
    "Then, we create a mapping in the form of a dictionary that maps each unique words, in our dataset, to a unique integer number. We call this dictionary *word_to_int*, which can be used to convert the entire text of a review into a list of numbers. The unique words are sorted based on their counts, but any arbitrary order can be used without affecting the final results. This process of converting a text into a list of integers is performed using the following code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting words occurrences\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:03:35\n",
      "Map reviews to ints\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', '.', ',', 'and', 'a']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:02\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing the data:\n",
    "# Separate words and \n",
    "# count each word's occurrence\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "counts = Counter()\n",
    "pbar = pyprind.ProgBar(len(df['review']), title='Counting words occurrences')\n",
    "for i, review in enumerate(df['review']):\n",
    "    text = ''.join([c if c not in punctuation else ' '+c+' ' for c in review]).lower()\n",
    "    df.loc[i, 'review'] = text\n",
    "    pbar.update()\n",
    "    counts.update(text.split())\n",
    "    \n",
    "# Create a mapping\n",
    "# Map each unique word to an integer\n",
    "word_counts = sorted(counts, key=counts.get, reverse=True)\n",
    "print(word_counts[:5])\n",
    "word_to_int = {word: ii for ii, word in enumerate(word_counts, 1)}\n",
    "\n",
    "mapped_reviews = []\n",
    "pbar = pyprind.ProgBar(len(df['review']), title='Map reviews to ints')\n",
    "for review in df['review']:\n",
    "    mapped_reviews.append([word_to_int[word] for word in review.split()])\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have converted sequences of words into sequences of integers. However, there is one issue that we still need to solve - the sequences currently have different lenghts. In order to generate input data that is compatible with our RNNs architecture, we will need to make sure that all the sequences have the same length. \n",
    "\n",
    "For this purpose, we define a parameter called *sequence_length* that we set to 200. Sequences that have fewer than 200 words will be left-padded with zeros. Vice versa, sequences that are longer than 200 words are cut such that only the last 200 corresponding words will be used. We can implement this preprocessing step in two steps:\n",
    "\n",
    "1. Create a matrix of zeros, where each row correspond to a sequence of size 200. \n",
    "2. Fill the index of words in each sequence from the right-hand side of the matrix. Thus, if a sequence has a length of 150, the first 50 elements of the corresponding row will stay zero. \n",
    "\n",
    "These two steps are shown in the following figure, for a small example with eight sequences of sizes 4, 12, 8, 11, 7, 3, 10, and 13:\n",
    "\n",
    "<img src='images/16_09.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that *sequence_length* is, in fact, a hyperparameter and can be used for optimal performance. Due to page limitations, we did not optimize this hyperparameter further, but we encourage you to try this with different values for *sequence_length*, such as 50, 100, 200, 250, and 300. \n",
    "\n",
    "Check out the following code for the implementation for these steps to create sequences of the same length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define same-length sequences\n",
    "# if sequence length < 200: left-pad with zeros\n",
    "# if sequence length > 200: use the last 200 elements\n",
    "\n",
    "sequence_length = 200 # (Known as T in our RNN formulas)\n",
    "sequences = np.zeros((len(mapped_reviews), sequence_length), dtype=int)\n",
    "\n",
    "for i, row in enumerate(mapped_reviews):\n",
    "    review_arr = np.array(row)\n",
    "    sequences[i, -len(row):] = review_arr[-sequence_length:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we preprocess the dataset, we can proceed with splitting the data into separate training and test sets. Since the dataset was already shuffled, we can simply take the first half of the dataset for training and the second half for testing, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sequences[:25000, :]\n",
    "y_train = df.loc[:25000, 'sentiment'].values\n",
    "X_test = sequences[25000:, :]\n",
    "y_test = df.loc[25000:, 'sentiment'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we want to separate the dataset for cross-validation, we can further split the second half of the data further to generate a smaller test set and a validation set for hyperparameter optimization. \n",
    "\n",
    "Finally, we define a helper function that breaks a given dataset (which could be a training set or test set) into chunks and returns a generator to iterate through these chunks (also known as **mini-batches**): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123) # for reproducibility\n",
    "\n",
    "# Define a function to generate mini-batches:\n",
    "def create_batch_generator(x, y=None, batch_size=64):\n",
    "    n_batches = len(x)//batch_size\n",
    "    x = x[:n_batches*batch_size]\n",
    "    if y is not None:\n",
    "        y = y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        if y is not None: \n",
    "            yield x[ii:ii+batch_size], y[ii:ii+batch_size]\n",
    "        else:\n",
    "            yield x[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using generators, as we have done in this code, is a very useful technique for handling memory limitations. This is the recommended approach for splitting the dataset into mini-batches for training a neural network, rather than creating all the data splits upfront and keeping them in memory during training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the data preparation in the previous step, we generated sequences of the same length. The elements of these sequences were integer numbers that corresponded to the *indices* of unique words. \n",
    "\n",
    "These word indices can be converted into input features in several different ways. One naive way is to apply one-hot encoding to convert indices into vectors of zeros and ones. Then, each word will be mapped to a vector whose size is the number of unique words in the entire dataset. Given that the number of unique words (the size of the vocabulary) can be in the order of 20,000, which will also be the number of our input features, a model trained on such features may suffer from the **curse of dimensionality**. Furthermore, these features are very sparse, since all are zero except one. \n",
    "\n",
    "A more elegant way is to map each word to a vector of fixed size with real-valued elements (not necessarily integers). In contrast to the one-hot encoded vectors, we can use finite-sized vectors to represent an infinite number of real numbers (in theory, we can extract infinite real numbers from a given interval, for example [-1, 1]). \n",
    "\n",
    "This is the idea behind the so-called **embedding**, which is a feature-learning technique that we can utilize here to automatically learn the salient features to represent the words in our dataset. Given the number of unique words *unique_words*, we can choose the size of the embedding vectors to be much smaller than the number of unique words (*embedding_size << unique_words*) to represent the entire vocabulary as input features. \n",
    "\n",
    "The advantages of embedding over one-hot encoding are as follows:\n",
    "\n",
    "* A reduction in the dimensionality of the feature space to decrease the effect of effect of the curse of dimensionality. \n",
    "* The extraction of salient features since the embedding layer in a neural network is trainable. \n",
    "\n",
    "The following schematic representation shows how embedding works by mapping vocabulary indices to a trainable embedding matrix: \n",
    "\n",
    "<img src='images/16_10.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow implements an efficient function, *tf.nn.embedding_lookup*, that maps each integer that corresponds to a unique word, to a row of this trainable matrix. For example, integer 1 is mapped to the first row, integer 2 is mapped to the second row, and so on. Then, given a sequence of integers, such as <0, 5, 3, 4, 19, 2, ...>, we need to look up the corresponding rows for each element in this sequence. \n",
    "\n",
    "Now, let's see how we can create an embedding layer in practice. If we have *tf_x* as the input layer where the corresponding vocabulary indices are fed with type *tf.int32*, then creating an embedding layer can be done in two steps, as follows:\n",
    "\n",
    "1. We start by creating a matrix of size $[n_words \\times embedding_size]$ as a tensor variable, which we call *embedding* and we initialize its elements randomly with floats between $[-1, 1]$. \n",
    "2. Then, we use the *tf.nn.embedding_lookup* function to look up the row in the embedding matrix associated with each element of *tf_x*.\n",
    "\n",
    "As you may have observed in these steps, to create an embedding layer, the *tf.nn.embedding_lookup* function requires two arguments, the embedding tensor and the lookup IDs. \n",
    "\n",
    "The *tf.nn.embedding_lookup* function has a few optional arguments that allow you to tweak the behavior of the embedding layer, such as applying L2 normalization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an RNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to build an RNN model. We will implement a *SentimentRNN* class that has the following methods: \n",
    "\n",
    "* A constructor to set all the model parameters and then create a computation graph and call the *self.build* method to build the multilayer RNN model. \n",
    "* A *build* method that declares three placeholders for input data, input labels, and the keep-probability for the dropout configuration of the hidden layer. After declaring these, it creates an embedding layer, and builds the multilayer RNN using the embedded representation as input. \n",
    "* A *train* method that creates a TensorFlow session for launching the computation graph, iterates through the mini-batches of data, and runs for a fixed number of epochs, to minimize the cost function defined in the graph. This method also saves the model after 10 epochs for checkpointing. \n",
    "* A *predict* method that creates a new session, restores the last checkpoint saved during the training process, and carries out the predictions for the test data. \n",
    "\n",
    "In the following code, we will see the implementations of this class and its methods broken into separate code sections. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The SentimentRNN class constructor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the constructor of our *SentimentRNN* class, which we will code as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class SentimentRNN(object):\n",
    "    \n",
    "    def __init__(self, n_words, seq_len=200, \n",
    "                 lstm_size=256, num_layers=1, batch_size=64, \n",
    "                 learning_rate=0.0001, embed_size=200):\n",
    "        self.n_words = n_words\n",
    "        self.seq_len = seq_len\n",
    "        self.lstm_size = lstm_size # number of hidden units\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            tf.set_random_seed(123)\n",
    "            self.build()\n",
    "            self.saver = tf.train.Saver()\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "            \n",
    "    def build(self):\n",
    "        ## Define the placeholders\n",
    "        tf_x = tf.placeholder(tf.int32, shape=(self.batch_size, self.seq_len), name='tf_x')\n",
    "        tf_y = tf.placeholder(tf.float32, shape=(self.batch_size), name='tf_y')\n",
    "        tf_keepprob = tf.placeholder(tf.float32, name='tf_keepprob')\n",
    "        \n",
    "        ## Create the embedding layer\n",
    "        embedding = tf.Variable(tf.random_uniform((self.n_words, self.embed_size), minval=-1, maxval=1), name='embedding')\n",
    "        embed_x = tf.nn.embedding_lookup(embedding, tf_x, name='embeded_x')\n",
    "        \n",
    "        ## Define LSMT cell and stack them together\n",
    "        cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.BasicLSTMCell(self.lstm_size), output_keep_prob=tf_keepprob) for i in range(self.num_layers)])\n",
    "        \n",
    "        ## Define the initial state\n",
    "        self.initial_state = cells.zero_state(self.batch_size, tf.float32)\n",
    "        print(' << initial state >> ', self.initial_state)\n",
    "        \n",
    "        lstm_outputs, self.final_state = tf.nn.dynamic_rnn(cells, embed_x, initial_state=self.initial_state)\n",
    "        \n",
    "        # Note: lstm_outputs shape: \n",
    "        #[batch_size, max_time, cells.output_size]\n",
    "        print('\\n << lstm_output >> ', lstm_outputs)\n",
    "        print('\\n << final state >> ', self.final_state)\n",
    "        \n",
    "        logits = tf.layers.dense(inputs=lstm_outputs[:, -1], units=1, activation=None, name='logits')\n",
    "        \n",
    "        logits = tf.squeeze(logits, name='logits_squeezed')\n",
    "        print('\\n << logits >> ', logits)\n",
    "        \n",
    "        y_proba = tf.nn.sigmoid(logits, name='probabilities')\n",
    "        predictions = {\n",
    "            'probabilities': y_proba, \n",
    "            'labels': tf.cast(tf.round(y_proba), tf.int32, name='labels')\n",
    "        }\n",
    "        print('\\n << predictions >> ', predictions)\n",
    "        \n",
    "        ## Define the cost function\n",
    "        cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf_y, logits=logits), name='cost')\n",
    "        \n",
    "        ## Define the optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        train_op = optimizer.minimize(cost, name='train_op')\n",
    "        \n",
    "    def train(self, X_train, y_train, num_epochs):\n",
    "        with tf.Session(graph=self.g) as sess:\n",
    "            sess.run(self.init_op)\n",
    "            iteration = 1\n",
    "            for epoch in range(num_epochs):\n",
    "                state = sess.run(self.initial_state)\n",
    "                \n",
    "                for batch_x, batch_y in create_batch_generator(X_train, y_train, self.batch_size):\n",
    "                    feed = {'tf_x:0': batch_x, 'tf_y:0': batch_y, \n",
    "                            'tf_keepprob:0': 0.5, self.initial_state: state}\n",
    "                    loss, _, state = sess.run(['cost:0', 'train_op', self.final_state], feed_dict=feed)\n",
    "                    \n",
    "                    if iteration % 20 == 0:\n",
    "                        print('Epoch %d/%d Iteration: %d | Train loss: %.5f' % (epoch+1, num_epochs, iteration, loss))\n",
    "                        \n",
    "                    iteration += 1\n",
    "                if (epoch+1)%10 == 0:\n",
    "                    self.saver.save(sess, 'model/sentiment-%d.ckpt' % epoch)\n",
    "                    \n",
    "    def predict(self, X_data, return_proba=False):\n",
    "        preds = []\n",
    "        with tf.Session(graph=self.g) as sess:\n",
    "            self.saver.restore(sess, tf.train.latest_checkpoint('./model/'))\n",
    "            test_state = sess.run(self.initial_state)\n",
    "            for ii, batch_x in enumerate(create_batch_generator(X_data, None, batch_size=self.batch_size), 1):\n",
    "                feed = {'tf_x:0': batch_x, 'tf_keepprob:0': 1.0, self.initial_state: test_state}\n",
    "                if return_proba:\n",
    "                    pred, test_state = sess.run(['probabilities:0', self.final_state], feed_dict=feed)\n",
    "                else:\n",
    "                    pred, test_state = sess.run(['labels:0', self.final_state], feed_dict=feed)\n",
    "                \n",
    "                preds.append(pred)\n",
    "                \n",
    "        return np.concatenate(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the *n_words* parameter must be set equal to the number of unique words (plus 1 since we use zero to fill sequences whose size is less than 200) and it is used while creating the embedding layer along with the *embed_size* hyperparameter. Meanwhile, the *seq_len* variable must be set according to the length of the sequences that were created in the preprocessing steps we went through previously. Note that *lstm_size* is another hyperparameter that we have used here, and it determines that number of hidden units in each RNN layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The build method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's discuss the *build* method for our *SentimentRNN* class. This is the longest and most critical method in our sequence, so we will be going through it in plenty of detail. First, we will look at the code in full, so we can see everything together, and then we will analyze each of its main parts:\n",
    "\n",
    "So first of all in our *build* method here, we created three placeholders, namely *tf_x*, *tf_y*, and *tf_keepprob*, which we need for feeding the input data. Then we added the embedding layer, which builds the embedded representation *embed_x*, as we discussed earlier. \n",
    "\n",
    "Next, in our *build* method, we built the RNN network with LSTM cells. We did this in three steps: \n",
    "\n",
    "1. First, we defined the multilayer RNN cells.\n",
    "2. Next, we defined the initial state of these cells. \n",
    "3. Finally, we created an RNN specified by the RNN cells and their initial states. \n",
    "\n",
    "Let's break these three steps out in detail in the following three sections, so we can examine in depth how we built the RNN network in our *build* method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - defining multilayer RNN cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To examine how we encoded our *build* method to build the RNN network, the first step was to define our multilayer RNN cells. \n",
    "\n",
    "Fortunately, TensorFlow has a very nice wrapper class to define LSTM cells - the *BasicLSTMCell* class - which can be stacked together to form a multilayer RNN using the *MultiRNNCell* wrapper class. The process of stacking RNN cells with a dropout has three nested steps; these three nested steps can be described from inside out as follows:\n",
    "\n",
    "1. First, create the RNN cells using *tf.contrib.rnn.BasicLSTMCell*. \n",
    "2. Apply the dropout to the RNN cells using *tf.contrib.rnn.DropoutWrapper*. \n",
    "3. Make a list of such cells according to the desired number of RNN layers and pass this list to *tf.contrib.rnn.MultiRNNCell*. \n",
    "\n",
    "In our *build* method code, this list is created using Python list comprehension. Note that for a single layer, this list has only once cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - defining the initial states for the RNN cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second step that our *build* method takes to build the RNN network was to define the initial states for the RNN cells. \n",
    "\n",
    "You will recall from the architecture of LSTM cells, there are three types of inputs in an LSTM cell - input data $x^{(t)}$, activations of hidden units from the previous time step $h^{(t-1)}$, and the cell state from the previous time step $C^{(t-1)}$. \n",
    "\n",
    "So, in our *build* method implementation, $x^{(t)}$ is the embedded *embed_x* data tensor. However, when we evaluate the *cells*, we also need to specify the previous state of the cells. So, when we start processing a new input sequence, we initialize the cell states to zero state; then after each time step, we need to store the updated state of the cells to use for the next time step. \n",
    "\n",
    "Once our multilayer RNN object is defined (*cells* in our implementation), we define its initial state in our *build* method using the *cells.zero_state* method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - creating the RNN using the RNN cells and their states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third step to creating the RNN in our *build* method, used the *tf.nn.dynamic_rnn* function to pull together all our components. \n",
    "\n",
    "The *tf.dynamic_rnn* function therefore pulls the embedded data, the RNN cells, and their initial states, and creates a pipeline for them according to the unrolled architecture of LSTM cells. \n",
    "\n",
    "The *tf.dynamic_rnn* function returns a tuple containing the activations of the RNN cells, *outputs*; and their final states, *state*. The output is a three-dimensional tensor with this shape *(batch_size, num_steps, lstm_size)*. We pass *outputs* to a fully connected layer to get *logits* and we store the final state to use as the initial state of the next mini-batch of data. \n",
    "\n",
    "Finally, in our *build* method, after setting up the RNN components of the network, the cost function and optimization schemes can be defined like any other neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The train method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next method in our *SentimentRNN* class is *train*. This method is quite similar to the train methods we created in Chapters 14 and 15, except that we have an additional tensor, *state*, that we feed into our network. \n",
    "\n",
    "The following code shows the implementation of the *train* method:\n",
    "\n",
    "In this implementation of our *train* method, at the beginning of each epoch, we start from the zero states of RNN cells as our current state. Running each mini-batch of data is performed by feeding the current state along with the data *batch_x* and their labels *batch_y*. Upon finishing the executing of a mini-batch, we update the state to be the final state, which is returned by the *tf.nn.dynamic_rnn* function. This updated state will be used toward executing of the next mini-batch. This process is repeated and the current state is updated throughout the epoch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The predict method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the last method in our *SentimentRNN* class is the *predict* method, which keeps updating the current state similar to the *train* method, shown in the following code: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating the SentimentRNN class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now coded and examined all four parts of our *SentimentRNN* class, which were the class constructor, the *build* mehtod, the *train* method, and *predict* method. \n",
    "\n",
    "We are now ready to create an object of the class *SentimentRNN*, with parameters as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      " << initial state >>  (LSTMStateTuple(c=<tf.Tensor 'MultiRNNCellZeroState/DropoutWrapperZeroState/BasicLSTMCellZeroState/zeros:0' shape=(100, 128) dtype=float32>, h=<tf.Tensor 'MultiRNNCellZeroState/DropoutWrapperZeroState/BasicLSTMCellZeroState/zeros_1:0' shape=(100, 128) dtype=float32>),)\n",
      "\n",
      " << lstm_output >>  Tensor(\"rnn/transpose_1:0\", shape=(100, 200, 128), dtype=float32)\n",
      "\n",
      " << final state >>  (LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_3:0' shape=(100, 128) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_4:0' shape=(100, 128) dtype=float32>),)\n",
      "\n",
      " << logits >>  Tensor(\"logits_squeezed:0\", shape=(100,), dtype=float32)\n",
      "\n",
      " << predictions >>  {'probabilities': <tf.Tensor 'probabilities:0' shape=(100,) dtype=float32>, 'labels': <tf.Tensor 'labels:0' shape=(100,) dtype=int32>}\n"
     ]
    }
   ],
   "source": [
    "n_words = max(list(word_to_int.values())) + 1\n",
    "\n",
    "rnn = SentimentRNN(n_words=n_words, \n",
    "                   seq_len=sequence_length, \n",
    "                   embed_size=256, \n",
    "                   lstm_size=128, \n",
    "                   num_layers=1, \n",
    "                   batch_size=100, \n",
    "                   learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice here that we use *num_layers=1* to use a single RNN layer. Although our implementation allows us to create a multilayer RNNs, by setting *num_layers* greater than 1. Here we should consider the small size of our dataset, and that a single RNN layer may generalize better to unseen data, since it is less likely to overfit the training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and optimizing the sentiment analysis RNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can train the RNN model by calling the *rnn.train* function. In the following code, we train the model for 40 epochs using the input from *X_train* and the corresponding class labels stored in *y_train*: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40 Iteration: 20 | Train loss: 0.69958\n",
      "Epoch 1/40 Iteration: 40 | Train loss: 0.62890\n",
      "Epoch 1/40 Iteration: 60 | Train loss: 0.64267\n",
      "Epoch 1/40 Iteration: 80 | Train loss: 0.57872\n",
      "Epoch 1/40 Iteration: 100 | Train loss: 0.56441\n",
      "Epoch 1/40 Iteration: 120 | Train loss: 0.56481\n",
      "Epoch 1/40 Iteration: 140 | Train loss: 0.50480\n",
      "Epoch 1/40 Iteration: 160 | Train loss: 0.51108\n",
      "Epoch 1/40 Iteration: 180 | Train loss: 0.47452\n",
      "Epoch 1/40 Iteration: 200 | Train loss: 0.42639\n",
      "Epoch 1/40 Iteration: 220 | Train loss: 0.41352\n",
      "Epoch 1/40 Iteration: 240 | Train loss: 0.44784\n",
      "Epoch 2/40 Iteration: 260 | Train loss: 0.39764\n",
      "Epoch 2/40 Iteration: 280 | Train loss: 0.37825\n",
      "Epoch 2/40 Iteration: 300 | Train loss: 0.30951\n",
      "Epoch 2/40 Iteration: 320 | Train loss: 0.25323\n",
      "Epoch 2/40 Iteration: 340 | Train loss: 0.33181\n",
      "Epoch 2/40 Iteration: 360 | Train loss: 0.25260\n",
      "Epoch 2/40 Iteration: 380 | Train loss: 0.37994\n",
      "Epoch 2/40 Iteration: 400 | Train loss: 0.32391\n",
      "Epoch 2/40 Iteration: 420 | Train loss: 0.28590\n",
      "Epoch 2/40 Iteration: 440 | Train loss: 0.28629\n",
      "Epoch 2/40 Iteration: 460 | Train loss: 0.49977\n",
      "Epoch 2/40 Iteration: 480 | Train loss: 0.26288\n",
      "Epoch 2/40 Iteration: 500 | Train loss: 0.24810\n",
      "Epoch 3/40 Iteration: 520 | Train loss: 0.40307\n",
      "Epoch 3/40 Iteration: 540 | Train loss: 0.25136\n",
      "Epoch 3/40 Iteration: 560 | Train loss: 0.41164\n",
      "Epoch 3/40 Iteration: 580 | Train loss: 0.27206\n",
      "Epoch 3/40 Iteration: 600 | Train loss: 0.22466\n",
      "Epoch 3/40 Iteration: 620 | Train loss: 0.26403\n",
      "Epoch 3/40 Iteration: 640 | Train loss: 0.19858\n",
      "Epoch 3/40 Iteration: 660 | Train loss: 0.18231\n",
      "Epoch 3/40 Iteration: 680 | Train loss: 0.29750\n",
      "Epoch 3/40 Iteration: 700 | Train loss: 0.16059\n",
      "Epoch 3/40 Iteration: 720 | Train loss: 0.24814\n",
      "Epoch 3/40 Iteration: 740 | Train loss: 0.26981\n",
      "Epoch 4/40 Iteration: 760 | Train loss: 0.22786\n",
      "Epoch 4/40 Iteration: 780 | Train loss: 0.22482\n",
      "Epoch 4/40 Iteration: 800 | Train loss: 0.11130\n",
      "Epoch 4/40 Iteration: 820 | Train loss: 0.31478\n",
      "Epoch 4/40 Iteration: 840 | Train loss: 0.19561\n",
      "Epoch 4/40 Iteration: 860 | Train loss: 0.14709\n",
      "Epoch 4/40 Iteration: 880 | Train loss: 0.24212\n",
      "Epoch 4/40 Iteration: 900 | Train loss: 0.17968\n",
      "Epoch 4/40 Iteration: 920 | Train loss: 0.20790\n",
      "Epoch 4/40 Iteration: 940 | Train loss: 0.15241\n",
      "Epoch 4/40 Iteration: 960 | Train loss: 0.15897\n",
      "Epoch 4/40 Iteration: 980 | Train loss: 0.15400\n",
      "Epoch 4/40 Iteration: 1000 | Train loss: 0.31969\n",
      "Epoch 5/40 Iteration: 1020 | Train loss: 0.14744\n",
      "Epoch 5/40 Iteration: 1040 | Train loss: 0.11334\n",
      "Epoch 5/40 Iteration: 1060 | Train loss: 0.20580\n",
      "Epoch 5/40 Iteration: 1080 | Train loss: 0.14594\n",
      "Epoch 5/40 Iteration: 1100 | Train loss: 0.06017\n",
      "Epoch 5/40 Iteration: 1120 | Train loss: 0.19716\n",
      "Epoch 5/40 Iteration: 1140 | Train loss: 0.12153\n",
      "Epoch 5/40 Iteration: 1160 | Train loss: 0.07483\n",
      "Epoch 5/40 Iteration: 1180 | Train loss: 0.11106\n",
      "Epoch 5/40 Iteration: 1200 | Train loss: 0.07324\n",
      "Epoch 5/40 Iteration: 1220 | Train loss: 0.09906\n",
      "Epoch 5/40 Iteration: 1240 | Train loss: 0.19839\n",
      "Epoch 6/40 Iteration: 1260 | Train loss: 0.07519\n",
      "Epoch 6/40 Iteration: 1280 | Train loss: 0.02650\n",
      "Epoch 6/40 Iteration: 1300 | Train loss: 0.07117\n",
      "Epoch 6/40 Iteration: 1320 | Train loss: 0.09500\n",
      "Epoch 6/40 Iteration: 1340 | Train loss: 0.05786\n",
      "Epoch 6/40 Iteration: 1360 | Train loss: 0.08426\n",
      "Epoch 6/40 Iteration: 1380 | Train loss: 0.08544\n",
      "Epoch 6/40 Iteration: 1400 | Train loss: 0.11900\n",
      "Epoch 6/40 Iteration: 1420 | Train loss: 0.09035\n",
      "Epoch 6/40 Iteration: 1440 | Train loss: 0.12220\n",
      "Epoch 6/40 Iteration: 1460 | Train loss: 0.32966\n",
      "Epoch 6/40 Iteration: 1480 | Train loss: 0.13025\n",
      "Epoch 6/40 Iteration: 1500 | Train loss: 0.10458\n",
      "Epoch 7/40 Iteration: 1520 | Train loss: 0.11861\n",
      "Epoch 7/40 Iteration: 1540 | Train loss: 0.07473\n",
      "Epoch 7/40 Iteration: 1560 | Train loss: 0.20360\n",
      "Epoch 7/40 Iteration: 1580 | Train loss: 0.05547\n",
      "Epoch 7/40 Iteration: 1600 | Train loss: 0.04844\n",
      "Epoch 7/40 Iteration: 1620 | Train loss: 0.12609\n",
      "Epoch 7/40 Iteration: 1640 | Train loss: 0.01784\n",
      "Epoch 7/40 Iteration: 1660 | Train loss: 0.02601\n",
      "Epoch 7/40 Iteration: 1680 | Train loss: 0.02967\n",
      "Epoch 7/40 Iteration: 1700 | Train loss: 0.02946\n",
      "Epoch 7/40 Iteration: 1720 | Train loss: 0.05913\n",
      "Epoch 7/40 Iteration: 1740 | Train loss: 0.05434\n",
      "Epoch 8/40 Iteration: 1760 | Train loss: 0.02269\n",
      "Epoch 8/40 Iteration: 1780 | Train loss: 0.02750\n",
      "Epoch 8/40 Iteration: 1800 | Train loss: 0.02006\n",
      "Epoch 8/40 Iteration: 1820 | Train loss: 0.02765\n",
      "Epoch 8/40 Iteration: 1840 | Train loss: 0.01369\n",
      "Epoch 8/40 Iteration: 1860 | Train loss: 0.03151\n",
      "Epoch 8/40 Iteration: 1880 | Train loss: 0.08518\n",
      "Epoch 8/40 Iteration: 1900 | Train loss: 0.06939\n",
      "Epoch 8/40 Iteration: 1920 | Train loss: 0.08638\n",
      "Epoch 8/40 Iteration: 1940 | Train loss: 0.04884\n",
      "Epoch 8/40 Iteration: 1960 | Train loss: 0.07556\n",
      "Epoch 8/40 Iteration: 1980 | Train loss: 0.14354\n",
      "Epoch 8/40 Iteration: 2000 | Train loss: 0.04755\n",
      "Epoch 9/40 Iteration: 2020 | Train loss: 0.05910\n",
      "Epoch 9/40 Iteration: 2040 | Train loss: 0.03710\n",
      "Epoch 9/40 Iteration: 2060 | Train loss: 0.06637\n",
      "Epoch 9/40 Iteration: 2080 | Train loss: 0.09210\n",
      "Epoch 9/40 Iteration: 2100 | Train loss: 0.03585\n",
      "Epoch 9/40 Iteration: 2120 | Train loss: 0.08653\n",
      "Epoch 9/40 Iteration: 2140 | Train loss: 0.12724\n",
      "Epoch 9/40 Iteration: 2160 | Train loss: 0.02180\n",
      "Epoch 9/40 Iteration: 2180 | Train loss: 0.04479\n",
      "Epoch 9/40 Iteration: 2200 | Train loss: 0.02537\n",
      "Epoch 9/40 Iteration: 2220 | Train loss: 0.06662\n",
      "Epoch 9/40 Iteration: 2240 | Train loss: 0.04517\n",
      "Epoch 10/40 Iteration: 2260 | Train loss: 0.01583\n",
      "Epoch 10/40 Iteration: 2280 | Train loss: 0.01302\n",
      "Epoch 10/40 Iteration: 2300 | Train loss: 0.01250\n",
      "Epoch 10/40 Iteration: 2320 | Train loss: 0.03017\n",
      "Epoch 10/40 Iteration: 2340 | Train loss: 0.01923\n",
      "Epoch 10/40 Iteration: 2360 | Train loss: 0.00690\n",
      "Epoch 10/40 Iteration: 2380 | Train loss: 0.03661\n",
      "Epoch 10/40 Iteration: 2400 | Train loss: 0.01338\n",
      "Epoch 10/40 Iteration: 2420 | Train loss: 0.09246\n",
      "Epoch 10/40 Iteration: 2440 | Train loss: 0.08763\n",
      "Epoch 10/40 Iteration: 2460 | Train loss: 0.03130\n",
      "Epoch 10/40 Iteration: 2480 | Train loss: 0.04457\n",
      "Epoch 10/40 Iteration: 2500 | Train loss: 0.02749\n",
      "Epoch 11/40 Iteration: 2520 | Train loss: 0.05788\n",
      "Epoch 11/40 Iteration: 2540 | Train loss: 0.01309\n",
      "Epoch 11/40 Iteration: 2560 | Train loss: 0.02433\n",
      "Epoch 11/40 Iteration: 2580 | Train loss: 0.01018\n",
      "Epoch 11/40 Iteration: 2600 | Train loss: 0.02860\n",
      "Epoch 11/40 Iteration: 2620 | Train loss: 0.04188\n",
      "Epoch 11/40 Iteration: 2640 | Train loss: 0.00633\n",
      "Epoch 11/40 Iteration: 2660 | Train loss: 0.01208\n",
      "Epoch 11/40 Iteration: 2680 | Train loss: 0.02004\n",
      "Epoch 11/40 Iteration: 2700 | Train loss: 0.00899\n",
      "Epoch 11/40 Iteration: 2720 | Train loss: 0.01879\n",
      "Epoch 11/40 Iteration: 2740 | Train loss: 0.05585\n",
      "Epoch 12/40 Iteration: 2760 | Train loss: 0.15768\n",
      "Epoch 12/40 Iteration: 2780 | Train loss: 0.00239\n",
      "Epoch 12/40 Iteration: 2800 | Train loss: 0.04917\n",
      "Epoch 12/40 Iteration: 2820 | Train loss: 0.05850\n",
      "Epoch 12/40 Iteration: 2840 | Train loss: 0.01792\n",
      "Epoch 12/40 Iteration: 2860 | Train loss: 0.00755\n",
      "Epoch 12/40 Iteration: 2880 | Train loss: 0.01838\n",
      "Epoch 12/40 Iteration: 2900 | Train loss: 0.02314\n",
      "Epoch 12/40 Iteration: 2920 | Train loss: 0.04700\n",
      "Epoch 12/40 Iteration: 2940 | Train loss: 0.00268\n",
      "Epoch 12/40 Iteration: 2960 | Train loss: 0.00674\n",
      "Epoch 12/40 Iteration: 2980 | Train loss: 0.00976\n",
      "Epoch 12/40 Iteration: 3000 | Train loss: 0.01709\n",
      "Epoch 13/40 Iteration: 3020 | Train loss: 0.03490\n",
      "Epoch 13/40 Iteration: 3040 | Train loss: 0.00312\n",
      "Epoch 13/40 Iteration: 3060 | Train loss: 0.00823\n",
      "Epoch 13/40 Iteration: 3080 | Train loss: 0.00156\n",
      "Epoch 13/40 Iteration: 3100 | Train loss: 0.04337\n",
      "Epoch 13/40 Iteration: 3120 | Train loss: 0.00398\n",
      "Epoch 13/40 Iteration: 3140 | Train loss: 0.00146\n",
      "Epoch 13/40 Iteration: 3160 | Train loss: 0.00721\n",
      "Epoch 13/40 Iteration: 3180 | Train loss: 0.00946\n",
      "Epoch 13/40 Iteration: 3200 | Train loss: 0.00099\n",
      "Epoch 13/40 Iteration: 3220 | Train loss: 0.00106\n",
      "Epoch 13/40 Iteration: 3240 | Train loss: 0.00315\n",
      "Epoch 14/40 Iteration: 3260 | Train loss: 0.00652\n",
      "Epoch 14/40 Iteration: 3280 | Train loss: 0.00096\n",
      "Epoch 14/40 Iteration: 3300 | Train loss: 0.00814\n",
      "Epoch 14/40 Iteration: 3320 | Train loss: 0.06354\n",
      "Epoch 14/40 Iteration: 3340 | Train loss: 0.01688\n",
      "Epoch 14/40 Iteration: 3360 | Train loss: 0.02321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/40 Iteration: 3380 | Train loss: 0.01933\n",
      "Epoch 14/40 Iteration: 3400 | Train loss: 0.02442\n",
      "Epoch 14/40 Iteration: 3420 | Train loss: 0.01667\n",
      "Epoch 14/40 Iteration: 3440 | Train loss: 0.00360\n",
      "Epoch 14/40 Iteration: 3460 | Train loss: 0.01194\n",
      "Epoch 14/40 Iteration: 3480 | Train loss: 0.01263\n",
      "Epoch 14/40 Iteration: 3500 | Train loss: 0.03146\n",
      "Epoch 15/40 Iteration: 3520 | Train loss: 0.01771\n",
      "Epoch 15/40 Iteration: 3540 | Train loss: 0.01262\n",
      "Epoch 15/40 Iteration: 3560 | Train loss: 0.00821\n",
      "Epoch 15/40 Iteration: 3580 | Train loss: 0.00374\n",
      "Epoch 15/40 Iteration: 3600 | Train loss: 0.00602\n",
      "Epoch 15/40 Iteration: 3620 | Train loss: 0.03432\n",
      "Epoch 15/40 Iteration: 3640 | Train loss: 0.05936\n",
      "Epoch 15/40 Iteration: 3660 | Train loss: 0.01452\n",
      "Epoch 15/40 Iteration: 3680 | Train loss: 0.00446\n",
      "Epoch 15/40 Iteration: 3700 | Train loss: 0.00493\n",
      "Epoch 15/40 Iteration: 3720 | Train loss: 0.00211\n",
      "Epoch 15/40 Iteration: 3740 | Train loss: 0.00135\n",
      "Epoch 16/40 Iteration: 3760 | Train loss: 0.05862\n",
      "Epoch 16/40 Iteration: 3780 | Train loss: 0.05829\n",
      "Epoch 16/40 Iteration: 3800 | Train loss: 0.00303\n",
      "Epoch 16/40 Iteration: 3820 | Train loss: 0.02846\n",
      "Epoch 16/40 Iteration: 3840 | Train loss: 0.00167\n",
      "Epoch 16/40 Iteration: 3860 | Train loss: 0.00099\n",
      "Epoch 16/40 Iteration: 3880 | Train loss: 0.00229\n",
      "Epoch 16/40 Iteration: 3900 | Train loss: 0.00661\n",
      "Epoch 16/40 Iteration: 3920 | Train loss: 0.00192\n",
      "Epoch 16/40 Iteration: 3940 | Train loss: 0.00803\n",
      "Epoch 16/40 Iteration: 3960 | Train loss: 0.00058\n",
      "Epoch 16/40 Iteration: 3980 | Train loss: 0.00127\n",
      "Epoch 16/40 Iteration: 4000 | Train loss: 0.00253\n",
      "Epoch 17/40 Iteration: 4020 | Train loss: 0.01097\n",
      "Epoch 17/40 Iteration: 4040 | Train loss: 0.00050\n",
      "Epoch 17/40 Iteration: 4060 | Train loss: 0.00105\n",
      "Epoch 17/40 Iteration: 4080 | Train loss: 0.01823\n",
      "Epoch 17/40 Iteration: 4100 | Train loss: 0.00426\n",
      "Epoch 17/40 Iteration: 4120 | Train loss: 0.00056\n",
      "Epoch 17/40 Iteration: 4140 | Train loss: 0.00078\n",
      "Epoch 17/40 Iteration: 4160 | Train loss: 0.00436\n",
      "Epoch 17/40 Iteration: 4180 | Train loss: 0.00032\n",
      "Epoch 17/40 Iteration: 4200 | Train loss: 0.00167\n",
      "Epoch 17/40 Iteration: 4220 | Train loss: 0.00582\n",
      "Epoch 17/40 Iteration: 4240 | Train loss: 0.00052\n",
      "Epoch 18/40 Iteration: 4260 | Train loss: 0.00166\n",
      "Epoch 18/40 Iteration: 4280 | Train loss: 0.00031\n",
      "Epoch 18/40 Iteration: 4300 | Train loss: 0.00048\n",
      "Epoch 18/40 Iteration: 4320 | Train loss: 0.00024\n",
      "Epoch 18/40 Iteration: 4340 | Train loss: 0.00027\n",
      "Epoch 18/40 Iteration: 4360 | Train loss: 0.00045\n",
      "Epoch 18/40 Iteration: 4380 | Train loss: 0.00315\n",
      "Epoch 18/40 Iteration: 4400 | Train loss: 0.00172\n",
      "Epoch 18/40 Iteration: 4420 | Train loss: 0.00097\n",
      "Epoch 18/40 Iteration: 4440 | Train loss: 0.00177\n",
      "Epoch 18/40 Iteration: 4460 | Train loss: 0.00016\n",
      "Epoch 18/40 Iteration: 4480 | Train loss: 0.00140\n",
      "Epoch 18/40 Iteration: 4500 | Train loss: 0.00065\n",
      "Epoch 19/40 Iteration: 4520 | Train loss: 0.00054\n",
      "Epoch 19/40 Iteration: 4540 | Train loss: 0.00013\n",
      "Epoch 19/40 Iteration: 4560 | Train loss: 0.00155\n",
      "Epoch 19/40 Iteration: 4580 | Train loss: 0.00126\n",
      "Epoch 19/40 Iteration: 4600 | Train loss: 0.00081\n",
      "Epoch 19/40 Iteration: 4620 | Train loss: 0.00749\n",
      "Epoch 19/40 Iteration: 4640 | Train loss: 0.00091\n",
      "Epoch 19/40 Iteration: 4660 | Train loss: 0.00047\n",
      "Epoch 19/40 Iteration: 4680 | Train loss: 0.00175\n",
      "Epoch 19/40 Iteration: 4700 | Train loss: 0.00025\n",
      "Epoch 19/40 Iteration: 4720 | Train loss: 0.00011\n",
      "Epoch 19/40 Iteration: 4740 | Train loss: 0.00831\n",
      "Epoch 20/40 Iteration: 4760 | Train loss: 0.01487\n",
      "Epoch 20/40 Iteration: 4780 | Train loss: 0.00603\n",
      "Epoch 20/40 Iteration: 4800 | Train loss: 0.01694\n",
      "Epoch 20/40 Iteration: 4820 | Train loss: 0.01253\n",
      "Epoch 20/40 Iteration: 4840 | Train loss: 0.00288\n",
      "Epoch 20/40 Iteration: 4860 | Train loss: 0.00451\n",
      "Epoch 20/40 Iteration: 4880 | Train loss: 0.02055\n",
      "Epoch 20/40 Iteration: 4900 | Train loss: 0.01078\n",
      "Epoch 20/40 Iteration: 4920 | Train loss: 0.00487\n",
      "Epoch 20/40 Iteration: 4940 | Train loss: 0.00294\n",
      "Epoch 20/40 Iteration: 4960 | Train loss: 0.00856\n",
      "Epoch 20/40 Iteration: 4980 | Train loss: 0.00181\n",
      "Epoch 20/40 Iteration: 5000 | Train loss: 0.00139\n",
      "Epoch 21/40 Iteration: 5020 | Train loss: 0.00090\n",
      "Epoch 21/40 Iteration: 5040 | Train loss: 0.00117\n",
      "Epoch 21/40 Iteration: 5060 | Train loss: 0.00143\n",
      "Epoch 21/40 Iteration: 5080 | Train loss: 0.03607\n",
      "Epoch 21/40 Iteration: 5100 | Train loss: 0.00123\n",
      "Epoch 21/40 Iteration: 5120 | Train loss: 0.00812\n",
      "Epoch 21/40 Iteration: 5140 | Train loss: 0.00473\n",
      "Epoch 21/40 Iteration: 5160 | Train loss: 0.00131\n",
      "Epoch 21/40 Iteration: 5180 | Train loss: 0.00670\n",
      "Epoch 21/40 Iteration: 5200 | Train loss: 0.00061\n",
      "Epoch 21/40 Iteration: 5220 | Train loss: 0.01438\n",
      "Epoch 21/40 Iteration: 5240 | Train loss: 0.00527\n",
      "Epoch 22/40 Iteration: 5260 | Train loss: 0.00090\n",
      "Epoch 22/40 Iteration: 5280 | Train loss: 0.00009\n",
      "Epoch 22/40 Iteration: 5300 | Train loss: 0.00148\n",
      "Epoch 22/40 Iteration: 5320 | Train loss: 0.00101\n",
      "Epoch 22/40 Iteration: 5340 | Train loss: 0.00213\n",
      "Epoch 22/40 Iteration: 5360 | Train loss: 0.00734\n",
      "Epoch 22/40 Iteration: 5380 | Train loss: 0.01303\n",
      "Epoch 22/40 Iteration: 5400 | Train loss: 0.00182\n",
      "Epoch 22/40 Iteration: 5420 | Train loss: 0.00582\n",
      "Epoch 22/40 Iteration: 5440 | Train loss: 0.00060\n",
      "Epoch 22/40 Iteration: 5460 | Train loss: 0.01637\n",
      "Epoch 22/40 Iteration: 5480 | Train loss: 0.00085\n",
      "Epoch 22/40 Iteration: 5500 | Train loss: 0.00254\n",
      "Epoch 23/40 Iteration: 5520 | Train loss: 0.00182\n",
      "Epoch 23/40 Iteration: 5540 | Train loss: 0.00027\n",
      "Epoch 23/40 Iteration: 5560 | Train loss: 0.00682\n",
      "Epoch 23/40 Iteration: 5580 | Train loss: 0.00063\n",
      "Epoch 23/40 Iteration: 5600 | Train loss: 0.00036\n",
      "Epoch 23/40 Iteration: 5620 | Train loss: 0.00112\n",
      "Epoch 23/40 Iteration: 5640 | Train loss: 0.00180\n",
      "Epoch 23/40 Iteration: 5660 | Train loss: 0.00068\n",
      "Epoch 23/40 Iteration: 5680 | Train loss: 0.00581\n",
      "Epoch 23/40 Iteration: 5700 | Train loss: 0.00029\n",
      "Epoch 23/40 Iteration: 5720 | Train loss: 0.00029\n",
      "Epoch 23/40 Iteration: 5740 | Train loss: 0.00034\n",
      "Epoch 24/40 Iteration: 5760 | Train loss: 0.00078\n",
      "Epoch 24/40 Iteration: 5780 | Train loss: 0.00044\n",
      "Epoch 24/40 Iteration: 5800 | Train loss: 0.00037\n",
      "Epoch 24/40 Iteration: 5820 | Train loss: 0.00046\n",
      "Epoch 24/40 Iteration: 5840 | Train loss: 0.00139\n",
      "Epoch 24/40 Iteration: 5860 | Train loss: 0.00100\n",
      "Epoch 24/40 Iteration: 5880 | Train loss: 0.00453\n",
      "Epoch 24/40 Iteration: 5900 | Train loss: 0.00488\n",
      "Epoch 24/40 Iteration: 5920 | Train loss: 0.09039\n",
      "Epoch 24/40 Iteration: 5940 | Train loss: 0.00176\n",
      "Epoch 24/40 Iteration: 5960 | Train loss: 0.00269\n",
      "Epoch 24/40 Iteration: 5980 | Train loss: 0.00051\n",
      "Epoch 24/40 Iteration: 6000 | Train loss: 0.00550\n",
      "Epoch 25/40 Iteration: 6020 | Train loss: 0.00161\n",
      "Epoch 25/40 Iteration: 6040 | Train loss: 0.00051\n",
      "Epoch 25/40 Iteration: 6060 | Train loss: 0.00130\n",
      "Epoch 25/40 Iteration: 6080 | Train loss: 0.00112\n",
      "Epoch 25/40 Iteration: 6100 | Train loss: 0.00041\n",
      "Epoch 25/40 Iteration: 6120 | Train loss: 0.00255\n",
      "Epoch 25/40 Iteration: 6140 | Train loss: 0.00083\n",
      "Epoch 25/40 Iteration: 6160 | Train loss: 0.00149\n",
      "Epoch 25/40 Iteration: 6180 | Train loss: 0.00144\n",
      "Epoch 25/40 Iteration: 6200 | Train loss: 0.00058\n",
      "Epoch 25/40 Iteration: 6220 | Train loss: 0.00027\n",
      "Epoch 25/40 Iteration: 6240 | Train loss: 0.00043\n",
      "Epoch 26/40 Iteration: 6260 | Train loss: 0.00058\n",
      "Epoch 26/40 Iteration: 6280 | Train loss: 0.00026\n",
      "Epoch 26/40 Iteration: 6300 | Train loss: 0.00025\n",
      "Epoch 26/40 Iteration: 6320 | Train loss: 0.00235\n",
      "Epoch 26/40 Iteration: 6340 | Train loss: 0.00308\n",
      "Epoch 26/40 Iteration: 6360 | Train loss: 0.00037\n",
      "Epoch 26/40 Iteration: 6380 | Train loss: 0.00193\n",
      "Epoch 26/40 Iteration: 6400 | Train loss: 0.00073\n",
      "Epoch 26/40 Iteration: 6420 | Train loss: 0.00053\n",
      "Epoch 26/40 Iteration: 6440 | Train loss: 0.00106\n",
      "Epoch 26/40 Iteration: 6460 | Train loss: 0.00497\n",
      "Epoch 26/40 Iteration: 6480 | Train loss: 0.00177\n",
      "Epoch 26/40 Iteration: 6500 | Train loss: 0.00059\n",
      "Epoch 27/40 Iteration: 6520 | Train loss: 0.00135\n",
      "Epoch 27/40 Iteration: 6540 | Train loss: 0.00138\n",
      "Epoch 27/40 Iteration: 6560 | Train loss: 0.00247\n",
      "Epoch 27/40 Iteration: 6580 | Train loss: 0.00159\n",
      "Epoch 27/40 Iteration: 6600 | Train loss: 0.08248\n",
      "Epoch 27/40 Iteration: 6620 | Train loss: 0.00895\n",
      "Epoch 27/40 Iteration: 6640 | Train loss: 0.00539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/40 Iteration: 6660 | Train loss: 0.01875\n",
      "Epoch 27/40 Iteration: 6680 | Train loss: 0.00133\n",
      "Epoch 27/40 Iteration: 6700 | Train loss: 0.00087\n",
      "Epoch 27/40 Iteration: 6720 | Train loss: 0.00988\n",
      "Epoch 27/40 Iteration: 6740 | Train loss: 0.00096\n",
      "Epoch 28/40 Iteration: 6760 | Train loss: 0.00093\n",
      "Epoch 28/40 Iteration: 6780 | Train loss: 0.00045\n",
      "Epoch 28/40 Iteration: 6800 | Train loss: 0.00052\n",
      "Epoch 28/40 Iteration: 6820 | Train loss: 0.00032\n",
      "Epoch 28/40 Iteration: 6840 | Train loss: 0.00090\n",
      "Epoch 28/40 Iteration: 6860 | Train loss: 0.00033\n",
      "Epoch 28/40 Iteration: 6880 | Train loss: 0.00033\n",
      "Epoch 28/40 Iteration: 6900 | Train loss: 0.00080\n",
      "Epoch 28/40 Iteration: 6920 | Train loss: 0.00079\n",
      "Epoch 28/40 Iteration: 6940 | Train loss: 0.00043\n",
      "Epoch 28/40 Iteration: 6960 | Train loss: 0.00114\n",
      "Epoch 28/40 Iteration: 6980 | Train loss: 0.00023\n",
      "Epoch 28/40 Iteration: 7000 | Train loss: 0.00060\n",
      "Epoch 29/40 Iteration: 7020 | Train loss: 0.00055\n",
      "Epoch 29/40 Iteration: 7040 | Train loss: 0.00014\n",
      "Epoch 29/40 Iteration: 7060 | Train loss: 0.00038\n",
      "Epoch 29/40 Iteration: 7080 | Train loss: 0.00022\n",
      "Epoch 29/40 Iteration: 7100 | Train loss: 0.00041\n",
      "Epoch 29/40 Iteration: 7120 | Train loss: 0.00012\n",
      "Epoch 29/40 Iteration: 7140 | Train loss: 0.00007\n",
      "Epoch 29/40 Iteration: 7160 | Train loss: 0.00032\n",
      "Epoch 29/40 Iteration: 7180 | Train loss: 0.00058\n",
      "Epoch 29/40 Iteration: 7200 | Train loss: 0.00017\n",
      "Epoch 29/40 Iteration: 7220 | Train loss: 0.00005\n",
      "Epoch 29/40 Iteration: 7240 | Train loss: 0.00008\n",
      "Epoch 30/40 Iteration: 7260 | Train loss: 0.00009\n",
      "Epoch 30/40 Iteration: 7280 | Train loss: 0.00019\n",
      "Epoch 30/40 Iteration: 7300 | Train loss: 0.00014\n",
      "Epoch 30/40 Iteration: 7320 | Train loss: 0.00010\n",
      "Epoch 30/40 Iteration: 7340 | Train loss: 0.00023\n",
      "Epoch 30/40 Iteration: 7360 | Train loss: 0.00007\n",
      "Epoch 30/40 Iteration: 7380 | Train loss: 0.00013\n",
      "Epoch 30/40 Iteration: 7400 | Train loss: 0.00062\n",
      "Epoch 30/40 Iteration: 7420 | Train loss: 0.00005\n",
      "Epoch 30/40 Iteration: 7440 | Train loss: 0.00016\n",
      "Epoch 30/40 Iteration: 7460 | Train loss: 0.00012\n",
      "Epoch 30/40 Iteration: 7480 | Train loss: 0.00008\n",
      "Epoch 30/40 Iteration: 7500 | Train loss: 0.00011\n",
      "Epoch 31/40 Iteration: 7520 | Train loss: 0.00013\n",
      "Epoch 31/40 Iteration: 7540 | Train loss: 0.00002\n",
      "Epoch 31/40 Iteration: 7560 | Train loss: 0.00009\n",
      "Epoch 31/40 Iteration: 7580 | Train loss: 0.00005\n",
      "Epoch 31/40 Iteration: 7600 | Train loss: 0.00006\n",
      "Epoch 31/40 Iteration: 7620 | Train loss: 0.00037\n",
      "Epoch 31/40 Iteration: 7640 | Train loss: 0.00006\n",
      "Epoch 31/40 Iteration: 7660 | Train loss: 0.00022\n",
      "Epoch 31/40 Iteration: 7680 | Train loss: 0.00011\n",
      "Epoch 31/40 Iteration: 7700 | Train loss: 0.00016\n",
      "Epoch 31/40 Iteration: 7720 | Train loss: 0.00004\n",
      "Epoch 31/40 Iteration: 7740 | Train loss: 0.00004\n",
      "Epoch 32/40 Iteration: 7760 | Train loss: 0.00008\n",
      "Epoch 32/40 Iteration: 7780 | Train loss: 0.00004\n",
      "Epoch 32/40 Iteration: 7800 | Train loss: 0.00002\n",
      "Epoch 32/40 Iteration: 7820 | Train loss: 0.00002\n",
      "Epoch 32/40 Iteration: 7840 | Train loss: 0.00025\n",
      "Epoch 32/40 Iteration: 7860 | Train loss: 0.00005\n",
      "Epoch 32/40 Iteration: 7880 | Train loss: 0.00004\n",
      "Epoch 32/40 Iteration: 7900 | Train loss: 0.00005\n",
      "Epoch 32/40 Iteration: 7920 | Train loss: 0.00001\n",
      "Epoch 32/40 Iteration: 7940 | Train loss: 0.00009\n",
      "Epoch 32/40 Iteration: 7960 | Train loss: 0.00002\n",
      "Epoch 32/40 Iteration: 7980 | Train loss: 0.00010\n",
      "Epoch 32/40 Iteration: 8000 | Train loss: 0.00003\n",
      "Epoch 33/40 Iteration: 8020 | Train loss: 0.00009\n",
      "Epoch 33/40 Iteration: 8040 | Train loss: 0.00006\n",
      "Epoch 33/40 Iteration: 8060 | Train loss: 0.00007\n",
      "Epoch 33/40 Iteration: 8080 | Train loss: 0.00006\n",
      "Epoch 33/40 Iteration: 8100 | Train loss: 0.00004\n",
      "Epoch 33/40 Iteration: 8120 | Train loss: 0.00114\n",
      "Epoch 33/40 Iteration: 8140 | Train loss: 0.00001\n",
      "Epoch 33/40 Iteration: 8160 | Train loss: 0.00013\n",
      "Epoch 33/40 Iteration: 8180 | Train loss: 0.00003\n",
      "Epoch 33/40 Iteration: 8200 | Train loss: 0.00002\n",
      "Epoch 33/40 Iteration: 8220 | Train loss: 0.00002\n",
      "Epoch 33/40 Iteration: 8240 | Train loss: 0.00003\n",
      "Epoch 34/40 Iteration: 8260 | Train loss: 0.00004\n",
      "Epoch 34/40 Iteration: 8280 | Train loss: 0.00008\n",
      "Epoch 34/40 Iteration: 8300 | Train loss: 0.00003\n",
      "Epoch 34/40 Iteration: 8320 | Train loss: 0.00001\n",
      "Epoch 34/40 Iteration: 8340 | Train loss: 0.00004\n",
      "Epoch 34/40 Iteration: 8360 | Train loss: 0.00005\n",
      "Epoch 34/40 Iteration: 8380 | Train loss: 0.00002\n",
      "Epoch 34/40 Iteration: 8400 | Train loss: 0.00031\n",
      "Epoch 34/40 Iteration: 8420 | Train loss: 0.00003\n",
      "Epoch 34/40 Iteration: 8440 | Train loss: 0.00007\n",
      "Epoch 34/40 Iteration: 8460 | Train loss: 0.00006\n",
      "Epoch 34/40 Iteration: 8480 | Train loss: 0.00002\n",
      "Epoch 34/40 Iteration: 8500 | Train loss: 0.00001\n",
      "Epoch 35/40 Iteration: 8520 | Train loss: 0.00002\n",
      "Epoch 35/40 Iteration: 8540 | Train loss: 0.00007\n",
      "Epoch 35/40 Iteration: 8560 | Train loss: 0.00010\n",
      "Epoch 35/40 Iteration: 8580 | Train loss: 0.00024\n",
      "Epoch 35/40 Iteration: 8600 | Train loss: 0.00002\n",
      "Epoch 35/40 Iteration: 8620 | Train loss: 0.00002\n",
      "Epoch 35/40 Iteration: 8640 | Train loss: 0.00001\n",
      "Epoch 35/40 Iteration: 8660 | Train loss: 0.00002\n",
      "Epoch 35/40 Iteration: 8680 | Train loss: 0.00002\n",
      "Epoch 35/40 Iteration: 8700 | Train loss: 0.00002\n",
      "Epoch 35/40 Iteration: 8720 | Train loss: 0.00001\n",
      "Epoch 35/40 Iteration: 8740 | Train loss: 0.00003\n",
      "Epoch 36/40 Iteration: 8760 | Train loss: 0.00002\n",
      "Epoch 36/40 Iteration: 8780 | Train loss: 0.00026\n",
      "Epoch 36/40 Iteration: 8800 | Train loss: 0.00002\n",
      "Epoch 36/40 Iteration: 8820 | Train loss: 0.00000\n",
      "Epoch 36/40 Iteration: 8840 | Train loss: 0.00006\n",
      "Epoch 36/40 Iteration: 8860 | Train loss: 0.00005\n",
      "Epoch 36/40 Iteration: 8880 | Train loss: 0.00005\n",
      "Epoch 36/40 Iteration: 8900 | Train loss: 0.00009\n",
      "Epoch 36/40 Iteration: 8920 | Train loss: 0.00003\n",
      "Epoch 36/40 Iteration: 8940 | Train loss: 0.00002\n",
      "Epoch 36/40 Iteration: 8960 | Train loss: 0.00002\n",
      "Epoch 36/40 Iteration: 8980 | Train loss: 0.00001\n",
      "Epoch 36/40 Iteration: 9000 | Train loss: 0.00001\n",
      "Epoch 37/40 Iteration: 9020 | Train loss: 0.00006\n",
      "Epoch 37/40 Iteration: 9040 | Train loss: 0.00001\n",
      "Epoch 37/40 Iteration: 9060 | Train loss: 0.00007\n",
      "Epoch 37/40 Iteration: 9080 | Train loss: 0.00003\n",
      "Epoch 37/40 Iteration: 9100 | Train loss: 0.00001\n",
      "Epoch 37/40 Iteration: 9120 | Train loss: 0.00012\n",
      "Epoch 37/40 Iteration: 9140 | Train loss: 0.00007\n",
      "Epoch 37/40 Iteration: 9160 | Train loss: 0.00005\n",
      "Epoch 37/40 Iteration: 9180 | Train loss: 0.00001\n",
      "Epoch 37/40 Iteration: 9200 | Train loss: 0.00001\n",
      "Epoch 37/40 Iteration: 9220 | Train loss: 0.00004\n",
      "Epoch 37/40 Iteration: 9240 | Train loss: 0.00001\n",
      "Epoch 38/40 Iteration: 9260 | Train loss: 0.00002\n",
      "Epoch 38/40 Iteration: 9280 | Train loss: 0.00001\n",
      "Epoch 38/40 Iteration: 9300 | Train loss: 0.00001\n",
      "Epoch 38/40 Iteration: 9320 | Train loss: 0.00001\n",
      "Epoch 38/40 Iteration: 9340 | Train loss: 0.00003\n",
      "Epoch 38/40 Iteration: 9360 | Train loss: 0.00001\n",
      "Epoch 38/40 Iteration: 9380 | Train loss: 0.00001\n",
      "Epoch 38/40 Iteration: 9400 | Train loss: 0.00001\n",
      "Epoch 38/40 Iteration: 9420 | Train loss: 0.00001\n",
      "Epoch 38/40 Iteration: 9440 | Train loss: 0.00002\n",
      "Epoch 38/40 Iteration: 9460 | Train loss: 0.00002\n",
      "Epoch 38/40 Iteration: 9480 | Train loss: 0.00002\n",
      "Epoch 38/40 Iteration: 9500 | Train loss: 0.00006\n",
      "Epoch 39/40 Iteration: 9520 | Train loss: 0.00092\n",
      "Epoch 39/40 Iteration: 9540 | Train loss: 0.00306\n",
      "Epoch 39/40 Iteration: 9560 | Train loss: 0.01709\n",
      "Epoch 39/40 Iteration: 9580 | Train loss: 0.01798\n",
      "Epoch 39/40 Iteration: 9600 | Train loss: 0.12664\n",
      "Epoch 39/40 Iteration: 9620 | Train loss: 0.01687\n",
      "Epoch 39/40 Iteration: 9640 | Train loss: 0.00727\n",
      "Epoch 39/40 Iteration: 9660 | Train loss: 0.00462\n",
      "Epoch 39/40 Iteration: 9680 | Train loss: 0.05224\n",
      "Epoch 39/40 Iteration: 9700 | Train loss: 0.00544\n",
      "Epoch 39/40 Iteration: 9720 | Train loss: 0.00282\n",
      "Epoch 39/40 Iteration: 9740 | Train loss: 0.00656\n",
      "Epoch 40/40 Iteration: 9760 | Train loss: 0.09288\n",
      "Epoch 40/40 Iteration: 9780 | Train loss: 0.00726\n",
      "Epoch 40/40 Iteration: 9800 | Train loss: 0.00167\n",
      "Epoch 40/40 Iteration: 9820 | Train loss: 0.00115\n",
      "Epoch 40/40 Iteration: 9840 | Train loss: 0.00141\n",
      "Epoch 40/40 Iteration: 9860 | Train loss: 0.00069\n",
      "Epoch 40/40 Iteration: 9880 | Train loss: 0.00128\n",
      "Epoch 40/40 Iteration: 9900 | Train loss: 0.03402\n",
      "Epoch 40/40 Iteration: 9920 | Train loss: 0.01177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/40 Iteration: 9940 | Train loss: 0.00268\n",
      "Epoch 40/40 Iteration: 9960 | Train loss: 0.00092\n",
      "Epoch 40/40 Iteration: 9980 | Train loss: 0.00989\n",
      "Epoch 40/40 Iteration: 10000 | Train loss: 0.01256\n"
     ]
    }
   ],
   "source": [
    "rnn.train(X_train, y_train, num_epochs=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained model is saved using TensorFlow's checkpointing system. Now, we can use the trained model for predicting the class labels on the test set, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/sentiment-39.ckpt\n",
      "Test Acc.: 0.853\n"
     ]
    }
   ],
   "source": [
    "preds = rnn.predict(X_test)\n",
    "y_true = y_test[:len(preds)]\n",
    "print('Test Acc.: %.3f' % (np.sum(preds==y_true) / len(y_true)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result will show an accuracy of 86 percent. Given the small size of this dataset, this is comparable to the test prediction accuracy obtained in Chapter 8. \n",
    "\n",
    "We can optimize this further by changing the hyperparameters of the model, such as *lstm_size*, *seq_len*, and *embed_size*, to achieve better generalization performance. However, for hyperparameter tuning, it is recommended that we create a separate validation set and that we do not repeatedly use the test set for evaluation to avoid introducing bias through test data leakage. \n",
    "\n",
    "Also, if you are interested in the prediction probabilities on the test set rather than the class labels, then you can set *return_proba=True* as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/sentiment-39.ckpt\n",
      "[1.7920318e-06 9.9999547e-01 6.9740318e-02 ... 6.9026237e-06 1.4025514e-01\n",
      " 9.9837083e-01]\n"
     ]
    }
   ],
   "source": [
    "proba = rnn.predict(X_test, return_proba=True)\n",
    "print(proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this was our first RNN model for sentiment analysis. We will now go further and create an RNN for character-by-character language modeling in TensorFlow, as another popular application of sequence modeling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project two - implementing an RNN for character-level language modeling in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language modeling is a fascinating application that enables machines to perform human-language-related tasks, such as generating English sentences.\n",
    "\n",
    "In the model that we will build now, the input is a text document, and our goal is to develop a model that can generate new text similar to the input document. Examples of such an input can be a book or a computer program in a specific programming language. \n",
    "\n",
    "In character-level language modeling, the input is broken down into a sequence of characters that are fed into our network one character at time. The network will process each new character in conjunction with the memory of the previously seen characters to predict the new character. The following figure shows an example of character-level language modeling:\n",
    "\n",
    "<img src='images/16_11.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can break this implementation down into three separate steps - preparing the data, building the RNN model, and performing next-character prediction and sampling to generate new text. \n",
    "\n",
    "If you recall from the previous sections of this chapter, we mentioned the exploding gradient problem. In this application, we will also get a chance to play with a gradient clipping technique to avoid this exploding gradient problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we prepare the data for character-level language modeling. \n",
    "\n",
    "To get the input data, visit the Project Gutenberg website at https://www.gutenberg.org, which provides thousands of free e-books. For our example, we can get the book *The Tragedie of Hamlet* by William Shakespeare in plain text format from http://www.gutenberg.org/cache/epub/2265/pg2265.txt. \n",
    "\n",
    "Once we have some data, we can read it into a Python session as plain text. In the following code, the Python variable *chars* represents the set of *unique* characters observed in this text. We then create a dictionary that maps each character to an integer, *char2int*, and a dictionary that performs reverse mapping, for instance, mapping integers to those unique characters - *int2char*. Using the *char2int* dictionary, we convert the text into a NumPy array of integers. The following figure shows an example of converting characters into integers and the reverse for the words *\"Hello\"* and *\"world\"*: \n",
    "\n",
    "<img src='images/16_12.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code reads the text from the downloaded link, removes the beginning portion of the text that contains some legal description of the Gutenberg project, and then constructs the dictionaries based on the text: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "# Reading and processing text\n",
    "with open('pg2265.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "text = text[15858:]\n",
    "chars = set(text)\n",
    "char2int = {ch:i for i, ch in enumerate(chars)}\n",
    "int2char = dict(enumerate(chars))\n",
    "text_ints = np.array([char2int[ch] for ch in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we should reshape the data into batches of sequences, the most important step in preparing the data. As we know, the goal is to predict the next character based on the sequence of characters that we have observed so far. Therefore, we shift the input $x$ and output $y$ of the neural network by one character. The following figure shows the preprocessing steps, starting from a text corpus to generating data arrays for $x$ and $y$: \n",
    "\n",
    "<img src='images/16_13.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in this figure, the training arrays $x$ and $y$ have the same shapes or dimensions, where the number of rows is equal to the *batch size* and the number of columns is *number of batches x number of steps*. \n",
    "\n",
    "Given the input array *data* that contains the integers that correspond to the characters in the text corpus, the following function will generate $x$ and $y$ with the same structure shown in the previous figure: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_data(sequence, batch_size, num_steps):\n",
    "    tot_batch_length = batch_size * num_steps\n",
    "    num_batches = int(len(sequence) / tot_batch_length)\n",
    "    if num_batches*tot_batch_length + 1 > len(sequence):\n",
    "        num_batches = num_batches - 1\n",
    "    # Truncate the sequence at the end to get rid of \n",
    "    # remaining characters that do not make a full batch\n",
    "    x = sequence[0:num_batches*tot_batch_length]\n",
    "    y = sequence[1:num_batches*tot_batch_length+1]\n",
    "    # Split x and y into a list batches of sequences\n",
    "    x_batch_splits = np.split(x, batch_size)\n",
    "    y_batch_splits = np.split(y, batch_size)\n",
    "    # Stack the batches together\n",
    "    # batch size x tot_bach_length\n",
    "    x = np.stack(x_batch_splits)\n",
    "    y = np.stack(y_batch_splits)\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to split the arrays $x$ and $y$ into mini-batches where each row is a sequence with length equal to the *number of steps*. The process of splitting the data array $x$ is shown in the following figure: \n",
    "\n",
    "<img src='images/16_14.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code, we define a function named *create_batch_generator* that splits the data arrays $x$ and $y$, as shown in the previous figure, and outputs a batch generator. Later, we will use this generator to iterate through the mini-batches during the training of our network: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch_generator(data_x, data_y, num_steps):\n",
    "    batch_size, tot_batch_length = data_x.shape\n",
    "    num_batches = int(tot_batch_length/num_steps)\n",
    "    for b in range(num_batches):\n",
    "        yield (data_x[:, b*num_steps:(b+1)*num_steps], \n",
    "               data_y[:, b*num_steps:(b+1)*num_steps])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this points, we have now completed the data preprocessing steps, and we have the data in the proper format. In the next section, we will implement the RNN model for character-level language modeling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a character-level RNN model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a character-level neural network, we will implement a class called *CharRNN* that constructs the graph of the RNN in order to predict the next character, after observing a given sequence of characters. From the classification perspective, the number of classes is the total number of unique characters that exists in the text corpus. The *CharRNN* class has four methods, as follows: \n",
    "\n",
    "* A constructor that sets up the learning parameters, creates a computation graph, and calls the *build* method to construct the graph based on the sampling mode versus the training mode. \n",
    "* A *build* method that defines the placeholders for feeding the data, constructs the RNN using LSTM cells, and defines the output of the network, the cost function, and the optimizer. \n",
    "* A *train* method to iterate through the mini-batches and train the network for the specified number of epochs. \n",
    "* A *sample* method to start from a given string, calculate the probabilities for the next character, and choose a character randomly according to these probabilities. This process will be repeated, and the sampled characters will be concatenated together to form a string. Once the size of this string reaches the specified length, it will return the string. \n",
    "\n",
    "We will break these four methods into separate code sections and explain each one. Note that implementing the RNN part of this model is very similar to the implementation in the *Project One*. So, we will skip the description of building the RNN components here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The constructor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast to our previous implementation for sentiment analysis, where the same computation graph was used for both training and prediction modes, this time our computation graph is going to be different for the training versus the sampling mode. \n",
    "\n",
    "Therefore, we need to add a new Boolean type argument to the constructor, to determine whether we are building the model for the training mode or the sampling mode. The following code shows the implementation of the constructor enclosed in the class definition: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "def get_top_char(probas, char_size, top_n=5):\n",
    "    p = np.squeeze(probas)\n",
    "    p[np.argsort(p)[:-top_n]] = 0.0\n",
    "    p = p / np.sum(p)\n",
    "    ch_id = np.random.choice(char_size, 1, p=p)[0]\n",
    "    return ch_id\n",
    "\n",
    "class CharRNN(object):\n",
    "    \n",
    "    def __init__(self, num_classes, batch_size=64, \n",
    "                 num_steps=100, lstm_size=128, \n",
    "                 num_layers=1, learning_rate=0.001, \n",
    "                 keep_prob=0.5, grad_clip=5, \n",
    "                 sampling=False):\n",
    "        self.num_classes = num_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        self.lstm_size = lstm_size\n",
    "        self.num_layers = num_layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.keep_prob = keep_prob\n",
    "        self.grad_clip = grad_clip\n",
    "        \n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            tf.set_random_seed(123)\n",
    "            \n",
    "            self.build(sampling=sampling)\n",
    "            \n",
    "            self.saver = tf.train.Saver()\n",
    "            \n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "    \n",
    "    def build(self, sampling):\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size = self.batch_size\n",
    "            num_steps = self.num_steps\n",
    "        \n",
    "        ## Define the placeholders\n",
    "        tf_x = tf.placeholder(tf.int32, shape=(batch_size, num_steps), name='tf_x')\n",
    "        tf_y = tf.placeholder(tf.int32, shape=(batch_size, num_steps), name='tf_y')\n",
    "        tf_keepprob = tf.placeholder(tf.float32, name='tf_keepprob')\n",
    "       \n",
    "        ## One-hot encoding\n",
    "        x_onehot = tf.one_hot(tf_x, depth=self.num_classes)\n",
    "        y_onehot = tf.one_hot(tf_y, depth=self.num_classes)\n",
    "        \n",
    "        ## Build the multi-layer RNN cells\n",
    "        cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.BasicLSTMCell(self.lstm_size), output_keep_prob=tf_keepprob) for _ in range(self.num_layers)])\n",
    "        \n",
    "        ## Define the initial state\n",
    "        self.initial_state = cells.zero_state(batch_size, tf.float32)\n",
    "        \n",
    "        ## Run each sequence step through the RNN\n",
    "        lstm_outputs, self.final_state = tf.nn.dynamic_rnn(cells, x_onehot, initial_state=self.initial_state)\n",
    "        \n",
    "        seq_output_reshaped = tf.reshape(lstm_outputs, shape=[-1, self.lstm_size], name='seq_output_reshaped')\n",
    "        \n",
    "        logits = tf.layers.dense(inputs=seq_output_reshaped, units=self.num_classes, activation=None, name='logits')\n",
    "        \n",
    "        proba = tf.nn.softmax(logits, name='probabilities')\n",
    "        \n",
    "        y_reshaped = tf.reshape(y_onehot, shape=[-1, self.num_classes], name='y_reshaped')\n",
    "        \n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_reshaped, logits=logits), name='cost')\n",
    "        \n",
    "        # Gradient clipping to avoid \"exploding gradients\"\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), self.grad_clip)\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        train_op = optimizer.apply_gradients(zip(grads, tvars), name='train_op')\n",
    "        \n",
    "    def train(self, train_x, train_y, num_epochs, \n",
    "              ckpt_dir='./model/'):\n",
    "        ## Create the checkpoint directory\n",
    "        ## if it does not exist\n",
    "        if not os.path.exists(ckpt_dir):\n",
    "            os.mkdir(ckpt_dir)  \n",
    "        \n",
    "        with tf.Session(graph=self.g) as sess:\n",
    "            sess.run(self.init_op)\n",
    "            \n",
    "            n_batches = int(train_x.shape[1] / self.num_steps)\n",
    "            iterations = n_batches * num_epochs\n",
    "            \n",
    "            for epoch in range(num_epochs):\n",
    "                ## Train network\n",
    "                new_state = sess.run(self.initial_state)\n",
    "                loss = 0\n",
    "                ## Mini-batch generator\n",
    "                bgen = create_batch_generator(train_x, train_y, self.num_steps)\n",
    "                \n",
    "                for b, (batch_x, batch_y) in enumerate(bgen, 1):\n",
    "                    iteration = epoch*n_batches + b\n",
    "                    \n",
    "                    feed = {'tf_x:0': batch_x, \n",
    "                            'tf_y:0': batch_y, \n",
    "                            'tf_keepprob:0': self.keep_prob, \n",
    "                            self.initial_state: new_state}\n",
    "                    batch_cost, _, new_state = sess.run(['cost:0', 'train_op', self.final_state], feed_dict=feed)\n",
    "                    \n",
    "                    if iteration % 10 == 0:\n",
    "                        print('Epoch %d/%d Iteration: %d | Training loss: %.4f' % (epoch+1, num_epochs, iteration, batch_cost))\n",
    "                \n",
    "                ## Save the trained model\n",
    "                self.saver.save(sess, os.path.join(ckpt_dir, 'language_modeling.ckpt'))\n",
    "    \n",
    "    def sample(self, output_length, ckpt_dir, \n",
    "               starter_seq=\"The \"):\n",
    "        observed_seq = [ch for ch in starter_seq]\n",
    "        with tf.Session(graph=self.g) as sess:\n",
    "            self.saver.restore(sess, tf.train.latest_checkpoint(ckpt_dir))\n",
    "            \n",
    "            ## 1: run the model using the starter sequence\n",
    "            new_state = sess.run(self.initial_state)\n",
    "            for ch in starter_seq:\n",
    "                x = np.zeros((1, 1))\n",
    "                x[0, 0] = char2int[ch]\n",
    "                feed = {'tf_x:0': x, \n",
    "                        'tf_keepprob:0': 1.0, \n",
    "                        self.initial_state: new_state}\n",
    "                proba, new_state = sess.run(['probabilities:0', self.final_state], feed_dict=feed)\n",
    "                \n",
    "            ch_id = get_top_char(proba, len(chars))\n",
    "            observed_seq.append(int2char[ch_id])\n",
    "                \n",
    "            ## 2: run the model using the updated observed_seq\n",
    "            for i in range(output_length):\n",
    "                x[0,0] = ch_id\n",
    "                feed = {'tf_x:0': x, \n",
    "                        'tf_keepprob:0': 1.0, \n",
    "                        self.initial_state: new_state}\n",
    "                proba, new_state = sess.run(['probabilities:0', self.final_state], feed_dict=feed)\n",
    "\n",
    "                ch_id = get_top_char(proba, len(chars))\n",
    "                observed_seq.append(int2char[ch_id])\n",
    "        \n",
    "        return ''.join(observed_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we planned earlier, the Boolean *sampling* argument is used to determine whether the instance of *CharRNN* is for building the graph in the training mode (*sampling=False*) or the sampling mode (*sampling=True*). \n",
    "\n",
    "In addition to the *sampling* argument, we have introduced a new argument called *grad_clip*, which is used for clipping the gradients to avoid the exploding gradient problem that we mentioned earlier. \n",
    "\n",
    "Then, similar to the previous implementation, the constructor creates a computation graph, sets the graph-level random seed for consistent output, and builds the graph by calling the *build* method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The build method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next method of the *CharRNN* class is *build*, which is very similar to the *build* method in the *Project One*, except for some minor differences. The *build* method first defines two local variables, *batch_size* and *num_steps*, based on the mode, as follows:\n",
    "\n",
    "* In sampling mode: batch_size = 1 and num_steps = 1\n",
    "* In training mode: batch_size = self.batch_size and num_steps = self.num_steps\n",
    "\n",
    "Recall that in the sentiment analysis implementation, we used an embedding layer to create a salient representation of the unique words in the dataset. In contrast, here we are using the one-hot encoding scheme for both $x$ and $y$ with *depth=num_classes*, where *num_classes* is in fact the total number of characters in the text corpus. \n",
    "\n",
    "Building a multilayer RNN component of the model is exactly the same as in our sentiment analysis implementation, using the *tf.nn.dynamic_rnn* function. However, *outputs* form the *tf.nn.dynamic_rnn* function is a three-dimensional tensor with this shape - *batch_size, num_steps, lstm_size*. Next, this tensor will be reshaped into a two-dimensional tensor with the *batch_size*num_steps, lstm_size* shape, which is passed to the *tf.layers.dense* function to make a fully connected layer and obtain *logits* (net inputs). Finally, the probabilities for the next batch of characters are obtained and the cost function is defined. In addition, here, we apply gradient clipping using the *tf.clip_by_global_norm* function to avoid the exploding gradient problem. \n",
    "\n",
    "The following code shows the implementation of what we have just described for our new *build* method: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The train method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next method of the *CharRNN* class is the *train* method, which is very similar to the *train* method described in the *Project One*. Here is the *train* method code, which will look very familiar to the sentiment analysis version we built earlier in this chapter: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The sample method "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final method in our *CharRNN* class is the *sample* method. The behavior of this *sample* method is similar to that of the *predict* method that we implemented in the *Project One*. However, the difference here is that we calculate the probabilities for the next character from an observed sequence - *observed_seq*. Then, these probabilities are passed to a function named *get_top_char*, which randomly selects one character according to the obtained probabilities. \n",
    "\n",
    "Initially, the observed sequence starts from *starter_seq*, which is provided as an argument. When new characters are sampled according to their predicted probabilities, they are appended to the observed sequence, and the new observed sequence is used for predicting the next character. \n",
    "\n",
    "The implementation of the *sample* method is as follows:\n",
    "\n",
    "So here, the *sample* method calls the *get_top_char* function to choose a character ID randomly (*ch_id*) according to the obtained probabilities. \n",
    "\n",
    "In this *get_top_char* function, the probabilities are first sorted, then the *top_n* probabilities are passed to the *numpy.random.choice* function to randomly select one out of these top probabilities. The implementation of the *get_top_char* function is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and training the CharRNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to create an instance of the *CharRNN* class to build the RNN model, and to train it with the following configurations: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-20-784f6e96ca40>:70: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "Epoch 1/100 Iteration: 10 | Training loss: 3.7287\n",
      "Epoch 1/100 Iteration: 20 | Training loss: 3.3645\n",
      "Epoch 2/100 Iteration: 30 | Training loss: 3.2881\n",
      "Epoch 2/100 Iteration: 40 | Training loss: 3.2401\n",
      "Epoch 2/100 Iteration: 50 | Training loss: 3.2364\n",
      "Epoch 3/100 Iteration: 60 | Training loss: 3.2156\n",
      "Epoch 3/100 Iteration: 70 | Training loss: 3.1875\n",
      "Epoch 4/100 Iteration: 80 | Training loss: 3.1801\n",
      "Epoch 4/100 Iteration: 90 | Training loss: 3.1576\n",
      "Epoch 4/100 Iteration: 100 | Training loss: 3.1440\n",
      "Epoch 5/100 Iteration: 110 | Training loss: 3.1313\n",
      "Epoch 5/100 Iteration: 120 | Training loss: 3.0920\n",
      "Epoch 6/100 Iteration: 130 | Training loss: 3.0538\n",
      "Epoch 6/100 Iteration: 140 | Training loss: 3.0104\n",
      "Epoch 6/100 Iteration: 150 | Training loss: 2.9652\n",
      "Epoch 7/100 Iteration: 160 | Training loss: 2.9453\n",
      "Epoch 7/100 Iteration: 170 | Training loss: 2.8769\n",
      "Epoch 8/100 Iteration: 180 | Training loss: 2.8283\n",
      "Epoch 8/100 Iteration: 190 | Training loss: 2.8002\n",
      "Epoch 8/100 Iteration: 200 | Training loss: 2.7327\n",
      "Epoch 9/100 Iteration: 210 | Training loss: 2.7317\n",
      "Epoch 9/100 Iteration: 220 | Training loss: 2.6803\n",
      "Epoch 10/100 Iteration: 230 | Training loss: 2.6414\n",
      "Epoch 10/100 Iteration: 240 | Training loss: 2.6480\n",
      "Epoch 10/100 Iteration: 250 | Training loss: 2.5841\n",
      "Epoch 11/100 Iteration: 260 | Training loss: 2.5952\n",
      "Epoch 11/100 Iteration: 270 | Training loss: 2.5511\n",
      "Epoch 12/100 Iteration: 280 | Training loss: 2.5217\n",
      "Epoch 12/100 Iteration: 290 | Training loss: 2.5313\n",
      "Epoch 12/100 Iteration: 300 | Training loss: 2.4686\n",
      "Epoch 13/100 Iteration: 310 | Training loss: 2.5037\n",
      "Epoch 13/100 Iteration: 320 | Training loss: 2.4569\n",
      "Epoch 14/100 Iteration: 330 | Training loss: 2.4390\n",
      "Epoch 14/100 Iteration: 340 | Training loss: 2.4585\n",
      "Epoch 14/100 Iteration: 350 | Training loss: 2.4061\n",
      "Epoch 15/100 Iteration: 360 | Training loss: 2.4376\n",
      "Epoch 15/100 Iteration: 370 | Training loss: 2.4012\n",
      "Epoch 16/100 Iteration: 380 | Training loss: 2.3963\n",
      "Epoch 16/100 Iteration: 390 | Training loss: 2.4088\n",
      "Epoch 16/100 Iteration: 400 | Training loss: 2.3552\n",
      "Epoch 17/100 Iteration: 410 | Training loss: 2.3895\n",
      "Epoch 17/100 Iteration: 420 | Training loss: 2.3597\n",
      "Epoch 18/100 Iteration: 430 | Training loss: 2.3523\n",
      "Epoch 18/100 Iteration: 440 | Training loss: 2.3666\n",
      "Epoch 18/100 Iteration: 450 | Training loss: 2.3031\n",
      "Epoch 19/100 Iteration: 460 | Training loss: 2.3420\n",
      "Epoch 19/100 Iteration: 470 | Training loss: 2.3299\n",
      "Epoch 20/100 Iteration: 480 | Training loss: 2.3133\n",
      "Epoch 20/100 Iteration: 490 | Training loss: 2.3320\n",
      "Epoch 20/100 Iteration: 500 | Training loss: 2.2781\n",
      "Epoch 21/100 Iteration: 510 | Training loss: 2.3243\n",
      "Epoch 21/100 Iteration: 520 | Training loss: 2.3000\n",
      "Epoch 22/100 Iteration: 530 | Training loss: 2.2866\n",
      "Epoch 22/100 Iteration: 540 | Training loss: 2.3140\n",
      "Epoch 22/100 Iteration: 550 | Training loss: 2.2470\n",
      "Epoch 23/100 Iteration: 560 | Training loss: 2.2953\n",
      "Epoch 23/100 Iteration: 570 | Training loss: 2.2581\n",
      "Epoch 24/100 Iteration: 580 | Training loss: 2.2660\n",
      "Epoch 24/100 Iteration: 590 | Training loss: 2.2801\n",
      "Epoch 24/100 Iteration: 600 | Training loss: 2.2256\n",
      "Epoch 25/100 Iteration: 610 | Training loss: 2.2753\n",
      "Epoch 25/100 Iteration: 620 | Training loss: 2.2498\n",
      "Epoch 26/100 Iteration: 630 | Training loss: 2.2367\n",
      "Epoch 26/100 Iteration: 640 | Training loss: 2.2552\n",
      "Epoch 26/100 Iteration: 650 | Training loss: 2.2047\n",
      "Epoch 27/100 Iteration: 660 | Training loss: 2.2586\n",
      "Epoch 27/100 Iteration: 670 | Training loss: 2.2261\n",
      "Epoch 28/100 Iteration: 680 | Training loss: 2.2188\n",
      "Epoch 28/100 Iteration: 690 | Training loss: 2.2539\n",
      "Epoch 28/100 Iteration: 700 | Training loss: 2.1885\n",
      "Epoch 29/100 Iteration: 710 | Training loss: 2.2420\n",
      "Epoch 29/100 Iteration: 720 | Training loss: 2.1960\n",
      "Epoch 30/100 Iteration: 730 | Training loss: 2.2118\n",
      "Epoch 30/100 Iteration: 740 | Training loss: 2.2268\n",
      "Epoch 30/100 Iteration: 750 | Training loss: 2.1524\n",
      "Epoch 31/100 Iteration: 760 | Training loss: 2.2101\n",
      "Epoch 31/100 Iteration: 770 | Training loss: 2.1898\n",
      "Epoch 32/100 Iteration: 780 | Training loss: 2.1900\n",
      "Epoch 32/100 Iteration: 790 | Training loss: 2.2180\n",
      "Epoch 32/100 Iteration: 800 | Training loss: 2.1540\n",
      "Epoch 33/100 Iteration: 810 | Training loss: 2.1993\n",
      "Epoch 33/100 Iteration: 820 | Training loss: 2.1651\n",
      "Epoch 34/100 Iteration: 830 | Training loss: 2.1770\n",
      "Epoch 34/100 Iteration: 840 | Training loss: 2.1941\n",
      "Epoch 34/100 Iteration: 850 | Training loss: 2.1325\n",
      "Epoch 35/100 Iteration: 860 | Training loss: 2.1719\n",
      "Epoch 35/100 Iteration: 870 | Training loss: 2.1503\n",
      "Epoch 36/100 Iteration: 880 | Training loss: 2.1569\n",
      "Epoch 36/100 Iteration: 890 | Training loss: 2.1768\n",
      "Epoch 36/100 Iteration: 900 | Training loss: 2.1054\n",
      "Epoch 37/100 Iteration: 910 | Training loss: 2.1583\n",
      "Epoch 37/100 Iteration: 920 | Training loss: 2.1370\n",
      "Epoch 38/100 Iteration: 930 | Training loss: 2.1483\n",
      "Epoch 38/100 Iteration: 940 | Training loss: 2.1717\n",
      "Epoch 38/100 Iteration: 950 | Training loss: 2.1090\n",
      "Epoch 39/100 Iteration: 960 | Training loss: 2.1658\n",
      "Epoch 39/100 Iteration: 970 | Training loss: 2.1236\n",
      "Epoch 40/100 Iteration: 980 | Training loss: 2.1266\n",
      "Epoch 40/100 Iteration: 990 | Training loss: 2.1457\n",
      "Epoch 40/100 Iteration: 1000 | Training loss: 2.0978\n",
      "Epoch 41/100 Iteration: 1010 | Training loss: 2.1495\n",
      "Epoch 41/100 Iteration: 1020 | Training loss: 2.1010\n",
      "Epoch 42/100 Iteration: 1030 | Training loss: 2.1175\n",
      "Epoch 42/100 Iteration: 1040 | Training loss: 2.1498\n",
      "Epoch 42/100 Iteration: 1050 | Training loss: 2.0769\n",
      "Epoch 43/100 Iteration: 1060 | Training loss: 2.1378\n",
      "Epoch 43/100 Iteration: 1070 | Training loss: 2.0973\n",
      "Epoch 44/100 Iteration: 1080 | Training loss: 2.1011\n",
      "Epoch 44/100 Iteration: 1090 | Training loss: 2.1266\n",
      "Epoch 44/100 Iteration: 1100 | Training loss: 2.0645\n",
      "Epoch 45/100 Iteration: 1110 | Training loss: 2.1236\n",
      "Epoch 45/100 Iteration: 1120 | Training loss: 2.0859\n",
      "Epoch 46/100 Iteration: 1130 | Training loss: 2.0816\n",
      "Epoch 46/100 Iteration: 1140 | Training loss: 2.1206\n",
      "Epoch 46/100 Iteration: 1150 | Training loss: 2.0473\n",
      "Epoch 47/100 Iteration: 1160 | Training loss: 2.1034\n",
      "Epoch 47/100 Iteration: 1170 | Training loss: 2.0759\n",
      "Epoch 48/100 Iteration: 1180 | Training loss: 2.0780\n",
      "Epoch 48/100 Iteration: 1190 | Training loss: 2.1055\n",
      "Epoch 48/100 Iteration: 1200 | Training loss: 2.0366\n",
      "Epoch 49/100 Iteration: 1210 | Training loss: 2.0856\n",
      "Epoch 49/100 Iteration: 1220 | Training loss: 2.0537\n",
      "Epoch 50/100 Iteration: 1230 | Training loss: 2.0632\n",
      "Epoch 50/100 Iteration: 1240 | Training loss: 2.0967\n",
      "Epoch 50/100 Iteration: 1250 | Training loss: 2.0340\n",
      "Epoch 51/100 Iteration: 1260 | Training loss: 2.0845\n",
      "Epoch 51/100 Iteration: 1270 | Training loss: 2.0371\n",
      "Epoch 52/100 Iteration: 1280 | Training loss: 2.0349\n",
      "Epoch 52/100 Iteration: 1290 | Training loss: 2.0874\n",
      "Epoch 52/100 Iteration: 1300 | Training loss: 2.0174\n",
      "Epoch 53/100 Iteration: 1310 | Training loss: 2.0649\n",
      "Epoch 53/100 Iteration: 1320 | Training loss: 2.0341\n",
      "Epoch 54/100 Iteration: 1330 | Training loss: 2.0379\n",
      "Epoch 54/100 Iteration: 1340 | Training loss: 2.0683\n",
      "Epoch 54/100 Iteration: 1350 | Training loss: 2.0049\n",
      "Epoch 55/100 Iteration: 1360 | Training loss: 2.0462\n",
      "Epoch 55/100 Iteration: 1370 | Training loss: 2.0259\n",
      "Epoch 56/100 Iteration: 1380 | Training loss: 2.0299\n",
      "Epoch 56/100 Iteration: 1390 | Training loss: 2.0668\n",
      "Epoch 56/100 Iteration: 1400 | Training loss: 1.9945\n",
      "Epoch 57/100 Iteration: 1410 | Training loss: 2.0448\n",
      "Epoch 57/100 Iteration: 1420 | Training loss: 2.0284\n",
      "Epoch 58/100 Iteration: 1430 | Training loss: 2.0231\n",
      "Epoch 58/100 Iteration: 1440 | Training loss: 2.0512\n",
      "Epoch 58/100 Iteration: 1450 | Training loss: 1.9832\n",
      "Epoch 59/100 Iteration: 1460 | Training loss: 2.0277\n",
      "Epoch 59/100 Iteration: 1470 | Training loss: 2.0039\n",
      "Epoch 60/100 Iteration: 1480 | Training loss: 2.0046\n",
      "Epoch 60/100 Iteration: 1490 | Training loss: 2.0537\n",
      "Epoch 60/100 Iteration: 1500 | Training loss: 1.9773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/100 Iteration: 1510 | Training loss: 2.0235\n",
      "Epoch 61/100 Iteration: 1520 | Training loss: 1.9941\n",
      "Epoch 62/100 Iteration: 1530 | Training loss: 1.9986\n",
      "Epoch 62/100 Iteration: 1540 | Training loss: 2.0255\n",
      "Epoch 62/100 Iteration: 1550 | Training loss: 1.9739\n",
      "Epoch 63/100 Iteration: 1560 | Training loss: 2.0103\n",
      "Epoch 63/100 Iteration: 1570 | Training loss: 1.9979\n",
      "Epoch 64/100 Iteration: 1580 | Training loss: 1.9969\n",
      "Epoch 64/100 Iteration: 1590 | Training loss: 2.0227\n",
      "Epoch 64/100 Iteration: 1600 | Training loss: 1.9530\n",
      "Epoch 65/100 Iteration: 1610 | Training loss: 2.0092\n",
      "Epoch 65/100 Iteration: 1620 | Training loss: 1.9868\n",
      "Epoch 66/100 Iteration: 1630 | Training loss: 1.9812\n",
      "Epoch 66/100 Iteration: 1640 | Training loss: 2.0193\n",
      "Epoch 66/100 Iteration: 1650 | Training loss: 1.9446\n",
      "Epoch 67/100 Iteration: 1660 | Training loss: 1.9951\n",
      "Epoch 67/100 Iteration: 1670 | Training loss: 1.9750\n",
      "Epoch 68/100 Iteration: 1680 | Training loss: 1.9604\n",
      "Epoch 68/100 Iteration: 1690 | Training loss: 2.0105\n",
      "Epoch 68/100 Iteration: 1700 | Training loss: 1.9360\n",
      "Epoch 69/100 Iteration: 1710 | Training loss: 1.9816\n",
      "Epoch 69/100 Iteration: 1720 | Training loss: 1.9631\n",
      "Epoch 70/100 Iteration: 1730 | Training loss: 1.9588\n",
      "Epoch 70/100 Iteration: 1740 | Training loss: 2.0093\n",
      "Epoch 70/100 Iteration: 1750 | Training loss: 1.9281\n",
      "Epoch 71/100 Iteration: 1760 | Training loss: 1.9703\n",
      "Epoch 71/100 Iteration: 1770 | Training loss: 1.9591\n",
      "Epoch 72/100 Iteration: 1780 | Training loss: 1.9504\n",
      "Epoch 72/100 Iteration: 1790 | Training loss: 1.9990\n",
      "Epoch 72/100 Iteration: 1800 | Training loss: 1.9055\n",
      "Epoch 73/100 Iteration: 1810 | Training loss: 1.9748\n",
      "Epoch 73/100 Iteration: 1820 | Training loss: 1.9503\n",
      "Epoch 74/100 Iteration: 1830 | Training loss: 1.9516\n",
      "Epoch 74/100 Iteration: 1840 | Training loss: 1.9871\n",
      "Epoch 74/100 Iteration: 1850 | Training loss: 1.9077\n",
      "Epoch 75/100 Iteration: 1860 | Training loss: 1.9720\n",
      "Epoch 75/100 Iteration: 1870 | Training loss: 1.9495\n",
      "Epoch 76/100 Iteration: 1880 | Training loss: 1.9331\n",
      "Epoch 76/100 Iteration: 1890 | Training loss: 1.9587\n",
      "Epoch 76/100 Iteration: 1900 | Training loss: 1.9137\n",
      "Epoch 77/100 Iteration: 1910 | Training loss: 1.9579\n",
      "Epoch 77/100 Iteration: 1920 | Training loss: 1.9313\n",
      "Epoch 78/100 Iteration: 1930 | Training loss: 1.9291\n",
      "Epoch 78/100 Iteration: 1940 | Training loss: 1.9677\n",
      "Epoch 78/100 Iteration: 1950 | Training loss: 1.8985\n",
      "Epoch 79/100 Iteration: 1960 | Training loss: 1.9579\n",
      "Epoch 79/100 Iteration: 1970 | Training loss: 1.9348\n",
      "Epoch 80/100 Iteration: 1980 | Training loss: 1.9300\n",
      "Epoch 80/100 Iteration: 1990 | Training loss: 1.9650\n",
      "Epoch 80/100 Iteration: 2000 | Training loss: 1.9060\n",
      "Epoch 81/100 Iteration: 2010 | Training loss: 1.9508\n",
      "Epoch 81/100 Iteration: 2020 | Training loss: 1.9254\n",
      "Epoch 82/100 Iteration: 2030 | Training loss: 1.9186\n",
      "Epoch 82/100 Iteration: 2040 | Training loss: 1.9547\n",
      "Epoch 82/100 Iteration: 2050 | Training loss: 1.9006\n",
      "Epoch 83/100 Iteration: 2060 | Training loss: 1.9368\n",
      "Epoch 83/100 Iteration: 2070 | Training loss: 1.9200\n",
      "Epoch 84/100 Iteration: 2080 | Training loss: 1.9243\n",
      "Epoch 84/100 Iteration: 2090 | Training loss: 1.9619\n",
      "Epoch 84/100 Iteration: 2100 | Training loss: 1.8790\n",
      "Epoch 85/100 Iteration: 2110 | Training loss: 1.9302\n",
      "Epoch 85/100 Iteration: 2120 | Training loss: 1.9224\n",
      "Epoch 86/100 Iteration: 2130 | Training loss: 1.9097\n",
      "Epoch 86/100 Iteration: 2140 | Training loss: 1.9423\n",
      "Epoch 86/100 Iteration: 2150 | Training loss: 1.8794\n",
      "Epoch 87/100 Iteration: 2160 | Training loss: 1.9215\n",
      "Epoch 87/100 Iteration: 2170 | Training loss: 1.8930\n",
      "Epoch 88/100 Iteration: 2180 | Training loss: 1.8956\n",
      "Epoch 88/100 Iteration: 2190 | Training loss: 1.9315\n",
      "Epoch 88/100 Iteration: 2200 | Training loss: 1.8665\n",
      "Epoch 89/100 Iteration: 2210 | Training loss: 1.9205\n",
      "Epoch 89/100 Iteration: 2220 | Training loss: 1.8885\n",
      "Epoch 90/100 Iteration: 2230 | Training loss: 1.8877\n",
      "Epoch 90/100 Iteration: 2240 | Training loss: 1.9299\n",
      "Epoch 90/100 Iteration: 2250 | Training loss: 1.8701\n",
      "Epoch 91/100 Iteration: 2260 | Training loss: 1.9057\n",
      "Epoch 91/100 Iteration: 2270 | Training loss: 1.8868\n",
      "Epoch 92/100 Iteration: 2280 | Training loss: 1.9071\n",
      "Epoch 92/100 Iteration: 2290 | Training loss: 1.9250\n",
      "Epoch 92/100 Iteration: 2300 | Training loss: 1.8551\n",
      "Epoch 93/100 Iteration: 2310 | Training loss: 1.8961\n",
      "Epoch 93/100 Iteration: 2320 | Training loss: 1.8781\n",
      "Epoch 94/100 Iteration: 2330 | Training loss: 1.8918\n",
      "Epoch 94/100 Iteration: 2340 | Training loss: 1.9246\n",
      "Epoch 94/100 Iteration: 2350 | Training loss: 1.8591\n",
      "Epoch 95/100 Iteration: 2360 | Training loss: 1.9072\n",
      "Epoch 95/100 Iteration: 2370 | Training loss: 1.8782\n",
      "Epoch 96/100 Iteration: 2380 | Training loss: 1.8703\n",
      "Epoch 96/100 Iteration: 2390 | Training loss: 1.9196\n",
      "Epoch 96/100 Iteration: 2400 | Training loss: 1.8436\n",
      "Epoch 97/100 Iteration: 2410 | Training loss: 1.8911\n",
      "Epoch 97/100 Iteration: 2420 | Training loss: 1.8705\n",
      "Epoch 98/100 Iteration: 2430 | Training loss: 1.8792\n",
      "Epoch 98/100 Iteration: 2440 | Training loss: 1.9072\n",
      "Epoch 98/100 Iteration: 2450 | Training loss: 1.8423\n",
      "Epoch 99/100 Iteration: 2460 | Training loss: 1.8873\n",
      "Epoch 99/100 Iteration: 2470 | Training loss: 1.8705\n",
      "Epoch 100/100 Iteration: 2480 | Training loss: 1.8644\n",
      "Epoch 100/100 Iteration: 2490 | Training loss: 1.9047\n",
      "Epoch 100/100 Iteration: 2500 | Training loss: 1.8314\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_steps = 100\n",
    "\n",
    "train_x, train_y = reshape_data(text_ints, batch_size, \n",
    "                                num_steps)\n",
    "\n",
    "rnn = CharRNN(num_classes=len(chars), batch_size=batch_size)\n",
    "rnn.train(train_x, train_y, num_epochs=100, \n",
    "          ckpt_dir='./model-100/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained model will be saved in a directory called *./model-100/* so that we can reload it later for prediction or for continuing the training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The CharRNN model in the sampling mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, we can create a new instance of the *CharRNN* class in the sampling mode by specifying that *sampling=True*. We will call the *sample* method to load the saved model in the *./model-100/* folder, and generate a sequence of 500 characters: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model-100/language_modeling.ckpt\n",
      "The seall, in his, in the bese me and mestine:\n",
      "I worcout in his sollous and shall wish with\n",
      "\n",
      "   Ham. The same thould to heart, wire thes times in the part\n",
      "And a the soull the withen, what hiu that in a but\n",
      "Are a farth mess ant that same that hount,\n",
      "It the beare andore all thee hash aray astitinnesers,\n",
      "And, at is the prosing tone tersine,\n",
      "I mart the morithousse of the pestion to him\n",
      "Whore than the menthers as oul the bants, thith,\n",
      "To beere heere, these whilge the hourd, and to my Lord:\n",
      "If astind the m\n"
     ]
    }
   ],
   "source": [
    "del rnn\n",
    "\n",
    "np.random.seed(123)\n",
    "rnn = CharRNN(len(chars), sampling=True)\n",
    "print(rnn.sample(ckpt_dir='./model-100/', output_length=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated text will look like the following: \n",
    "\n",
    "<img src='images/16_15.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that in the resulting output, that some English words are mostly preserved. It's also important to note that this is from an old English text; therefore, some words in the original text may by unfamiliar. To get a better result, we would need to train the model for higher number of epochs. Feel free to repeat this with a much larger document and train the model for more epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter and book summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hope you enjoyed this last chapter of *Python Machine Learning* and our exciting tour of machine learning and deep learning. Through the journey of this book, we have covered the essential topics that this field has to offer, and you should now be well equipped to put those techniques into action to solve real-world problems. \n",
    "\n",
    "We started our journey with a brief overview of the different types of learning tasks: supervised learning, reinforcement learning, and unsupervised learning. We then discussed several different learning algorithms that you can use for classification, starting with simple single-layer neural networks in Chapter 2. \n",
    "\n",
    "We continued to discuss advanced classification algorithms in Chapter 3, and we learned about the most important aspects of a machine learning pipeline in Chapter 4. \n",
    "\n",
    "Remember that even the most advanced algorithm is limited by the information in the training data that it gets to learn from. So in Chapter 6, we learned about the best practices to build and evaluate predictive models, which is another important aspect in machine learning applications. \n",
    "\n",
    "If one single learning algorithm does not achieve the performance we desire, it can be sometimes helpful to create an ensemble of experts to make a prediction. We explored this in Chapter 7. \n",
    "\n",
    "THen in Chapter 8, we applied machine learning to analyze one of the most popular and interesting forms of data in the modern age that's dominated by social media plataform on the internet - text documents. \n",
    "\n",
    "Next, we reminded ourselves that machine learning techniques are not limited to offline data analysis, and in Chapter 9, we saw how to embed a machine learning model into a web application to share it with the outside world. \n",
    "\n",
    "For the most part, our focus was on algorithms for classification, which is probably the most popular application of machine learning. However, this is not where our journey ended! In Chapter 10, we explored several algorithms for regression analysis to predict continuous valued output values. \n",
    "\n",
    "Another exciting subfield of machine learning is clustering analysis, which can help us find hidden structures in the data, even if our training data does not come with the right answers to learn from. We worked with this in Chapter 11. \n",
    "\n",
    "We then shifted our attention to one of the most exciting algorithms in the whole machine learning field - artificial neural networks. We started by implementing a multilayer perceptron from scratch with NumPy in Chapter 12. \n",
    "\n",
    "The power of TensorFlow became obvious in Chapter 13, where we used TensorFlow to facilitate the process of build neural network models and make use of GPUs to make the training of multilayer neural networks more efficient. \n",
    "\n",
    "We delved deeper into the mechanics of TensorFlow in Chapter 14, and discussed the different aspects and mechanics of TensorFlow, including variables and operators in a TensorFlow computation graph, variables scopes, launching graphs, and different ways of executing nodes. \n",
    "\n",
    "In Chapter 15, we dived into convolutional neural networks, which are widely usde in computer vision at the moment, due to their great performance in image classification tasks. \n",
    "\n",
    "Finally, here in Chapter 16, we learned about sequence modeling using RNNs. While a comprehensive study of deep learning is well beyond the scope of this book, we hope that we have kindled your interest enough to follow the most recent advancements in this field of deep learning. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
