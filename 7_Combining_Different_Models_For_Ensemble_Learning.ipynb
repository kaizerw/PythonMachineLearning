{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Different Models for Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous chapter, we focused on the best practices for tuning and evaluating different models for classification. In this chapter, we will build upon these techniques and explore different methods for constructing a set of classifiers that can often have a better predictive performance than any of its individual members. We will learn how to do the following:\n",
    "\n",
    "* Make predictions based on majority voting\n",
    "* Use bagging to reduce overfitting by drawing random combinations of the training set with repetition\n",
    "* Apply boosting to build powerful models from *weak learners* that learn from their mistakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning with ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of **ensemble methods** is to combine different classifiers into a meta-classifier that has better generalization performance than each individual classifier alone. For example, assuming that we collected predictions from 10 experts, ensemble methods would allow us to strategically combine these predictions by the 10 experts to come up with a prediction that is more accurate and robust than the predictions bt each individual expert. As we will see later in this chapter, there are several different approaches for creating an ensemble of classifiers. In this section, we will introduce a basic perception of how ensembles work and why they are typically recognized for yielding a good generalization performance. \n",
    "\n",
    "In this chapter, we will focus on the most popular ensemble method that use the **majority voting** principle. Majority voting simply means that we select the class label that has been predicted by the majority of classifiers, that is, received more than 50 percent of the votes. Strictly speaking, the term **majority vote** refers to binary class settings only. However, it is easy to generalize the majority voting principle to multi-class settings, which is called **plurality voting**. Here, we select the class label that received the most votes (mode). The following diagram illustrates the concept of majority and plurality voting for an ensemble of 10 classifiers where each unique symbol (triangle, square and circle) represents a unique class label:\n",
    "\n",
    "<img src='images/07_01.png'>\n",
    "\n",
    "Using the training set, we start by training *m* different classifiers ($C_1, \\ldots, C_m$). Depending on the technique, the ensemble can be built from different classification algorithms, for example, decision trees, suport vector machines, logistic regression classifiers, and so on. Alternatively, we can also use the same class classification algorithm, fitting different subsets of the training set. One prominent example of this approach is the random forest algorithm, which combines different decision tree classifiers. The following figure illustrates the concept of a general ensemble approach using majority voting: \n",
    "\n",
    "<img src='images/07_02.png'>\n",
    "\n",
    "To predict a class label via simple majority or plurality voting, we combine the predicted class labels of each individual classifier and select the class label that received the most votes. \n",
    "\n",
    "To illustrate why ensemble methods can work betten than individual classifiers alone, let's apply the simple concepts of combinatorics. For the following example, we make the assumption that all $n$-base classifiers for a binary classification task have an equal error rate $\\epsilon$. Furthermore, we assume that the classifiers are independent and the error rates are not correlated. Under those assumptions, we can simply express the error probability of an ensemble of base classifiers as a probability mass function of a binomial distribution:\n",
    "\n",
    "$$P(y \\ge k) = \\sum_k^n {n \\choose k} \\epsilon^k (1 - 0.25)^{n-k} = \\epsilon_{ensemble}$$\n",
    "\n",
    "Here, ${n \\choose k}$ is the binomial coefficient **n choose k**. In other words, we compute the probability that the prediction of the ensemble is wrong. Now let's take a look at a more concrete example of 11 base classifiers ($n = 11$), where each classifier has an error rate of 0.25 ($\\epsilon = 0.25$):\n",
    "\n",
    "$$P(y \\ge k) = \\sum_{k=6}^{11} {11 \\choose k} 0.25^k (1 - \\epsilon)^{11-k} = 0.034$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the error rate of ensemble (0.034) is much lower than the error rate of each individual classifier (0.25) if all the assumptions are met. Note that, in this simplified illustration, a 50-50 split by an even number of classifiers $n$ is treated as an error, whereas this is only true half of time. To compare such an idealistic ensemble classifier to a base classifier over a range of different base error rates, let's implement the probability mass function in Python: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03432750701904297"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.special import comb\n",
    "import math\n",
    "\n",
    "def ensemble_error(n_classifier, error):\n",
    "    k_start = int(math.ceil(n_classifier / 2.0))\n",
    "    probs = [comb(n_classifier, k) * error**k * (1-error)**(n_classifier - k) for k in range(k_start, n_classifier + 1)]\n",
    "    return sum(probs)\n",
    "\n",
    "ensemble_error(n_classifier=11, error=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have implemented the *ensemble_error* function, we can compute the ensemble error rates for a range of different base errors from 0.0 to 1.0 to visualize the relationship between ensemble and base errors in a line graph: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "error_range = np.arange(0.0, 1.01, 0.01)\n",
    "ens_errors = [ensemble_error(n_classifier=11, error=error)\n",
    "              for error in error_range]\n",
    "plt.plot(error_range, ens_errors, label='Ensemble error', \n",
    "         linewidth=2)\n",
    "plt.plot(error_range, error_range, linestyle='--', \n",
    "         label='Base error', linewidth=2)\n",
    "plt.xlabel('Base error')\n",
    "plt.ylabel('Base/Ensemble error')\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the resulting plot, the error probability of an ensemble is always better than the error of an individual base classifier, as long as the base classifiers perform betten than random guessing ($\\epsilon < 0.5$). Note that the $y$-axis depicts the base error (dotted line) as well as the ensemble error (continuous line). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining classifiers via majority vote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the short introduction to ensemble learning in the previous section, let's start with a warm-up exercise and implement a simple ensemble classifier for majority voting in Python. \n",
    "\n",
    "Although the majority voting algorithm that we will discuss in this section also generalizes to multi-class settings via plurarity voting, we will use the term majority voting for simplicity, as it is also often done in the literature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a simple majority vote classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm we we are going ot implement in this section will allow us to combine different classification algorithms associated with individual weights for confidence. Our goal is to build a stronger meta-classifier that balances out the individual classifiers's weaknesses on a particular dataset. \n",
    "\n",
    "To translate the concept of the weighted majority vote into Python, we can use NumPy's convenient *argmax* and *bincount* functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.argmax(np.bincount([0, 0, 1], weights=[0.2, 0.2, 0.6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we remember from the discussion on logistic regression, certain classifiers in scikit-learn can also return the probability of a predicted class label via the *predict_proba* method. Using the predicted class probabilities instead of the class labels for majority voting can be useful if the classifiers in our ensemble are well calibrated. \n",
    "\n",
    "To implement the weighted majority vote based on class probabilities, we can again make use of NumPy using *numpy.average* and *np.argmax*: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.58, 0.42])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = np.array([[0.9, 0.1], \n",
    "               [0.8, 0.2], \n",
    "               [0.4, 0.6]])\n",
    "p = np.average(ex, axis=0, weights=[0.2, 0.2, 0.6])  \n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting everything together, let's new implement *MajorityVoteClassifier* in Python: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.externals import six\n",
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import _name_estimators\n",
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "class MajorityVoteClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\" A majority vote ensemble classifier\n",
    "    \n",
    "    Parameters\n",
    "    --------------\n",
    "    classifiers : array-like, shape = [n_classifiers]\n",
    "        Different classifiers for the ensemble\n",
    "    \n",
    "    vote : str, {'classlabel', 'probability'}\n",
    "        Default: 'classlabel'\n",
    "        If 'classlabel' the prediction is based on \n",
    "        the argmax of class labels. Else if \n",
    "        'probability', the argmax of the sum of the \n",
    "        probabilities is used to predict the class label \n",
    "        (recommended for calibrated classifiers). \n",
    "        \n",
    "    weights : array-like, shape = [n_classifiers]\n",
    "        Optional, default: None\n",
    "        If a list of 'int' or 'float' values are provided, \n",
    "        the classifiers are weighted by importance; \n",
    "        Uses uniform weights if 'weights=None'. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, classifiers, vote='classlabel', \n",
    "                 weights=None):\n",
    "        self.classifiers = classifiers\n",
    "        self.named_classifiers = {key: value for key, value in \n",
    "                                  _name_estimators(classifiers)}\n",
    "        self.vote = vote\n",
    "        self.weights = weights\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\" Fit classifiers.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        X : {array-like, sparse matrix}, \n",
    "            shape = [n_samples, n_features]\n",
    "            Matrix of training samples. \n",
    "        y : array-like, shape = [n_samples]\n",
    "            Vector of target class labels.\n",
    "            \n",
    "        Returns\n",
    "        --------------\n",
    "        self : object \n",
    "        \"\"\"\n",
    "        \n",
    "        # Use LabelEncoder to ensure class labels start\n",
    "        # with 0, which is important for np.argmax\n",
    "        # call in self.predict\n",
    "        self.lablenc__ = LabelEncoder()\n",
    "        self.lablenc_.fit(y)\n",
    "        self.classes_ = self.lablenc_.classes_\n",
    "        self.classifiers_ = []\n",
    "        for clf in self.classifiers:\n",
    "            fitted_clf = clone(clf).fit(X, self.lablenc_.transform(y))\n",
    "            self.classifiers_.append(fitted_clf)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\" Predict class labels for X.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        X : {array-like, sparse matrix}, \n",
    "            Shape = [n_samples, n_features]\n",
    "            Matrix of training samples. \n",
    "            \n",
    "        Returns\n",
    "        --------------\n",
    "        maj_vote : array-like, shape = [n_samples]\n",
    "            Predicted class labels.\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.vote == 'probability':\n",
    "            maj_vote= np.argmax(self.predict_proba(X), axis=1)\n",
    "        else: # 'classlabel' vote\n",
    "            # Collect results from clf.predict calls\n",
    "            predictions = np.asarray([clf.predict(X) \n",
    "                                      for clf in \n",
    "                                      self.classifiers_]).T\n",
    "            maj_vote = np.apply_along_axis(lambda x:\n",
    "                                           np.argmax(np.bincount(x), \n",
    "                                                     weights=self.weights), \n",
    "                                           axis=1, arr=predictions)\n",
    "        maj_vote = self.lablenc_.inverse_transform(maj_vote)\n",
    "        return maj_vote\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\" Predict class probabilities for X.\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        X : {array-like, sparse matrix}, \n",
    "            shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples is \n",
    "            the number of samples and n_features is the \n",
    "            number of features. \n",
    "\n",
    "        Returns\n",
    "        --------------\n",
    "        avg_proba : array-like, \n",
    "            shape = [n_samples, n_classes]\n",
    "            Weighted average probability for \n",
    "            each class per sample. \n",
    "        \"\"\"\n",
    "        \n",
    "        probas = np.array([clf.predict_proba(X)\n",
    "                           for clf in self.classifiers_])\n",
    "        avg_proba = np.average(probas, axis=0, weights=self.weights)\n",
    "        return avg_proba\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        \"\"\" Get classifier parameter names for GridSearch\n",
    "        \"\"\"\n",
    "        \n",
    "        if not deep:\n",
    "            return super(MajorityVoteClassifier, self).get_params(deep=False)\n",
    "        else:\n",
    "            out = self.named_classifiers.copy()\n",
    "            for name, step in six.iteritems(self.named_classifiers):\n",
    "                for key, value in six.iteritems(step.get_params(deep=True)):\n",
    "                    out['%s__%s' % (name, key)] = value\n",
    "            return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, note that we defined our own modified version of the *get_params* method to use the *_name_estimators* function to access the paramters of individual classifiers in the ensemble; this may look a little bit complicated at first, but it will make perfect sense when we use grid search for hyperparameter tuning in later sections. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the majority voting principle to make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is about time to put the *MajorityVoteClassifier* that we implemented into action. But first, let's prepare a dataset that we can test it on. Since we are already familiar with techniques to load datasets from CSV files, we will take a shortcut and load the Iris dataset from scikit-learn's dataset module. Furthermore, we will only select two features, **sepal width** and **petal length**, to make the classification task more challenging for illustration purposes. Although our *MajorityVoteClassifier* generalizes to multiclass problems, we will only classify flower samples from the *Iris-versicolor* and *Iris-virginica* classes, with which we will compute the ROC AUC later. The code is as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data[50:, [1, 2]], iris.target[50:]\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we split the Iris samples into 50 percent training and 50 percent test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.5, \n",
    "                     random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the training dataset, we now will train three different classifiers:\n",
    "* Logistic regression classifier\n",
    "* Decision tree classifier\n",
    "* k-nearest neighbors classifier\n",
    "\n",
    "We then evaluate the model performance of each classifier via 10-fold cross-validation on the training dataset before we combine them into an ensemble classifier: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold cross validation:\n",
      "\n",
      "ROC AUC: 0.87 (+/- 0.17) [Logistic regression]\n",
      "ROC AUC: 0.89 (+/- 0.16) [Decision tree]\n",
      "ROC AUC: 0.88 (+/- 0.15) [KNN]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "clf1 = LogisticRegression(penalty='l2', C=0.001, random_state=1)\n",
    "clf2 = DecisionTreeClassifier(max_depth=1, criterion='entropy', random_state=0)\n",
    "clf3 = KNeighborsClassifier(n_neighbors=1, p=2, metric='minkowski')\n",
    "\n",
    "pipe1 = Pipeline([['sc', StandardScaler()], \n",
    "                  ['clf', clf1]])\n",
    "pipe3 = Pipeline([['sc', StandardScaler()], \n",
    "                  ['clf', clf3]])\n",
    "clf_labels = ['Logistic regression', 'Decision tree', 'KNN']\n",
    "print('10-fold cross validation:\\n')\n",
    "for clf, label in zip([pipe1, clf2, pipe3] , clf_labels):\n",
    "    scores = cross_val_score(estimator=clf, X=X_train, \n",
    "                             y=y_train, cv=10, scoring='roc_auc')\n",
    "    print('ROC AUC: %0.2f (+/- %0.2f) [%s]'\n",
    "          % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's move on to the more exciting part and combine the individual classifiers for majority rule voting in our *MajorityVoteClassifier*: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_clf = MajorityVoteClassifier(classifiers=[pipe1, clf2, pipe3])\n",
    "clf_labels += ['Majority voting']\n",
    "all_clf = [pipe1, clf2, pipe3, mv_clf]\n",
    "for clf, label in zip(all_clf, clf_labels):\n",
    "    scores = cross_val_score(estimator=clf, \n",
    "                             X=X_train, y=y_train, \n",
    "                             cv=10, scoring='roc_auc')\n",
    "    print('Accuracy: %0.2f (+/- %0.2f) [%s]'\n",
    "          %)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
