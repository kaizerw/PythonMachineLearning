
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{3\_A\_Tour\_Of\_Machine\_Learning\_Classifiers\_Using\_Scikit\_Learn}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \section{A Tour of Machine Learning Classifiers Using
scikit-learn}\label{a-tour-of-machine-learning-classifiers-using-scikit-learn}

    In this chapter, we will take a tour through a selection of popular and
powerful machine learning algorithms that are commonly used in academia
as well as in industry. While learning about the differences between
several supervised learning algorithms for classification, we will also
develop an intuitive appreciation of their individual strengths and
weaknesses. In addition, we will take our first step with the
scikit-learn library, which offers a user-friedly interface for using
those algorithms efficiently and productively.

The topics that we will learn about throughout this chapter are as
follows: * Introduction to robust and popular algorithms for
classification, such as logistic regression, support vector machines,
and decision trees * Examples and explanations using the scikit-learn
machine learning library, which provides a wide variety of machine
learning algorithms via a user-friedly Python API * Discussion about the
strengths and weaknesses of classifiers with linear and non-linear
decision boundaries

    \section{Choosing a classification
algorithm}\label{choosing-a-classification-algorithm}

    Choosing an appropriate classification algorithm for a particular
problem task requires practice; each algorithm has it own quirks and is
based on certain assumptions. To restate the No Free Lunch theorem by
David H. Wolpert, no single classifier works best across all posible
scenarios. In practice, it is always recommended that you compare the
performance of at least a handful of different learning algorithms to
select the best model for the particular problem; these may differ in
the number of features or samples, the amount of noise in the dataset,
and whether the class is linearly separable or not.

    Eventually, the performance of a classifier; computational performance
as well as predictive power; depends heavily on the underlying data that
is available for learning. The five main steps that are involved in
training a machine learning algorithm can be summarized as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Selecting features and collectiong training samples.
\item
  Choosing a performance metric.
\item
  Choosing a classifier and optimization algorithm.
\item
  Evaluating the performance of the model.
\item
  Tuning the algorithm.
\end{enumerate}

Since the approach of this book is to build machine learning knowledge
step by step, we will mainly focus on the main concepts of the different
algorithms in this chapter and revisit topics such as feature selection
and preprocessing, performance metrics, and hyperparameter tuning for
more detailed discussions later in this book.

    \section{First steps with scikit-learn: Training a
perceptron}\label{first-steps-with-scikit-learn-training-a-perceptron}

    In previous chapter, you learned about two related learning algorithms
for classification, the perceptron rule and Adaline, which we
implemented in Python by ourselves. Now we will take a look at the
scikit-learn API, which combines a user-friendly interface with a highly
optimized implementation of several classification algorithms. The
scikit-learn library offers not only a large variety of learning
algorithms, but also many convenient functions to preprocess data and to
fine-tune and evaluate our models.

To get started with the scikit-learn library, we will train a perceptron
model similar to the one that we implemented previously. For simplicity,
we will use the already familiar Iris dataset throughout the following
sections. Conveniently, the Iris dataset is already available via
scikit-learn, since it is a simple yet popular dataset that is
frequently used for testing and experimenting with algorithms. We will
only use two features from the Iris dataset for visualization purposes.

We will assign the petal length and petal width of the 150 flower
samples to the feature matrix \emph{X} and the corresponding class
labels of the flower species to the vector \emph{y}:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn} \PY{k}{import} \PY{n}{datasets}
        \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        
        \PY{n}{iris} \PY{o}{=} \PY{n}{datasets}\PY{o}{.}\PY{n}{load\PYZus{}iris}\PY{p}{(}\PY{p}{)}
        \PY{n}{X} \PY{o}{=} \PY{n}{iris}\PY{o}{.}\PY{n}{data}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{]}\PY{p}{]}
        \PY{n}{y} \PY{o}{=} \PY{n}{iris}\PY{o}{.}\PY{n}{target}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Class labels:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{unique}\PY{p}{(}\PY{n}{y}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Class labels: [0 1 2]

    \end{Verbatim}

    The \emph{np.unique(y)} function returned the three unique class labels
stored in \emph{iris.target}, and as we see, the Iris flower class
\emph{Iris-setosa}, \emph{Iris-versicolor}, and \emph{Iris-virginica}
are already stored as integers (here, 0, 1, 2). Although many
scikit-learn functions and class methods also work with class labels in
string format, using integer labels is a recommended approach to avoid
technical glitches and improve computational performance due to a
smaller memory footprint; furthermore, encoding class labels as integers
is a common convention among most machine learning libraries.

To evaluate how well a trained model performs on unseen data, we will
further split the dataset into separate training and test datasets.
Later we will discuss the best practices around model evaluation in more
detail:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{train\PYZus{}test\PYZus{}split}
        
        \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}
            \PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.3}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{stratify}\PY{o}{=}\PY{n}{y}\PY{p}{)}
\end{Verbatim}


    Using the \emph{train\_test\_split} function from scikit-learn's
\emph{model\_selection} module, we randomly split the \emph{x} and
\emph{y} arrays into 30 percent test data (45 samples) and 70 percent
training data (105 samples).

Note that the \emph{train\_test\_split} function already shuffles the
training sets internally before splitting; otherwise, all class 0 and
class 1 samples would have ended up in the training set, and the test
set would consist of 45 samples of class 2. Via the \emph{random\_state}
parameter, we provided a fixed random seed (\emph{random\_state=1}), for
the internal pseudo-random number generator that is used for shuffling
the datasets prior to splitting. Using such as fixed
\emph{random\_state} ensures that our results are reproducible.

Lastly, we took advantage of the built-in support for stratification via
\emph{stratify=y}. In this context, stratification means that the
\emph{train\_test\_split} method returns a training and test subsets
that have the same proportions of class labels as the input dataset. We
can use NumPy's \emph{bincount} function, which counts the number of
occurrences of each value in an array, to verify that this is indeed the
case:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Labels counts in y:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{bincount}\PY{p}{(}\PY{n}{y}\PY{p}{)}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Labels counts in y\PYZus{}train:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{bincount}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{)}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Labels counts in y\PYZus{}test:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{bincount}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Labels counts in y: [50 50 50]
Labels counts in y\_train: [35 35 35]
Labels counts in y\_test: [15 15 15]

    \end{Verbatim}

    Many machine learning and optimization algorithms also require feature
scaling for optimal performance, as we remember from the
\textbf{gradient descent} examples in previous chapter. Here, we will
standardize the features using the \emph{StandardScaler} class from
scikit-learn's \emph{preprocessing} module:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{preprocessing} \PY{k}{import} \PY{n}{StandardScaler}
        
        \PY{n}{sc} \PY{o}{=} \PY{n}{StandardScaler}\PY{p}{(}\PY{p}{)}
        \PY{n}{sc}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
        \PY{n}{X\PYZus{}train\PYZus{}std} \PY{o}{=} \PY{n}{sc}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
        \PY{n}{X\PYZus{}test\PYZus{}std} \PY{o}{=} \PY{n}{sc}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
\end{Verbatim}


    Using the preceding code, we loaded the \emph{StandardScaler} class from
the \emph{preprocessing} module and initialized a new
\emph{StandardScaler} object that we assigned to the \emph{sc} variable.
Using the \emph{fit} method, \emph{StandardScaler} estimated the
parameters \(\mu\) (sample mean) and \(\sigma\) (standard deviation) for
each feature dimension from the training data. By calling the
\emph{transform} method, we then stardardized the training data using
those estimated parameters \(\mu\) and \(\sigma\). Note that we used the
same scaling parameters to standardize the test set so that both the
values in the training and test dataset are comparable to each other.

Having stardardized the training data, we can now train a perceptron
model. Most algorithms in scikit-learn already support multiclass
classification by default via \emph{One-versus-Rest (OvR)} method, which
allows us to feed the three flower classes to the perceptron all at
once. The code is as follows:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{linear\PYZus{}model} \PY{k}{import} \PY{n}{Perceptron}
        
        \PY{n}{ppn} \PY{o}{=} \PY{n}{Perceptron}\PY{p}{(}\PY{n}{n\PYZus{}iter}\PY{o}{=}\PY{l+m+mi}{40}\PY{p}{,} \PY{n}{eta0}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
        \PY{n}{ppn}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}std}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
/usr/lib/python3.6/site-packages/sklearn/linear\_model/stochastic\_gradient.py:117: DeprecationWarning: n\_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max\_iter and tol instead.
  DeprecationWarning)

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}5}]:} Perceptron(alpha=0.0001, class\_weight=None, eta0=0.1, fit\_intercept=True,
              max\_iter=None, n\_iter=40, n\_jobs=1, penalty=None, random\_state=1,
              shuffle=True, tol=None, verbose=0, warm\_start=False)
\end{Verbatim}
            
    The scikit-learn interface remind us of our perceptron implementation in
previous chapter: after loading the \emph{Perceptron} class from the
\emph{linear\_model} module, we initialized a new \emph{Perceptron}
object and trained the model via the \emph{fit} method. Here, the model
parameter \emph{eta0} is equivalent to the learning rate \emph{eta} that
we used in our own perceptron implementation, and the \emph{n\_iter}
parameter defines the number of epochs (passes over the training set).

As we remember from previous chapter, findind and appropriate learning
rate requires some experimentation. If the learning rate is too large,
the algorithm will overshoot the global cost minimum. If the learning
rate is too small, the algorithm requires more epochs until convergence,
which can make the learning slow, specially for large datasets. Also, we
used the \emph{random\_state} parameter to ensure the reproducibility of
the initial shuffling of the training dataset after each epoch.

Having trained a model in \emph{scikit-learn}, we can make predictions
via the \emph{predict} method, just like in our own perceptron
implementation. The code is as follows:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{ppn}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}std}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Misclassified samples: }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{y\PYZus{}test} \PY{o}{!=} \PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Misclassified samples: 3

    \end{Verbatim}

    Executing the code, we see that the perceptron misclassifies three out
of the 45 flower samples. Thus, the misclassification error on the
dataset is approximately \emph{0.067} or \emph{6.7} percent
(\(3/45 \approx 0.067\)).

    Instead of the misclassification \textbf{error}, many machine learning
practitioners report the classification \textbf{accuracy} of a model,
which is simply calculated as follows:

\[1-error = 0.933\ \text{or 93.3 percent}\]

The scikit-learn library also implements a large variety of different
perfomance metrics that are available via the \emph{metrics} module. For
example, we can calculate the classification accuracy of the perceptron
on the test set as follows:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{accuracy\PYZus{}score}
        
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy: }\PY{l+s+si}{\PYZpc{}.2f}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Accuracy: 0.93

    \end{Verbatim}

    Here, \emph{y\_test} are the true class labels and \emph{y\_pred} are
the class labels that we predicted previously. Alternatively, each
classifier in scikit-learn has a \emph{score} method, which computes a
classifier's prediction accuracy by combining the predict class with
\emph{accuracy\_score} method as shown here:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy: }\PY{l+s+si}{\PYZpc{}.2f}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n}{ppn}\PY{o}{.}\PY{n}{score}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}std}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Accuracy: 0.93

    \end{Verbatim}

    Note that we evaluate the performance of our models based on the test
set in this chapter. Later you will learn about useful techniques,
including graphical analysis such as learning curves, to detect and
prevend \textbf{overfitting}. Overffiting means that the model captures
the patters in the training data well, but fails to generalize well to
unseen data.

    Finally, we can use our \emph{plot\_decision\_regions} function to plot
the \textbf{decision regions} of our newly trained perceptron model and
visualize how well it separates the different flower samples. However,
let's add a small modification to highlight the samples from the tes
dataset via small circles:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{k+kn}{from} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{colors} \PY{k}{import} \PY{n}{ListedColormap}
        \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
        
        \PY{k}{def} \PY{n+nf}{plot\PYZus{}decision\PYZus{}regions}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{classifier}\PY{p}{,} \PY{n}{test\PYZus{}idx}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,} 
                                  \PY{n}{resolution}\PY{o}{=}\PY{l+m+mf}{0.02}\PY{p}{)}\PY{p}{:}
            \PY{c+c1}{\PYZsh{}setup marker generator and color map}
            \PY{n}{markers} \PY{o}{=} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{s}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{x}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZca{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{v}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            \PY{n}{colors} \PY{o}{=} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{red}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{blue}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{lightgreen}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gray}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cyan}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            \PY{n}{cmap} \PY{o}{=} \PY{n}{ListedColormap}\PY{p}{(}\PY{n}{colors}\PY{p}{[}\PY{p}{:}\PY{n+nb}{len}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{unique}\PY{p}{(}\PY{n}{y}\PY{p}{)}\PY{p}{)}\PY{p}{]}\PY{p}{)}
            
            \PY{c+c1}{\PYZsh{} plot the decision surface}
            \PY{n}{x1\PYZus{}min}\PY{p}{,} \PY{n}{x1\PYZus{}max} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{p}{)} \PY{o}{+} \PY{l+m+mi}{1}
            \PY{n}{x2\PYZus{}min}\PY{p}{,} \PY{n}{x2\PYZus{}max} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{p}{)} \PY{o}{+} \PY{l+m+mi}{1}
            \PY{n}{xx1}\PY{p}{,} \PY{n}{xx2} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{meshgrid}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n}{x1\PYZus{}min}\PY{p}{,} \PY{n}{x1\PYZus{}max}\PY{p}{,} \PY{n}{resolution}\PY{p}{)}\PY{p}{,} 
                                   \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n}{x2\PYZus{}min}\PY{p}{,} \PY{n}{x2\PYZus{}max}\PY{p}{,} \PY{n}{resolution}\PY{p}{)}\PY{p}{)}
            \PY{n}{Z} \PY{o}{=} \PY{n}{classifier}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{n}{xx1}\PY{o}{.}\PY{n}{ravel}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{xx2}\PY{o}{.}\PY{n}{ravel}\PY{p}{(}\PY{p}{)}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{T}\PY{p}{)}
            \PY{n}{Z} \PY{o}{=} \PY{n}{Z}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{xx1}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{contourf}\PY{p}{(}\PY{n}{xx1}\PY{p}{,} \PY{n}{xx2}\PY{p}{,} \PY{n}{Z}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.3}\PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{n}{cmap}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{xlim}\PY{p}{(}\PY{n}{xx1}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{xx1}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{p}{)}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{ylim}\PY{p}{(}\PY{n}{xx2}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{xx2}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{p}{)}\PY{p}{)}
            
            \PY{k}{for} \PY{n}{idx}\PY{p}{,} \PY{n}{cl} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{unique}\PY{p}{(}\PY{n}{y}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{x}\PY{o}{=}\PY{n}{X}\PY{p}{[}\PY{n}{y}\PY{o}{==}\PY{n}{cl}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{y}\PY{o}{=}\PY{n}{X}\PY{p}{[}\PY{n}{y}\PY{o}{==}\PY{n}{cl}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} 
                            \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.8}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{n}{colors}\PY{p}{[}\PY{n}{idx}\PY{p}{]}\PY{p}{,} 
                            \PY{n}{marker}\PY{o}{=}\PY{n}{markers}\PY{p}{[}\PY{n}{idx}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{n}{cl}\PY{p}{,} 
                            \PY{n}{edgecolor}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{black}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            
            \PY{c+c1}{\PYZsh{} highlight test samples}
            \PY{k}{if} \PY{n}{test\PYZus{}idx}\PY{p}{:}
                \PY{c+c1}{\PYZsh{} plot all samples}
                \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{n}{test\PYZus{}idx}\PY{p}{,} \PY{p}{:}\PY{p}{]}\PY{p}{,} \PY{n}{y}\PY{p}{[}\PY{n}{test\PYZus{}idx}\PY{p}{]}
                \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                            \PY{n}{edgecolor}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{black}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{1.0}\PY{p}{,} 
                            \PY{n}{linewidth}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{marker}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                            \PY{n}{s}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test set}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    With the slight modification that we made to the
\emph{plot\_decision\_regions} function, we can now specify the indices
of the samples that we want to mark on the resulting plots. The code is
as follows:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{n}{X\PYZus{}combined\PYZus{}std} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{vstack}\PY{p}{(}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}std}\PY{p}{,} \PY{n}{X\PYZus{}test\PYZus{}std}\PY{p}{)}\PY{p}{)}
         \PY{n}{y\PYZus{}combined} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{hstack}\PY{p}{(}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{)}
         \PY{n}{plot\PYZus{}decision\PYZus{}regions}\PY{p}{(}\PY{n}{X}\PY{o}{=}\PY{n}{X\PYZus{}combined\PYZus{}std}\PY{p}{,} \PY{n}{y}\PY{o}{=}\PY{n}{y\PYZus{}combined}\PY{p}{,} 
                               \PY{n}{classifier}\PY{o}{=}\PY{n}{ppn}\PY{p}{,} 
                               \PY{n}{test\PYZus{}idx}\PY{o}{=}\PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{105}\PY{p}{,} \PY{l+m+mi}{150}\PY{p}{)}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{petal length [standardized]}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{petal width [standardized]}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{upper left}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_27_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Remember that the perceptron algorithm never converges on datasets that
are not perfectly linearly separable, which is why the use of the
perceptron algorithm is typically not recommended in practice. In the
following sections, we will look at more powerful linear classifiers
that converge to a cost minimum even if the classes are not perfectly
linearly separable.

    \section{Modeling class probabilities via logistic
regression}\label{modeling-class-probabilities-via-logistic-regression}

    Although the perceptron rule offers a nice and easygoing introduction to
machine learning algorithms for classification, its biggest disadvantage
is that it never converges if the class are not perfectly linearly
separable. The classification task in the previous section would be an
example of such a scenario, Intuitively, we can think of the reason as
the weights are continuously being updated since there is always at
least one misclassified sample present in each epoch. Of course, you can
change the learning rate and increase the number of epochs, but be
warned that the perceptron will never converge on this dataset. To make
better use of our time, we will now take a look at another simple yet
more powerfull algorithm for linear and binary classification problems:
\textbf{logistic regression}. Note that, in spite of its name, logistic
regression is a model for classification, not regression.

    \section{Logistic regression intuition and conditional
probabilities}\label{logistic-regression-intuition-and-conditional-probabilities}

    Logistic regression is a classification model that is very easy to
implement but performs very well on linearly separable classes. It is
one of the most widely used algorithms for classification in industry.
Similar to the perceptron and Adaline, the logistic regression model in
this chapter is also a linear model for binary classification that can
be extended to multiclass classification, for example, via the OvR
technique.

To explain the idea behind logistic regression as a probabilistic model,
let's first introduce the \textbf{odds ratio}: the odds in favor of a
particular event. The odds ratio can be written as \(\frac{p}{1-p}\)
where \(p\) stands for the probability of the positive event. The term
\emph{positive event} does not necessarily mean \emph{good}, but refers
to the event that we can want to predict, for example, the probability
that a patient has a certain disease; we can think of the positive event
as class label \(y = 1\). We can then further define the \emph{logit}
function, which is simply the logarithm of the odds ratio (log-odds):

\[\text{logit}(p) = \log\frac{p}{(1-p)}\]

Note that \emph{log} refers to the natural logarithm, as it is the
common convention in computer science. The \emph{logit} function takes
as input values in the range 0 to 1 and transforms them to values over
the entire real-number range, which we can use to express a linear
relationship between feature values and the log-odds:

\[\text{logit}(p(y = 1 | x)) = w_0x_0 + w_1x_1 + ... + w_mx_m = \sum_{i=0}^{m}w_ix_i = w^Tx\]

Here, \(p(y = 1 | x)\) is the condition probability that a particular
sample belongs to class 1 given its features \(x\).

Now, we are actually interested in predicting the probability that a
certain sample belongs to a particular class, which is the inverse form
of the \emph{logit} function. It is also called \textbf{logistic sigmoid
function}, sometimes simply abbreviated to \textbf{sigmoid function} due
to its characteristic S-shape:

\[\phi(z) = \frac{1}{1+e^{-z}}\]

Here \(z\) is the net input, the linear combination of weights and
sample features, \(z = w^Tx = w_0x_0 + w_1x_1 + ... + w_mx_m\).

Note that \(w_0\) refers to the bias unit, and is an additional input
value that we provide \(x_0\), which is set equal to 1.

    Now let us simply plot the sigmoid function for some values in the range
-7 to 7 to see how it looks:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
         \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
         
         \PY{k}{def} \PY{n+nf}{sigmoid}\PY{p}{(}\PY{n}{z}\PY{p}{)}\PY{p}{:} 
             \PY{k}{return} \PY{l+m+mf}{1.0} \PY{o}{/} \PY{p}{(}\PY{l+m+mf}{1.0} \PY{o}{+} \PY{n}{np}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{n}{z}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{z} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{7}\PY{p}{,} \PY{l+m+mi}{7}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{)}
         \PY{n}{phi\PYZus{}z} \PY{o}{=} \PY{n}{sigmoid}\PY{p}{(}\PY{n}{z}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{z}\PY{p}{,} \PY{n}{phi\PYZus{}z}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{axvline}\PY{p}{(}\PY{l+m+mf}{0.0}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{k}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylim}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mf}{1.1}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{z}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZdl{}}\PY{l+s+s1}{\PYZbs{}}\PY{l+s+s1}{phi(z)\PYZdl{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} y axis ticks and gridline}
         \PY{n}{plt}\PY{o}{.}\PY{n}{yticks}\PY{p}{(}\PY{p}{[}\PY{l+m+mf}{0.0}\PY{p}{,} \PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mf}{1.0}\PY{p}{]}\PY{p}{)}
         \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{gca}\PY{p}{(}\PY{p}{)}
         \PY{n}{ax}\PY{o}{.}\PY{n}{yaxis}\PY{o}{.}\PY{n}{grid}\PY{p}{(}\PY{k+kc}{True}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_34_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    We can see that \(\phi(z)\) approaches 1 if \(z\) goes towards infinity
(\(z \to \infty\) since \(e^{-z}\) becomes very small for large values
of z. Similarly, \(\phi(z)\) goes towards 0 for \(z \to -\infty\) as a
result of an increasingly large denominator. Thus, we conclude that this
sigmoid function takes real number values as input and transform them
into values in the range {[}0, 1{]} with an intercept at
\(\phi(z)=0.5\).

To build some intuition for the logistic regression model, we can relate
it to previous chapter. In Adaline, we used the identity function
\(\phi(z) = z\) as the activation function. In logistic regression, this
activation function simply becomes the sigmoid function that we defined
earlier. The difference between Adaline and logistic regression is
illustrated in the following figure:

    The output of the sigmoid function is then interpreted as the
probability of a particular sample belonging to class 1,
\(\phi(z) = P(y = 1 | x;w)\), given its features \(x\) parameterized by
the weights \(w\). For example, if we compute \(\phi(z) = 0.8\) for a
particular flower sample, it means that the chance that this sample is
an \emph{Iris-versicolor} flower is 80 percent. Therefore, the
probability that this flower is an \emph{Iris-setosa} flower can be
calculated as \(P(y = 0 | x;w) = 1 - P(y = 1 | x;w) = 0.2\) or 20
percent. The predicted probability can then simply be converted into a
binary outcome via a threshold function:

\begin{equation}
ŷ = 
\begin{cases}
   1, & \text{if}\ \phi(z) \ge 0.5 \\
   0, & \text{otherwise}
\end{cases}
\end{equation}

If we look at the preceding plot of the sigmoid function, this is
equivalent to the following:

\begin{equation}
ŷ = 
\begin{cases}
   1, & \text{if}\ z \ge 0.0 \\
   0, & \text{otherwise}
\end{cases}
\end{equation}

In fact, there are many applications where we are not only interested in
the predicted class labels, but where the estimation of the
class-memberships probability is particularly useful (the output of the
sigmoid function prior to applying the threshold function). Logistic
regression is used in weather forecasting, for example, not only to
predict if it will rain on a particular day but also to report the
chance of rain. Similarly, logistic regression can be used to predict
the chance that a patient has a particular disease given certain
symptoms, which is why logistic regression enjoys great popularity in
the field of medicine.

    \section{Learning the weights of the logistic cost
function}\label{learning-the-weights-of-the-logistic-cost-function}

    You learned how we could use the logistic regression model to predict
probabilities and class labels; now, let us briefly talk about how we
fit the parameters of the model, for instance the weights \(w\). In the
previous chapter, we defined the sum-squared-error cost function as
follows:

\[J(w) = \sum_i \frac{1}{2}(\phi(z^{(i)}) - y^{(i)})^2\]

We minimized this function in order to learn the weights \(w\) for our
Adaline classification model. To explain how we can derive the cost
function for logistic regression, let's first define the likelihood
\(L\) that we want to maximize when we build a logistic regression
model, assuming that the indiviual samples in our dataset are
independent of one another. The formula is as follows:

\[L(w) = P(y|x;w) = \prod_{i = 1}^n P(y^{(i)}|x^{(i)};w) = \prod_{i = 1}^n (\phi(z^{(i)}))^{y^{(i)}} (1 - \phi(z^{(i)}))^{1-y^{(i)}}\]

In practice, it is easir to maximize the (natural) log of this question,
which is called the log-likelihood function:

\[l(w) = \log L(w) = \sum_{i = 1}^n [y^{(i)} \log(\phi(z^{(i)})) + (1 - y^{(i)}) \log(1 - \phi(z^{(t)}))]\]

Firstly, applying the log function reduces the potential for numerical
overflow, which can occur if the likelihoods are very small. Secondly,
we can convert the product of fators into a summation of factors, which
makes it easier to obtan the derivative of this function via the
addiction trick, as you may remember from calculus.

Now we could use an optimization algorithm such as gradient ascent to
maximize this log-likelihood function. Alternatively, let's rewrite the
log-likelihood as a cost function \(J\) that can be minimized usind
gradient descent:

\[J(w) = \sum_{i = 1}^n [-y^{(i)} \log(\phi(z^{(i)})) - (1 - y^{(i)}) \log(1 - \phi(z^{(t)}))]\]

To get a better grasp of this cost function, let us take a look at the
cost that we want to calculate for one single-sample training instance:

\[J(\phi(z), y;w) = -y \log(\phi(z)) - (1 - y) \log(1 - \phi(z))\]

Looking at the equation, we can see that the first term becomes zero if
\(y = 0\), and the second term becomes zero if \(y = 1\):

\begin{equation}
J(\phi(z), y;w) = 
\begin{cases}
   -log(\phi(z)), & \text{if}\ y = 1 \\
   -log(1 - \phi(z)), & \text{if}\ y = 0 
\end{cases}
\end{equation}

Let's write a short code snippet to create a plot that illustrates the
cost of classifying a single-sample instance for different values of
\(\phi(z)\):

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{k}{def} \PY{n+nf}{cost\PYZus{}1}\PY{p}{(}\PY{n}{z}\PY{p}{)}\PY{p}{:}
             \PY{k}{return} \PY{o}{\PYZhy{}} \PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{sigmoid}\PY{p}{(}\PY{n}{z}\PY{p}{)}\PY{p}{)}
         
         \PY{k}{def} \PY{n+nf}{cost\PYZus{}0}\PY{p}{(}\PY{n}{z}\PY{p}{)}\PY{p}{:}
             \PY{k}{return} \PY{o}{\PYZhy{}} \PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{sigmoid}\PY{p}{(}\PY{n}{z}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{z} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{)}
         \PY{n}{phi\PYZus{}z} \PY{o}{=} \PY{n}{sigmoid}\PY{p}{(}\PY{n}{z}\PY{p}{)}
         \PY{n}{c1} \PY{o}{=} \PY{p}{[}\PY{n}{cost\PYZus{}1}\PY{p}{(}\PY{n}{x}\PY{p}{)} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n}{z}\PY{p}{]}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{phi\PYZus{}z}\PY{p}{,} \PY{n}{c1}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{J(w) if y=1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{c0} \PY{o}{=} \PY{p}{[}\PY{n}{cost\PYZus{}0}\PY{p}{(}\PY{n}{x}\PY{p}{)} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n}{z}\PY{p}{]}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{phi\PYZus{}z}\PY{p}{,} \PY{n}{c0}\PY{p}{,} \PY{n}{linestyle}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{J(w) if y=0}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylim}\PY{p}{(}\PY{l+m+mf}{0.0}\PY{p}{,} \PY{l+m+mf}{5.1}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlim}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZdl{}}\PY{l+s+s1}{\PYZbs{}}\PY{l+s+s1}{phi\PYZdl{}(z)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{J(w)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{best}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_39_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    We can see that the cost approaches 0 (continuous line) if we correctly
predict that a sample belongs to class 1. Similarly, we can see on the
\(y\)-axis that the cost also approaches 0 if we correctly predict
\(y=0\) (dashed line. However, if the prediction is wrong, the cost goes
towards infinity. The main point is that we penalize wrong predictions
with an increasingly larger cost.

    \section{Converting an Adaline implementation into an algorithm for
logistic
regression}\label{converting-an-adaline-implementation-into-an-algorithm-for-logistic-regression}

    If we were to implement logistic regression ourselves, we could simply
substitute the cost function \(J\) in our Adaline implementation with
the new cost function:

\[J(w) = \sum_{i = 1}^n [-y^{(i)} \log(\phi(z^{(i)})) - (1 - y^{(i)}) \log(1 - \phi(z^{(t)}))]\]

We use this to compute the cost of classifying all training samples per
epoch. Also, we need to swap the linear activation function with the
sigmoid activation and change the threshold function to return class
labels 0 and 1 instead of -1 and 1. If we make those three changes to
the Adaline code, we would end up with a working logistic regression
implementation, as shown here:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{k}{class} \PY{n+nc}{LogisticRegressionGD}\PY{p}{(}\PY{n+nb}{object}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Logistic Regression Classifier using gradient descent.}
         
         \PY{l+s+sd}{    Parameters}
         \PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
         \PY{l+s+sd}{    eta : float}
         \PY{l+s+sd}{      Learning rate (between 0.0 and 1.0)}
         \PY{l+s+sd}{    n\PYZus{}iter : int}
         \PY{l+s+sd}{      Passes over the training dataset.}
         \PY{l+s+sd}{    random\PYZus{}state : int}
         \PY{l+s+sd}{      Random number generator seed for random weight}
         \PY{l+s+sd}{      initialization.}
         
         
         \PY{l+s+sd}{    Attributes}
         \PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
         \PY{l+s+sd}{    w\PYZus{} : 1d\PYZhy{}array}
         \PY{l+s+sd}{      Weights after fitting.}
         \PY{l+s+sd}{    cost\PYZus{} : list}
         \PY{l+s+sd}{      Sum\PYZhy{}of\PYZhy{}squares cost function value in each epoch.}
         
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{k}{def} \PY{n+nf}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{eta}\PY{o}{=}\PY{l+m+mf}{0.05}\PY{p}{,} \PY{n}{n\PYZus{}iter}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{eta} \PY{o}{=} \PY{n}{eta}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}iter} \PY{o}{=} \PY{n}{n\PYZus{}iter}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{random\PYZus{}state} \PY{o}{=} \PY{n}{random\PYZus{}state}
         
             \PY{k}{def} \PY{n+nf}{fit}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{)}\PY{p}{:}
                 \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{} Fit training data.}
         
         \PY{l+s+sd}{        Parameters}
         \PY{l+s+sd}{        \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
         \PY{l+s+sd}{        X : \PYZob{}array\PYZhy{}like\PYZcb{}, shape = [n\PYZus{}samples, n\PYZus{}features]}
         \PY{l+s+sd}{          Training vectors, where n\PYZus{}samples is the number of samples and}
         \PY{l+s+sd}{          n\PYZus{}features is the number of features.}
         \PY{l+s+sd}{        y : array\PYZhy{}like, shape = [n\PYZus{}samples]}
         \PY{l+s+sd}{          Target values.}
         
         \PY{l+s+sd}{        Returns}
         \PY{l+s+sd}{        \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
         \PY{l+s+sd}{        self : object}
         
         \PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
                 \PY{n}{rgen} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{RandomState}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{random\PYZus{}state}\PY{p}{)}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{w\PYZus{}} \PY{o}{=} \PY{n}{rgen}\PY{o}{.}\PY{n}{normal}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+m+mf}{0.0}\PY{p}{,} \PY{n}{scale}\PY{o}{=}\PY{l+m+mf}{0.01}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{l+m+mi}{1} \PY{o}{+} \PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{cost\PYZus{}} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         
                 \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}iter}\PY{p}{)}\PY{p}{:}
                     \PY{n}{net\PYZus{}input} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{net\PYZus{}input}\PY{p}{(}\PY{n}{X}\PY{p}{)}
                     \PY{n}{output} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{activation}\PY{p}{(}\PY{n}{net\PYZus{}input}\PY{p}{)}
                     \PY{n}{errors} \PY{o}{=} \PY{p}{(}\PY{n}{y} \PY{o}{\PYZhy{}} \PY{n}{output}\PY{p}{)}
                     \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{w\PYZus{}}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]} \PY{o}{+}\PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{eta} \PY{o}{*} \PY{n}{X}\PY{o}{.}\PY{n}{T}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{errors}\PY{p}{)}
                     \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{w\PYZus{}}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{+}\PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{eta} \PY{o}{*} \PY{n}{errors}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}
                     
                     \PY{c+c1}{\PYZsh{} note that we compute the logistic cost now}
                     \PY{c+c1}{\PYZsh{} instead of the sum of squared errors cost}
                     \PY{n}{cost} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{n}{y}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{output}\PY{p}{)}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{p}{(}\PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{y}\PY{p}{)}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{output}\PY{p}{)}\PY{p}{)}\PY{p}{)}
                     \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{cost\PYZus{}}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{cost}\PY{p}{)}
                 \PY{k}{return} \PY{n+nb+bp}{self}
             
             \PY{k}{def} \PY{n+nf}{net\PYZus{}input}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{X}\PY{p}{)}\PY{p}{:}
                 \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Calculate net input\PYZdq{}\PYZdq{}\PYZdq{}}
                 \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{w\PYZus{}}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)} \PY{o}{+} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{w\PYZus{}}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
         
             \PY{k}{def} \PY{n+nf}{activation}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{z}\PY{p}{)}\PY{p}{:}
                 \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Compute logistic sigmoid activation\PYZdq{}\PYZdq{}\PYZdq{}}
                 \PY{k}{return} \PY{l+m+mf}{1.} \PY{o}{/} \PY{p}{(}\PY{l+m+mf}{1.} \PY{o}{+} \PY{n}{np}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{n}{np}\PY{o}{.}\PY{n}{clip}\PY{p}{(}\PY{n}{z}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{250}\PY{p}{,} \PY{l+m+mi}{250}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         
             \PY{k}{def} \PY{n+nf}{predict}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{X}\PY{p}{)}\PY{p}{:}
                 \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Return class label after unit step\PYZdq{}\PYZdq{}\PYZdq{}}
                 \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{where}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{net\PYZus{}input}\PY{p}{(}\PY{n}{X}\PY{p}{)} \PY{o}{\PYZgt{}}\PY{o}{=} \PY{l+m+mf}{0.0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{)}
                 \PY{c+c1}{\PYZsh{} equivalent to:}
                 \PY{c+c1}{\PYZsh{} return np.where(self.activation(self.net\PYZus{}input(X)) \PYZgt{}= 0.5, 1, 0)}
\end{Verbatim}


    When we fit a logistic regression model, we have to keep in mind that it
only work for binary classification tasks. So, let us consider only
\emph{Iris-setosa} and \emph{Iris-versicolor} flowers (classes 0 and 1)
and check that our implementation of logistic regression works:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{n}{X\PYZus{}train\PYZus{}01\PYZus{}subset} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{p}{[}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{o}{==}\PY{l+m+mi}{0}\PY{p}{)} \PY{o}{|} \PY{p}{(}\PY{n}{y\PYZus{}train}\PY{o}{==}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{]}
         \PY{n}{y\PYZus{}train\PYZus{}01\PYZus{}subset} \PY{o}{=} \PY{n}{y\PYZus{}train}\PY{p}{[}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{o}{==}\PY{l+m+mi}{0}\PY{p}{)} \PY{o}{|} \PY{p}{(}\PY{n}{y\PYZus{}train}\PY{o}{==}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{]}
         \PY{n}{lrgd} \PY{o}{=} \PY{n}{LogisticRegressionGD}\PY{p}{(}\PY{n}{eta}\PY{o}{=}\PY{l+m+mf}{0.05}\PY{p}{,} \PY{n}{n\PYZus{}iter}\PY{o}{=}\PY{l+m+mi}{1000}\PY{p}{,} 
                                     \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
         \PY{n}{lrgd}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}01\PYZus{}subset}\PY{p}{,} \PY{n}{y\PYZus{}train\PYZus{}01\PYZus{}subset}\PY{p}{)}
         \PY{n}{plot\PYZus{}decision\PYZus{}regions}\PY{p}{(}\PY{n}{X}\PY{o}{=}\PY{n}{X\PYZus{}train\PYZus{}01\PYZus{}subset}\PY{p}{,} \PY{n}{y}\PY{o}{=}\PY{n}{y\PYZus{}train\PYZus{}01\PYZus{}subset}\PY{p}{,} 
                               \PY{n}{classifier}\PY{o}{=}\PY{n}{lrgd}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylim}\PY{p}{(}\PY{l+m+mf}{0.0}\PY{p}{,} \PY{l+m+mf}{2.6}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{petal length [standardized]}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{petal width [standardized]}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{upper left}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_45_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \section{Training a logistic regression model with
scikit-learn}\label{training-a-logistic-regression-model-with-scikit-learn}

    We just went through useful coding and math exercises in the previous
subsection, which helped illustrate the conceptual differences between
Adaline and logistic regression. Now, let's learn how to use
scikit-learn's more optimized implementation of logistic regression that
also supports multi-class settings off the shelf (OvR by default). In
the following code example, we will use the
\emph{sklearn.linear\_model.LogisticRegression} class as well as the
familiar \emph{fit} method to train the model on all three classes in
the stardardized flower training dataset:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{linear\PYZus{}model} \PY{k}{import} \PY{n}{LogisticRegression}
         
         \PY{n}{lr} \PY{o}{=} \PY{n}{LogisticRegression}\PY{p}{(}\PY{n}{C}\PY{o}{=}\PY{l+m+mf}{100.0}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
         \PY{n}{lr}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}std}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
         \PY{n}{plot\PYZus{}decision\PYZus{}regions}\PY{p}{(}\PY{n}{X\PYZus{}combined\PYZus{}std}\PY{p}{,} \PY{n}{y\PYZus{}combined}\PY{p}{,} 
                               \PY{n}{classifier}\PY{o}{=}\PY{n}{lr}\PY{p}{,} 
                               \PY{n}{test\PYZus{}idx}\PY{o}{=}\PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{105}\PY{p}{,} \PY{l+m+mi}{150}\PY{p}{)}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{petal length [stardardized]}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{petal width [stardardized]}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{upper left}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_48_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Looking at the preceding code that we used to train the
\emph{LogisticRegression} model, you might now be wondering, "What is
the mysterious parameter \emph{c}?" We will discuss this parameter in
the next subsection, where we first introduce the concepts of
overfitting and regularization. However, before we are moving on to
those topics, let's finish our discussion of class-membership
probabilities.

The probability that training samples belong to a certain class can be
computed using the \emph{predict\_proba} method. For example, we can
predict the probabilities of the first three samples in the test set as
follows:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}16}]:} \PY{n}{lr}\PY{o}{.}\PY{n}{predict\PYZus{}proba}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}std}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{3}\PY{p}{,} \PY{p}{:}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}16}]:} array([[3.20136878e-08, 1.46953648e-01, 8.53046320e-01],
                [8.34428069e-01, 1.65571931e-01, 4.57896429e-12],
                [8.49182775e-01, 1.50817225e-01, 4.65678779e-13]])
\end{Verbatim}
            
    The first row corresponds to the class-membership probabilities of the
first flower, the second row corresponds to the class-membership
probabilities of the third flower, and so forth. Notice that the columns
sum all up to one, as expected. The highest value in the first row is
approximately 0.853, which means that the first sample belongs to class
three (\emph{Iris-virginica}) with a predicted probability of 85.7
percent. So, as you may have already notice, we can get the predicted
class labels by identifying the largest column in each row, for example,
using NumPy's \emph{argmax} function:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}17}]:} \PY{n}{lr}\PY{o}{.}\PY{n}{predict\PYZus{}proba}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}std}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{3}\PY{p}{,} \PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}17}]:} array([2, 0, 0])
\end{Verbatim}
            
    The class labels we obtained from the preceding conditional
probabilities is, of course, just a manual approach to calling the
\emph{predict} method directly, which we can quickly verify as follows:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}18}]:} \PY{n}{lr}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}std}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{3}\PY{p}{,} \PY{p}{:}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}18}]:} array([2, 0, 0])
\end{Verbatim}
            
    Lastly, a word of caution if you want to predict the class label of a
single flower sample: scikit-learn expects a two-dimensional array as
data input; thus, we have to convert a single row slice into such a
format first. One way to convert a single row entry into a
two-dimensional data array is to use NumPy's \emph{reshape} method to
add a new dimension, as demonstrated here:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} \PY{n}{lr}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}std}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{p}{:}\PY{p}{]}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}19}]:} array([2])
\end{Verbatim}
            
    \section{Tackling overfitting via
regularization}\label{tackling-overfitting-via-regularization}

    \textbf{Overfitting} is a common problem in machine learning, where
\textbf{a model performs well on training data but does not generalize
well to unseen data (test data)}. If a model suffers from overfitting,
we also that \textbf{the model has high variance}, which can be caused
by having \textbf{too many parameters} that lead to a model that is
\textbf{too complex} given the underlying data. Similarly, our model can
also suffer from \textbf{underfitting} (\textbf{high bias}), which means
that our model is not complex enough to capture the pattern in the
training data well and therefore also suffers from low performance on
unseen data.

Although we have only encountered linear models for classification so
far, the problem of overfitting and underfitting can be best illustrated
by comparing a linear decision boundary to more complex, nonlinear
decision boundaries as shown in the following figure:

    Variance measures the consistency (or variability) of the model
prediction for a particular sample instance if we were to retrain the
model multiple times, for example, on different subsets of the training
dataset. We can say that the model is sensitive to the randomness in the
training data. In contrast, bias measures how far off the predictions
are from the correct values in general if we rebuild the model multiple
times on different training datasets; bias is the measure of the
systematic error that is not due to randomness.

    One way of findind a good bias-variance tradeoff is to tune the
complexity of the model via regularization. \textbf{Regularization} is a
very useful method to handle collinearity (high correlation among
features), filter out noise from data, and eventually prevent
overfitting. The concept behind regularization is to introduce
additional information (bias) to penalize extreme parameter (weight)
values. The most common form of regularization is so-called L2
regularization (sometimes also called L2 shrinkage or weight decay),
which can be written as follows:

\[\frac{\lambda}{2}\vert\vert w\vert\vert^2 = \frac{\lambda}{2}\sum_{j=1}^m w_j^2\]

Here, \(\lambda\) is the so-called \textbf{regularization parameter}.

    Regularization is another reason why feature scaling such as
standardization is important. For regularization to work property, we
need to ensure that all our features are on comparable scales.

The cost function for logistic regression can be regularized by adding a
simple regularization term, which will shrink the weights during model
training:

\[J(w) = \sum_{i = 1}^n [-y^{(i)} \log(\phi(z^{(i)})) - (1 - y^{(i)}) \log(1 - \phi(z^{(t)}))] + \frac{\lambda}{2}\vert\vert w\vert\vert^2\]

Via the regularization parameter \(\lambda\), we can then control how
well we fit the training data while keeping the weights small. By
increasing the value of \(\lambda\), we increase the regularization
strength.

The parameter \emph{C} that is implemented for the
\emph{LogisticRegression} class in scikit-learn comes from a convention
i support vector machines, which will be the topic of the next section.
The term \emph{C} is directly related to the regularization parameter
\(\lambda\), which is it inverse. Consequently, decreasing the value of
the inverse regularization parameter \emph{C} means that we are
increasing the regularization strength, which we can visualize by
plotting the L2-regularization path for the two weight coefficients:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}20}]:} \PY{n}{weights}\PY{p}{,} \PY{n}{params} \PY{o}{=} \PY{p}{[}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{p}{]}
         \PY{k}{for} \PY{n}{c} \PY{o+ow}{in} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{)}\PY{p}{:}
             \PY{n}{lr} \PY{o}{=} \PY{n}{LogisticRegression}\PY{p}{(}\PY{n}{C}\PY{o}{=}\PY{l+m+mf}{10.}\PY{o}{*}\PY{o}{*}\PY{n}{c}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
             \PY{n}{lr}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}std}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
             \PY{n}{weights}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{lr}\PY{o}{.}\PY{n}{coef\PYZus{}}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
             \PY{n}{params}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{l+m+mf}{10.}\PY{o}{*}\PY{o}{*}\PY{n}{c}\PY{p}{)}
         \PY{n}{weights} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{weights}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{params}\PY{p}{,} \PY{n}{weights}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{petal length}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{params}\PY{p}{,} \PY{n}{weights}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{linestyle}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{petal width}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{weight coefficient}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{C}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{upper left}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xscale}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{log}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_62_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    By executing the preceding code, we fitted ten logistic regression
models with different values for the inverse-regularization parameter
\emph{C}. For the purpose of illustration, we only collected the weight
coefficients of class 1 (here, the second class in the dataset,
\emph{Iris-versicolor}) versus all classifiers, remember that we are
using the OvR technique for multiclass classification.

As we can see in the resulting plot, the weight coefficients shrink if
we decrease parameter \emph{C}, that is, if we increase the
regularization strength.

    \section{Maximum margin classification with support vector
machines}\label{maximum-margin-classification-with-support-vector-machines}

    Another powerful and widely used learning algorithm is the
\textbf{Suport Vector Machine (SVM)}, which can be considered an
extension of the perceptron. Using the perceptron algorithm, we
minimized misclassification errors. However, in SVMs our optimization
objective is to maximize the margin. The margin is defined as the
distance between the separating hyperplane (decision boundary) and the
training samples that are closest to this hyperplane, which are
so-called \textbf{support vectors}. This is illustrated in the following
figure:

The rationale behind having decision boundaries with large margins is
that they tend to have a lower generalization error whereas models with
small margins are more prone to overfitting.

Although we do not want to dive much deeper into the more involved
mathematical concepts behind the maximum-margin classification, let us
briefly mention the slack variable \(\zeta\), which was introduced by
Vladimir Vapnik in 1995 and led to the so-called \textbf{soft-margin
classification}. The motivation for introducing the slack variable allow
the convergence of the optimization in the presence of
misclassifications, under appropriate cost penalization.

Via the variable \emph{C}, we can then control the penalty for
misclassification. Large values of \emph{C} correspond to large error
penalties, whereas we are less strict about misclassification errors if
we choose smaller values for \emph{C}. We can then use the \emph{C}
parameter to control the width of the margin and therefore tune the
bias-variance trade-off, as illustrated in the following figure:

    This concept is related to regularization, which we discussed in the
previous section in the context of regularized regression where
decreasing the value of \emph{C} increases the bias and lowers the
variance of the model.

Now that we have learned the basic concepts behind a linear SVM, let us
train an SVM model to classify the different flowers in our Iris
dataset:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}21}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{svm} \PY{k}{import} \PY{n}{SVC}
         
         \PY{n}{svm} \PY{o}{=} \PY{n}{SVC}\PY{p}{(}\PY{n}{kernel}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{linear}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{C}\PY{o}{=}\PY{l+m+mf}{1.0}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
         \PY{n}{svm}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}std}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
         \PY{n}{plot\PYZus{}decision\PYZus{}regions}\PY{p}{(}\PY{n}{X\PYZus{}combined\PYZus{}std}\PY{p}{,} \PY{n}{y\PYZus{}combined}\PY{p}{,} 
                               \PY{n}{classifier}\PY{o}{=}\PY{n}{svm}\PY{p}{,} \PY{n}{test\PYZus{}idx}\PY{o}{=}\PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{105}\PY{p}{,} \PY{l+m+mi}{150}\PY{p}{)}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{petal length [standardized]}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{petal width [stardardized]}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{upper left}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_67_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsection{Logistic regression versus support vector
machines}\label{logistic-regression-versus-support-vector-machines}

In practical classification tasks, linear logistic regression and linear
SVMs often yield very similar results. Logistic regression tries to
maximize the conditional likelihoods of the training data, which makes
it more prone to outliers than SVMs, which mostly care about the points
that are closest to the decision boundary (support vectors). On the
other hand, logistic regression has the advantage that it is a simpler
model that can be implemented more easily. Furthermore, logistic
regression models can be easily updated, which is attractive when
working with streaming data.

    \section{Alternative implementations in
scikit-learn}\label{alternative-implementations-in-scikit-learn}

    Scikit-learn offers alternative implementations via the
\emph{SGDClassifier} class, which also supports online learning via the
\emph{partial\_fit} method. The concept behind the \emph{SGDClassifier}
class is similiar to the stochastic gradient descent algororithm that we
implemented before. We could initialize the stochastic gradient descent
version of the perceptron, logistic regression and a support vector
machine with default parameters as follows:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}22}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{linear\PYZus{}model} \PY{k}{import} \PY{n}{SGDClassifier}
         
         \PY{n}{ppn} \PY{o}{=} \PY{n}{SGDClassifier}\PY{p}{(}\PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{perceptron}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{lr} \PY{o}{=} \PY{n}{SGDClassifier}\PY{p}{(}\PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{log}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{svm} \PY{o}{=} \PY{n}{SGDClassifier}\PY{p}{(}\PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{hinge}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \section{Solving nonlinear problems using a kernel
SVM}\label{solving-nonlinear-problems-using-a-kernel-svm}

    Another reason why SVMs enjoy high popularity among machine learning
practitioners is that it can be easily \emph{kernelized} to solve
nonlinear classification problems. Before we discuss the main concept
behind the \textbf{kernel SVM}, let's first create a sample dataset to
see what such a nonlinear classification problem may look like.

    \section{Kernel methods for linearly inseparable
data}\label{kernel-methods-for-linearly-inseparable-data}

    Using the following code, we will create a simple dataset that has the
form of an XOR gate using the \emph{logical\_or} function from NumPy,
where 100 samples will be assigned the class label 1, and 100 samples
will be assigned the class label -1:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}23}]:} \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
         \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
         
         \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}
         \PY{n}{X\PYZus{}xor} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{l+m+mi}{200}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}
         \PY{n}{y\PYZus{}xor} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{logical\PYZus{}xor}\PY{p}{(}\PY{n}{X\PYZus{}xor}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]} \PY{o}{\PYZgt{}} \PY{l+m+mi}{0}\PY{p}{,} \PY{n}{X\PYZus{}xor}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]} \PY{o}{\PYZgt{}} \PY{l+m+mi}{0}\PY{p}{)}
         \PY{n}{y\PYZus{}xor} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{where}\PY{p}{(}\PY{n}{y\PYZus{}xor}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X\PYZus{}xor}\PY{p}{[}\PY{n}{y\PYZus{}xor}\PY{o}{==}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} 
                     \PY{n}{X\PYZus{}xor}\PY{p}{[}\PY{n}{y\PYZus{}xor}\PY{o}{==}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{marker}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{x}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                     \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X\PYZus{}xor}\PY{p}{[}\PY{n}{y\PYZus{}xor}\PY{o}{==}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} 
                     \PY{n}{X\PYZus{}xor}\PY{p}{[}\PY{n}{y\PYZus{}xor}\PY{o}{==}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{marker}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{x}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                     \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlim}\PY{p}{(}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylim}\PY{p}{(}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{best}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_76_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Obviously, we would not be able to separate samples from the positive
and negative class very well using a linear hyperplane as a decision
boundary via the linear logistic regression or linear SVM model that we
discussed in earlier sections.

The basic idea behind \textbf{kernel methods} to deal with such linearly
inseparable data is to create nonlinear combinations of the original
features to project them onto a higher-dimensional space via a mapping
function \(\phi\) where it becomes linearly separable. As shown in the
following figure, we can transform a two-dimensional dataset onto a new
three-dimensional feature space where the classes become separable via
the following projection:

\[\phi(x_1, x_2) = (z_1, z_2, z_3) = (x_1, x_2, x_1^2 + x_2^2)\]

This allows us to separate the two classes shown in the plot via a
linear hyperplane that becomes a nonlinear decision boundary if we
project it back onot the original feature space:

    \section{Using the kernel trick to find separating hyperplanes in the
high-dimensional
space}\label{using-the-kernel-trick-to-find-separating-hyperplanes-in-the-high-dimensional-space}

    To solve a nonlinear problem using an SVM, we would transform the
training data onto a higher-dimensional feature via a mapping function
\(\phi\) and train a linear SVM model to classify the data in this new
feature space. Then, we can use the same mapping function \(\phi\) to
transform new, unseen data to classify it using the linear SVM model.

However, one problem with this mapping approach is that the construction
of the new features is computationally very expensive, especially if we
are dealing with high-dimensional data. This is where the so-called
kernel trick comes to play. Although we did not go into much detail
about how to solve the quadratic programming task to train an SVM, in
practice all we need is to replace the dot product \(x^{(i)T}x^j\) by
\(\phi(x^{(i)})^t \phi(x^{(j)})\). In order to save the expensive step
of calculating this dot product between two points explicitly, we define
a so-called \textbf{kernel function}. One of the most widely used
kernels is the \textbf{Radial Basis Function (RBF)} kernel or simply
called the \textbf{Gaussian kernel}:

\[\kappa(x^{(i)}, x^{(j)}) = \exp{-\frac{||x^{(i)} - x^{(j)}||^2}{2\sigma^2}}\]

This is often simplified to:

\[\kappa(x^{(i)}, x^{(j)}) = \exp{-\gamma||x^{(i)} - x^{(j)}||^2}\]

Here, \(\gamma = \frac{1}{2\sigma^2}\) is a free parameter that is to be
optimized.

    Roughly speaking, the term \textbf{kernel} can be interpreted as a
\textbf{similarity function} between a pair of samples. The minus sign
inverts the distance measure into a similarity score, and, due to the
exponential term, the resulting similarity score will fall into a range
between 1 (for exactly similar samples) and 0 (for very similar
samples).

Now that we defined the big picture behind the kernel trick, let us see
if we can train a kernel SVM that is able to draw a lonlinear decision
boundary that separates the XOR data well. Here, we simply use the
\emph{SVC} class from scikit-learn that we imported earlier and replace
the \emph{kernel='linear'} parameter with \emph{kernel='rbf'}:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}24}]:} \PY{n}{svm} \PY{o}{=} \PY{n}{SVC}\PY{p}{(}\PY{n}{kernel}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rbf}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{gamma}\PY{o}{=}\PY{l+m+mf}{0.10}\PY{p}{,} \PY{n}{C}\PY{o}{=}\PY{l+m+mf}{10.0}\PY{p}{)}
         \PY{n}{svm}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}xor}\PY{p}{,} \PY{n}{y\PYZus{}xor}\PY{p}{)}
         \PY{n}{plot\PYZus{}decision\PYZus{}regions}\PY{p}{(}\PY{n}{X\PYZus{}xor}\PY{p}{,} \PY{n}{y\PYZus{}xor}\PY{p}{,} \PY{n}{classifier}\PY{o}{=}\PY{n}{svm}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{upper left}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_81_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    The \(\gamma\) parameter, which we set to \emph{gamma=0.1} can be
understood as a \textbf{cut-off} parameter for the Gaussian sphere. If
we increase the value for \(\gamma\), we increase the influence or reach
of the training samples, which leads to a tighter and bumpier decision
boundary. To get a better intuition of \(\gamma\), let us apply an RBF
kernel SVM to our Iris flower dataset:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}25}]:} \PY{n}{svm} \PY{o}{=} \PY{n}{SVC}\PY{p}{(}\PY{n}{kernel}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rbf}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{gamma}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,} \PY{n}{C}\PY{o}{=}\PY{l+m+mf}{1.0}\PY{p}{)}
         \PY{n}{svm}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}std}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
         \PY{n}{plot\PYZus{}decision\PYZus{}regions}\PY{p}{(}\PY{n}{X\PYZus{}combined\PYZus{}std}\PY{p}{,} \PY{n}{y\PYZus{}combined}\PY{p}{,} 
                               \PY{n}{classifier}\PY{o}{=}\PY{n}{svm}\PY{p}{,} 
                               \PY{n}{test\PYZus{}idx}\PY{o}{=}\PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{105}\PY{p}{,} \PY{l+m+mi}{150}\PY{p}{)}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{petal length [stardardized]}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{petal width [stardardized]}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{upper left}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_83_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Since we chose a relatively small value for \(\gamma\), the resulting
decision boundary of the RBF kernel SVM model will be relatively soft,
as shown in the preceding figure.

    Now, let us increase the value of \(\gamma\) and observe the effect on
the decision boundary:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}26}]:} \PY{n}{svm} \PY{o}{=} \PY{n}{SVC}\PY{p}{(}\PY{n}{kernel}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rbf}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{gamma}\PY{o}{=}\PY{l+m+mf}{100.0}\PY{p}{,} \PY{n}{C}\PY{o}{=}\PY{l+m+mf}{1.0}\PY{p}{)}
         \PY{n}{svm}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}std}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
         \PY{n}{plot\PYZus{}decision\PYZus{}regions}\PY{p}{(}\PY{n}{X\PYZus{}combined\PYZus{}std}\PY{p}{,} \PY{n}{y\PYZus{}combined}\PY{p}{,} 
                               \PY{n}{classifier}\PY{o}{=}\PY{n}{svm}\PY{p}{,} 
                               \PY{n}{test\PYZus{}idx}\PY{o}{=}\PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{105}\PY{p}{,} \PY{l+m+mi}{150}\PY{p}{)}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{petal length [stardardized]}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{petal width [stardardized]}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{upper left}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_86_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    In the resulting plot, we can now see that the decision boundary around
the classes 0 and 1 is much tighter using a relatively large value of
\(\gamma\).

Although the model fits the training dataset very well, such a
classifier will likely have a high generalization error on unseen data.
This illustrates that the \(\gamma\) parameter also plays an important
role in controlling overfitting.

    \section{Decision tree learning}\label{decision-tree-learning}

    \textbf{Decision tree} classifiers are attractive models if we care
about interpretability. As the name decision tree suggests, we can think
of this model as breaking down our data by making decision based on
asking a series of questions.

Let's consider the following example in which we use a decision tree to
decide upon an activity on a particular day:

    Based on the features in our training set, the decision tree model
learns a series of questions to infer the class labels of the samples.
Although the preceding figure illustrates the concept of a decision tree
based on categorical variables, the same concept applies if our features
are real numbers, like in the Iris dataset. For example, we could simply
define a cut-off value along the \textbf{sepal width} feature axis and
ask a binary question "Is sepal width \textgreater{}= 2.0 cm?".

Using the decision algorithm, we start at the tree root and split the
data on the feature that results in the largest \textbf{Information Gain
(IG}). In an iterative process, we can then repeat this splitting
procedure at each child node until the leaves are pure. This means that
the samples at each node all belong to the same class. In practice, this
can result in a very deep tree with many nodes, which can easily lead to
overfitting. Thus, we typically want to \textbf{prune} the tree by
setting a limit for the maximal depth of the tree.

The three impurity measures or splitting criteria that are commonly used
in binary decision trees are \textbf{Gini impurity}, \textbf{entropy}
and the \textbf{classification error}.

    \section{Building a decision tree}\label{building-a-decision-tree}

    Decision trees can build complex decision boundaries by dividing the
feature space into rectangles. However, we have to be careful since the
deeper the decision tree, the more complex the decision boundary
becomes, which can easily result in overfitting. Using scikit-learn, we
will now train a decision tree with a maximum depth of 3, using entropy
as a criterion for impurity. Although feature scaling may be desired for
visualization purposes, note that feature scaling is not a requirement
for decision tree algorithms. The code is as follows:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}27}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{tree} \PY{k}{import} \PY{n}{DecisionTreeClassifier}
         
         \PY{n}{tree} \PY{o}{=} \PY{n}{DecisionTreeClassifier}\PY{p}{(}\PY{n}{criterion}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gini}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                                       \PY{n}{max\PYZus{}depth}\PY{o}{=}\PY{l+m+mi}{4}\PY{p}{,} 
                                       \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
         \PY{n}{tree}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
         \PY{n}{X\PYZus{}combined} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{vstack}\PY{p}{(}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{)}\PY{p}{)}
         \PY{n}{y\PYZus{}combined} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{hstack}\PY{p}{(}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{)}
         \PY{n}{plot\PYZus{}decision\PYZus{}regions}\PY{p}{(}\PY{n}{X\PYZus{}combined}\PY{p}{,} \PY{n}{y\PYZus{}combined}\PY{p}{,} 
                               \PY{n}{classifier}\PY{o}{=}\PY{n}{tree}\PY{p}{,} 
                               \PY{n}{test\PYZus{}idx}\PY{o}{=}\PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{150}\PY{p}{,} \PY{l+m+mi}{150}\PY{p}{)}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{petal length [cm]}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{petal width [cm]}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylim}\PY{p}{(}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mf}{3.5}\PY{p}{]}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{upper left}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_93_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    A nice feature in scikit-learn is that it allows us to export the
decision tree as a .dot file after training, which we can visualize
using the GraphViz program, for example. In addition to GraphViz, we
will use a Python library called \emph{pydotplus}, which has
capabilities similar to GraphViz and allows us to convert \emph{.doc}
data files into a decision tree image file.

The following code will create an image of our decision tree in PNG
format in our local directory:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}28}]:} \PY{k+kn}{from} \PY{n+nn}{pydotplus} \PY{k}{import} \PY{n}{graph\PYZus{}from\PYZus{}dot\PYZus{}data}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{tree} \PY{k}{import} \PY{n}{export\PYZus{}graphviz}
         
         \PY{n}{dot\PYZus{}data} \PY{o}{=} \PY{n}{export\PYZus{}graphviz}\PY{p}{(}\PY{n}{tree}\PY{p}{,} \PY{n}{filled}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{rounded}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} 
                                    \PY{n}{class\PYZus{}names}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Setosa}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                                                 \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Versicolor}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                                                 \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Virginica}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} 
                                    \PY{n}{feature\PYZus{}names}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{petal length}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                                                   \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{petal width}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} 
                                    \PY{n}{out\PYZus{}file}\PY{o}{=}\PY{k+kc}{None}\PY{p}{)}
         \PY{n}{graph} \PY{o}{=} \PY{n}{graph\PYZus{}from\PYZus{}dot\PYZus{}data}\PY{p}{(}\PY{n}{dot\PYZus{}data}\PY{p}{)}
         \PY{n}{graph}\PY{o}{.}\PY{n}{write\PYZus{}png}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{images/tree.png}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}28}]:} True
\end{Verbatim}
            
    By using the \emph{out\_file=None} setting, we directly assigned the dot
data to a \emph{dot\_data} variable, instead of writing an intermediate
\emph{tree.dot} file to disk. The arguments for \emph{filled, rounded,
class\_names} and \emph{feature\_names} are optional but make the
resulting image file visually more appealing by adding color, rounding
the box edges, showing the name of the majority class label at each
node, and displaying the feature names in the splitting criterion. These
setting resulted in the following decision tree image:

    Looking at the decision tree figure, we can now nicely trace back the
splits that the decision tree determined from our training dataset. We
started with 105 samples at the root and split them into two child nodes
with 35 and 70 samples, using the \textbf{petal width} cut-off
\(\le 0.75\) cm. After the first split, we can see that the left child
is already pure and only contains samples from the \emph{Iris-setosa}
class (Gini Impurity = 0). The further splits on the right are then used
to separate the samples from the \emph{Iris-versicolor} and
\emph{Iris-virginica} class.

Looking at this tree, and the decision region plot of the tree, we see
that the decision tree does a very good job of separating the flower
classes. Unfortunately, scikit-learn currently does not implement
functionality to manually post-prune a decision tree.

    \section{Combining multiple decision trees via random
forests}\label{combining-multiple-decision-trees-via-random-forests}

    \textbf{Random forests} have gained huge popularity in applications of
machine learning during the last decade due to their good classification
performance, scalability, and ease of use. Intuitively, a random forest
can be considered as an \textbf{ensemble} of decision trees. The idea
behind a random forest is to average multiple (deep) decision trees that
individually suffer from high variance, to build a more robust model
that has a better generalization performance and is less susceptible to
overfitting. The random forest algorithm can be summarized in four
simple steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Draw a random \textbf{bootstrap} sample of size \emph{n} (randomly
  choose n samples from the training set with replacement).
\item
  Grow a decision tree from the bootstrap sample. At each node:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    Randomly select \emph{d} features without replacement.
  \item
    Split the node using the feature that provides the best split
    according to the objective function, for instance, maximizing the
    information gain.
  \end{enumerate}
\item
  Repeat the step 1-2 \emph{k} times.
\item
  Aggregate the prediction by each tree to assign the class label by
  \textbf{majority vote}.
\end{enumerate}

We should note one slight modification in step 2 when we are training
the individual decision trees: instead of evaluating all features to
determine the best split at each node, we only consider a random subset
of those.

    Although random forest do not offer the same level of interpretability
as decision trees, a big advantage of random forests is that we do not
have to worry so much about choosing good hyperparameter values. We
typically do not need to prune the random forest since the ensemble
model is quite robust to noise from the individual decision trees. The
only parameter that we really need to care about in practice is the
number of trees \emph{k} (step 3) that we choose for the random forest.
Typically, the larger the number of trees, the better the performance of
the random forest classifier at the expense of an increased
computational cost.

Although it is less common in practice, other hyperparameters of the
random forest classifier that can be optimized are the size \emph{n} of
the bootstrap sample (step 1) and the number of features \emph{d} that
is randomly chosen for each split (step 2.1), respectively. Via the
sample size \emph{n} of the bootstrap sample, we control the
bias-variance tradeoff of the random forest.

Decreasing the size of the bootstrap sample increases the diversity
among the individual trees, since the probability that a particular
training sample is included in the bootstrap sample is lower. Thus,
shrinking the size of the bootstrap samples may increase the
\emph{randomness} of the random forest, and it can help to reduce the
effect of overfitting. However, smaller bootstrap samples typically
result in a lower overall performance of the random forest, a small gap
between training and test performance, but a low test performance
overall. Conversely, increasing the size of the bootstrap sample may
increase the degree of overfitting. Because the bootstrap samples, and
consequently the individual decision trees, become more similar to each
other, they can learn to fit the original training dataset more closely.

In most implementations, including the \emph{RandomForestClassifier}
implementation in scikit-learn, the size of the bootstrap sample is
chosen to be equal to the number of samples in the original training
set, which usually provides a good bias-variance tradeoff. For the
number of features \emph{d} at each split, we want to choose the value
that is smaller then the total number of features in the training set. A
reasonable default that is used in scikit-learn and other
implementations is \(d = \sqrt{m}\), where \emph{m} is the number of
features in the training set.

Conveniently, we do not have to construct the random forest classifier
from individual decision trees by ourselves because ther is already an
implementation in scikit-learn that we can use:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}29}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{ensemble} \PY{k}{import} \PY{n}{RandomForestClassifier}
         
         \PY{n}{forest} \PY{o}{=} \PY{n}{RandomForestClassifier}\PY{p}{(}\PY{n}{criterion}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gini}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                                         \PY{n}{n\PYZus{}estimators}\PY{o}{=}\PY{l+m+mi}{25}\PY{p}{,} 
                                         \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} 
                                         \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}
         \PY{n}{forest}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
         \PY{n}{plot\PYZus{}decision\PYZus{}regions}\PY{p}{(}\PY{n}{X\PYZus{}combined}\PY{p}{,} \PY{n}{y\PYZus{}combined}\PY{p}{,} 
                               \PY{n}{classifier}\PY{o}{=}\PY{n}{forest}\PY{p}{,} 
                               \PY{n}{test\PYZus{}idx}\PY{o}{=}\PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{105}\PY{p}{,} \PY{l+m+mi}{150}\PY{p}{)}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{petal length}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{petal width}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylim}\PY{p}{(}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mf}{3.5}\PY{p}{]}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{upper left}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_101_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Using the preceding code, we trained a random forest from 25 decision
trees via the \emph{n\_estimators} parameter and used the entropy
criterion as an impurity measure to split nodes. Although we are growing
a very small random forest from a very small training dataset, we used
the \emph{n\_jobs} parameter for demonstration purposes, which allows us
to parallelize the model training using multiple cores of our computer
(here two cores).

    \section{K-nearest neighbours - a lazy learning
algorithm}\label{k-nearest-neighbours---a-lazy-learning-algorithm}

    The last supervised learning algorithm that we want to discuss in this
chapter is the \textbf{k-nearest neighbour (KNN)} classifier, which is a
particularly interesting because it is fundamentally diferent from the
learning algorithms that we have discussed earlier.

KNN is a typical example of a \textbf{lazy learner}. It is called
\emph{lazy} not because of its apparent simplicity, but because it does
not learn a discriminative function from the training data, but
memorizes the training dataset instead.

\textbf{Parametric versus nonparametric models}

Machine learning algorithms can be grouped into \textbf{parametric} and
\textbf{nonparametric} models. Using parametric models, we estimate
parameters from the training dataset to learn a function that can
classify new data points without requiring the original training dataset
anymore. Typical examples of parametric models are the perceptron,
logistic regression, and the linear SVM. In contrast, non parametric
models cannot be characterized by a fixed set of parameters, and the
number of parameters grows with the training data. Two examples of
non-parametric models that we have seen so far are the decision tree
classifier/random forest and the kernel SVM.

KNN belongs to a subcategory of nonparametric models that is described
as \textbf{instance-based learning}. Models based on instance-based
learning are characterized by memorizing the training dataset, and lazy
learning is a special case of instance based learning that is associated
with no cost during the learning process.

The KNN algorithm itself is fairly straightforward and can be summarized
by the following steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Choose the number of \emph{k} and a distance metric.
\item
  Find the \emph{k}-nearest neigbors of the sample that we want to
  classify.
\item
  Assign the class label by majority vote.
\end{enumerate}

The following figure illustrates how a new data point is assigned the
triangle class label based on majority voting among its give nearest
neighbors.

    Based on the chosen distance metric, the KNN algorithm finds the
\emph{k} samples in the training dataset that are closest (most similar)
to the point that we want to classify. The class label of the new data
point is then determined by a majority vote among its \emph{k} nearest
neighbors.

The main advantage of such a memory-based approach is that the
classifier immediately adapts as we collect new training data. However,
the downside is that the computational complexity for classifying new
samples grows linearly with the number of samples in the training
dataset in the worst-case scenario, unless the dataset has very few
dimensions (features) and the algorithm has been implemented using
efficient data structures such as KD-trees. Furthermore, we cannot
discard training samples since no training step is involved. Thus,
storage space can become a challenge if we are working with large
datasets.

By executing the following code, we will now implement a KNN model in
scikit-learn using a Euclidean distance metric:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}30}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{neighbors} \PY{k}{import} \PY{n}{KNeighborsClassifier}
         
         \PY{n}{knn} \PY{o}{=} \PY{n}{KNeighborsClassifier}\PY{p}{(}\PY{n}{n\PYZus{}neighbors}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{p}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} 
                                    \PY{n}{metric}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{minkowski}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{knn}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}std}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
         \PY{n}{plot\PYZus{}decision\PYZus{}regions}\PY{p}{(}\PY{n}{X\PYZus{}combined\PYZus{}std}\PY{p}{,} \PY{n}{y\PYZus{}combined}\PY{p}{,} 
                               \PY{n}{classifier}\PY{o}{=}\PY{n}{knn}\PY{p}{,} \PY{n}{test\PYZus{}idx}\PY{o}{=}\PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{105}\PY{p}{,} \PY{l+m+mi}{150}\PY{p}{)}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{petal length [standardized]}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{petal width [stardardized]}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{upper left}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_106_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    In the case of a tie, the scikit-learn implementation of the KNN
algorithm will prefer the neighbors with a closer distance to the
sample. If the neighbors have similar distances, the algorithm will
choose that class label that comes first in the training dataset.

The \emph{right} choice of \emph{k} is crucial do find a good balance
between overfitting and underfitting. We also have to make sure that we
choose a distance metric that is appropriate for the features in the
dataset. Often, a simple Euclidean distance measure is used for
real-value samples, for example, the flowers in our Iris dataset, which
have features measured in centimeters. However, if we are using a
Euclidean distance measure, it is also important to stardardize the data
so that each feature contributes equally to the distance. The
\emph{minkowski} distance that we used in the previous code is just a
generalization of the Euclidean distance and Manhattan distance, which
can be written as follows:

\[d(x^{(i)}, x^{(j)}) = \sqrt[p]{\sum_k \vert x_k^{(i)} - x_k^{(j)} \vert^p}\]

It becomes the Euclidean distance if we set the parameter \emph{p=2} or
the Manhattan distance at \emph{p=1}. Many other distance metrics are
available in scikit-learn and can be provided to the metric parameter.

    \textbf{The curse of dimensionality}

It is important to mention that KNN is very susceptible to overfitting
due to the \textbf{curse of dimensionality}. The curse of dimensionality
describes the phenomenon where the feature space becomes increasingly
sparse for an increasing number of dimensions of a fixed-size training
dataset. Intuitively, we can think of even the closest neighbors being
too far away in high-dimensional space to give a good estimate.

We have discussed the concept of regularization in the section about
logistic regression as one way to avoid overffiting. However, in models
where regularization is not applicable, such as decision trees and KNN,
we can use feature selection and dimensionality reduction techniques to
help us avoid the curse of dimensionality. This will be discussed in
more detail in the next chapter.

    \section{Summary}\label{summary}

    In this chapter, you learned about many different machine learning
algorithms that are used to tackle linear and nonlinear problems. We
have seen that decision trees are particularly attractive if we care
about interpretability. Logistic regression is not only a useful model
for online learning via stochastic gradient descent, but also allows us
to predict the probability of a particular event. Although support
vector machines are powerful linear models that can be extended to
nonlinear problems via the kernel trick, they have many parameters that
have to be tuned in order to make good predictions. In contrast,
ensemble methods such as random forests do not require much parameter
tuning and do not overfit as easily as decision trees, which makes them
attractive models for many practical problems domains. The KNN
classifier offers an alternative approach to classification via lazy
learning that allows us to make predictions without any model training,
but with a more computationally expensive prediction step.

However, even more important than the choice of an appropriate learning
algorithm is the available data in our training dataset. No algorithm
will be able to make good predictions without informative and
discriminatory features.

In the next chapter, we will discuss important topics regarding the
processing of data, feature selection, and dimensionality reduction,
which we will need to build powerful machine learning models.


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
